{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cat_dog_manal.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Miladpolytechnique/cat-dog/blob/master/cat_dog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tBgUarnqak3r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#%autoreload 2\n",
        "\n",
        "#%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WHWKszjrak3v",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torchsummary import summary\n",
        "from torch.autograd import Variable\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "np.random.seed(1234)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9IHqvK7Wak3w",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data.sampler as sampler\n",
        "\n",
        "use_gpu = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Cmd6p6veak3z",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.datasets as dataset\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WOdKkDqw9Nl9",
        "colab_type": "code",
        "outputId": "6e31129b-5102-4bad-e273-d38eef0a7d1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QWd_N8Dzak30",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyper parameters\n",
        "Data_DIR = \"/drive/My Drive/cat-dog/\"\n",
        "Image_size = 224 # size of each img\n",
        "batch_size = 256 # 25 ta ax midam vase train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-AHS0a71ak33",
        "outputId": "d5d715c1-5fe9-4a6e-cca4-8662d8d45624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "os.listdir(Data_DIR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['trainset', 'testset']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "12TSOXTBak38",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "trn_dir = f'{Data_DIR}trainset'  #make string my train dataset\n",
        "tst_dir = f'{Data_DIR}testset'   # make string my validation dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NaqHJjGVak3-",
        "outputId": "cc09b926-c767-457b-e548-c3a5ccdda9ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print(os.listdir(trn_dir))\n",
        "print(os.listdir(tst_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Cat', 'Dog', '.DS_Store']\n",
            "['test']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5aNokBejak4D",
        "outputId": "c2cf4ba1-24a0-45a2-d734-7c5c6d3dd65a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "# use glob library to address my img\n",
        "trn_fname = glob.glob(f'{trn_dir}/*/*.jpg')  # use glob to see where is my trainset and I mentioned which files I want!!!\n",
        "trn_fname[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/drive/My Drive/cat-dog/trainset/Cat/5560.Cat.jpg',\n",
              " '/drive/My Drive/cat-dog/trainset/Cat/7407.Cat.jpg',\n",
              " '/drive/My Drive/cat-dog/trainset/Cat/9481.Cat.jpg',\n",
              " '/drive/My Drive/cat-dog/trainset/Cat/5386.Cat.jpg',\n",
              " '/drive/My Drive/cat-dog/trainset/Cat/564.Cat.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pf_MqX65ak4G",
        "outputId": "e132ce38-5c38-4a7f-8e5e-f4119037c107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "cell_type": "code",
      "source": [
        "img = plt.imread(trn_fname[1])\n",
        "plt.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0836f2e278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFMCAYAAABCsp4mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "v9-Jnyb-892f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Data processing\n",
        "# resize the image to uniqe one and transfor it to tensor\n",
        "tfms = transforms.Compose([\n",
        "    transforms.Resize((Image_size, Image_size)), \n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(), # Tensor or matrix\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # RGB ,normlzie all imgs,to use in my mean and sd\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b0G5T1VP892m",
        "colab_type": "code",
        "outputId": "d03e7d09-ee83-4044-b9c1-afe61417c545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Apply transformation\n",
        "train_ds = dataset.ImageFolder(trn_dir, transform=tfms)\n",
        "test_ds = dataset.ImageFolder(tst_dir, transform=tfms)\n",
        "train_ds.class_to_idx"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cat': 0, 'Dog': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ldwQSWBOak4N",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Split data into training and validation\n",
        "TRAIN_SIZE = int(len(train_ds.imgs) *0.8)\n",
        "VALID_SIZE = len(train_ds.imgs) - TRAIN_SIZE\n",
        "\n",
        "indices = list(range(len(train_ds.imgs)))\n",
        "np.random.seed(123)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[:TRAIN_SIZE], indices[TRAIN_SIZE:]\n",
        "\n",
        "train_sampler = sampler.SubsetRandomSampler(train_idx)\n",
        "valid_sampler = sampler.SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(\n",
        "    train_ds, batch_size=batch_size, sampler=train_sampler, num_workers=8)\n",
        "\n",
        "\n",
        "valid_dl = torch.utils.data.DataLoader(\n",
        "    train_ds, batch_size=1,  sampler=valid_sampler, num_workers=8)\n",
        "\n",
        "test_dl = torch.utils.data.DataLoader(\n",
        "    test_ds, batch_size=1,num_workers=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1pcU4Kju892z",
        "colab_type": "code",
        "outputId": "0230712b-c109-45f9-9949-ec9ba93738d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Summary:\")\n",
        "print('Size train Dataset:',len(train_dl)*batch_size)\n",
        "print('Size validation:',len(valid_dl)*batch_size)\n",
        "print('Test images:',len(test_ds))\n",
        "print('Image size:', train_ds[0][0].size())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary:\n",
            "Size train Dataset: 16128\n",
            "Size validation: 1024000\n",
            "Test images: 4999\n",
            "Image size: torch.Size([3, 224, 224])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YeW5-e2yak4o",
        "outputId": "035c4516-4fb8-403d-87dd-38c45a0adfa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "inputs, targets = next(iter(train_dl)) # there is an object (train_dl) that we can make itration, it take 16 imgs\n",
        "out = torchvision.utils.make_grid(inputs, padding=3) # khoshgel bechinam beside eachother LOL\n",
        "\n",
        "plt.figure(figsize=(16, 12));"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x864 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VmDfX2C1ak4r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\n",
        "    \"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    plt.axis('off')\n",
        "    if title is not None:\n",
        "        plt.title(title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Jx_e7DNUak4v",
        "outputId": "3e329290-fa2b-405d-ee89-85df14b93394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "cell_type": "code",
      "source": [
        "imshow(out, title='Random images from training data')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMcAAAFZCAYAAAAsHjtBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BJUKKWTIak4y",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class VGG(nn.Module):\n",
        "    \n",
        "#     def __init__(self): # __init__ is constructor \n",
        "#         super(VGG, self).__init__()\n",
        "        \n",
        "#         self.layer1 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3, 3),\n",
        "#             stride=1, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(2, stride=2)\n",
        "#         )\n",
        "\n",
        "#         self.layer2 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3),\n",
        "#             stride=1, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(2, stride=2)\n",
        "#         )\n",
        "        \n",
        "#         self.layer3 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3),\n",
        "#             stride=1, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3),\n",
        "#             stride=1, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(2, stride=2)\n",
        "#         )\n",
        "        \n",
        "#         self.layer4 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3, 3),\n",
        "#             stride=1, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3),\n",
        "#             stride=1, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(2, stride=2)\n",
        "#         )\n",
        "        \n",
        "#         self.layer5 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3),\n",
        "#             stride=1, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3),\n",
        "#             stride=1, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(2, stride=2)\n",
        "#         )\n",
        "        \n",
        "        \n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(512*7*7, 4096),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(4096, 4096),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(4096, 1000),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(1000, 2),\n",
        "#             nn.Softmax()\n",
        "#         )\n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         out = self.layer1(x)\n",
        "#         out = self.layer2(out)            \n",
        "#         out = self.layer3(out)\n",
        "#         out = self.layer4(out)\n",
        "#         out = self.layer5(out)\n",
        "#         out = out.view(out.size(0), -1) \n",
        "#         out = self.fc(out)\n",
        "        \n",
        "#         return out\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self): # __init__ is constructor \n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16,  kernel_size=(3, 3),stride=1, padding=1), # 3 = RGB(3channal), 16= 16 filter, padding= reduce the sie 1 column and rows\n",
        "            nn.ReLU(), # nonlinear AF, - make 0\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32,   kernel_size=(3, 3),stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64,  kernel_size=(3, 3),stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "        \n",
        "        self.fc =  nn.Sequential(\n",
        "            nn.Linear(28 * 28 * 64, 500), # fully connected layer, in pytorch we call linear layer, make the all layer to 2 vector (0,1)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(500, 2),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)             # (bs, C, H, W)\n",
        "        out = self.conv3(out)\n",
        "#         print(out.size())\n",
        "        out = out.view(out.size(0), -1)  #(bs, C, H, W)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1T6cPIBqz4n-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cuda_available = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OH60RjRszoK-",
        "colab_type": "code",
        "outputId": "26ccb049-bd45-4671-8286-85c7795d653a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "#  run\n",
        "model = SimpleCNN()\n",
        "model.cuda()\n",
        "summary(model, (3, 224,224))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 224, 224]             448\n",
            "              ReLU-2         [-1, 16, 224, 224]               0\n",
            "         MaxPool2d-3         [-1, 16, 112, 112]               0\n",
            "            Conv2d-4         [-1, 32, 112, 112]           4,640\n",
            "              ReLU-5         [-1, 32, 112, 112]               0\n",
            "         MaxPool2d-6           [-1, 32, 56, 56]               0\n",
            "            Conv2d-7           [-1, 64, 56, 56]          18,496\n",
            "              ReLU-8           [-1, 64, 56, 56]               0\n",
            "         MaxPool2d-9           [-1, 64, 28, 28]               0\n",
            "           Linear-10                  [-1, 500]      25,088,500\n",
            "             ReLU-11                  [-1, 500]               0\n",
            "           Linear-12                    [-1, 2]           1,002\n",
            "          Softmax-13                    [-1, 2]               0\n",
            "================================================================\n",
            "Total params: 25,113,086\n",
            "Trainable params: 25,113,086\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 24.12\n",
            "Params size (MB): 95.80\n",
            "Estimated Total Size (MB): 120.50\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "FtS9jNZ0zrzu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if cuda_available:\n",
        "    model = model.cuda()\n",
        "#Loss function and \n",
        "criterion = nn.CrossEntropyLoss() # minimize this \n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01) # stochastic gradien, W = W - lr * dW\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "z7J2FyNBak45",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_var(x):\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return Variable(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AYZfCjlhzdxi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for epoch in range(10):\n",
        "#     losses = []\n",
        "#     # Train\n",
        "#     for batch_idx, (inputs, targets) in enumerate(train_dl):\n",
        "#         if cuda_available:\n",
        "#             inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         inputs, targets = Variable(inputs), Variable(targets)\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         losses.append(loss.item())\n",
        "\n",
        "#     print('Epoch : %d Loss : %.3f ' % (epoch, np.mean(losses)))\n",
        "    \n",
        "#     # Evaluate\n",
        "#     model.eval()\n",
        "#     total = 0\n",
        "#     correct = 0\n",
        "#     for batch_idx, (inputs, targets) in enumerate(test_dl):\n",
        "#         if cuda_available:\n",
        "#             inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "#         inputs, targets = Variable(inputs, volatile=True), Variable(targets, volatile=True)\n",
        "#         outputs = model(inputs)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += targets.size(0)\n",
        "#         correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "#     print('Epoch : %d Test Acc : %.3f' % (epoch, 100.*correct/total))\n",
        "#     print('--------------------------------------------------------------')\n",
        "#     model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PxBAkRv9ak48",
        "outputId": "d41dd8ce-8d59-491e-a564-118fe5e93df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109462
        }
      },
      "cell_type": "code",
      "source": [
        "# #Train\n",
        "\n",
        "\n",
        "# num_epochs = 100\n",
        "# losses = []\n",
        "# epoch_losses = []\n",
        "# train_losses, test_losses = [], []\n",
        "# for epoch in range(num_epochs):\n",
        "#     for i, (inputs, targets) in enumerate(train_dl):\n",
        "#         inputs = to_var(inputs)\n",
        "#         targets = to_var(targets)\n",
        "                \n",
        "#         # forward\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "        \n",
        "#         # loss\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         losses += [loss.item()] # draw fig\n",
        "        \n",
        "#         # loos backward\n",
        "#         loss.backward()\n",
        "        \n",
        "#         #update params\n",
        "#         optimizer.step()\n",
        "        \n",
        "#         #report\n",
        "#         print ('Epoch [%2d/%3d], Step [%3d/%3d], Loss: %.4f' \n",
        "#                    % (epoch + 1, num_epochs, i + 1, len(train_ds) // batch_size, loss.item()))\n",
        "        \n",
        "#     epoch_losses.append(np.mean(losses))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 1/100], Step [  1/ 78], Loss: 0.6940\n",
            "Epoch [ 1/100], Step [  2/ 78], Loss: 0.6932\n",
            "Epoch [ 1/100], Step [  3/ 78], Loss: 0.6932\n",
            "Epoch [ 1/100], Step [  4/ 78], Loss: 0.6930\n",
            "Epoch [ 1/100], Step [  5/ 78], Loss: 0.6931\n",
            "Epoch [ 1/100], Step [  6/ 78], Loss: 0.6936\n",
            "Epoch [ 1/100], Step [  7/ 78], Loss: 0.6931\n",
            "Epoch [ 1/100], Step [  8/ 78], Loss: 0.6929\n",
            "Epoch [ 1/100], Step [  9/ 78], Loss: 0.6930\n",
            "Epoch [ 1/100], Step [ 10/ 78], Loss: 0.6927\n",
            "Epoch [ 1/100], Step [ 11/ 78], Loss: 0.6925\n",
            "Epoch [ 1/100], Step [ 12/ 78], Loss: 0.6930\n",
            "Epoch [ 1/100], Step [ 13/ 78], Loss: 0.6933\n",
            "Epoch [ 1/100], Step [ 14/ 78], Loss: 0.6929\n",
            "Epoch [ 1/100], Step [ 15/ 78], Loss: 0.6924\n",
            "Epoch [ 1/100], Step [ 16/ 78], Loss: 0.6923\n",
            "Epoch [ 1/100], Step [ 17/ 78], Loss: 0.6933\n",
            "Epoch [ 1/100], Step [ 18/ 78], Loss: 0.6926\n",
            "Epoch [ 1/100], Step [ 19/ 78], Loss: 0.6926\n",
            "Epoch [ 1/100], Step [ 20/ 78], Loss: 0.6928\n",
            "Epoch [ 1/100], Step [ 21/ 78], Loss: 0.6924\n",
            "Epoch [ 1/100], Step [ 22/ 78], Loss: 0.6927\n",
            "Epoch [ 1/100], Step [ 23/ 78], Loss: 0.6923\n",
            "Epoch [ 1/100], Step [ 24/ 78], Loss: 0.6919\n",
            "Epoch [ 1/100], Step [ 25/ 78], Loss: 0.6931\n",
            "Epoch [ 1/100], Step [ 26/ 78], Loss: 0.6923\n",
            "Epoch [ 1/100], Step [ 27/ 78], Loss: 0.6928\n",
            "Epoch [ 1/100], Step [ 28/ 78], Loss: 0.6923\n",
            "Epoch [ 1/100], Step [ 29/ 78], Loss: 0.6907\n",
            "Epoch [ 1/100], Step [ 30/ 78], Loss: 0.6932\n",
            "Epoch [ 1/100], Step [ 31/ 78], Loss: 0.6934\n",
            "Epoch [ 1/100], Step [ 32/ 78], Loss: 0.6921\n",
            "Epoch [ 1/100], Step [ 33/ 78], Loss: 0.6920\n",
            "Epoch [ 1/100], Step [ 34/ 78], Loss: 0.6927\n",
            "Epoch [ 1/100], Step [ 35/ 78], Loss: 0.6918\n",
            "Epoch [ 1/100], Step [ 36/ 78], Loss: 0.6923\n",
            "Epoch [ 1/100], Step [ 37/ 78], Loss: 0.6922\n",
            "Epoch [ 1/100], Step [ 38/ 78], Loss: 0.6932\n",
            "Epoch [ 1/100], Step [ 39/ 78], Loss: 0.6920\n",
            "Epoch [ 1/100], Step [ 40/ 78], Loss: 0.6934\n",
            "Epoch [ 1/100], Step [ 41/ 78], Loss: 0.6916\n",
            "Epoch [ 1/100], Step [ 42/ 78], Loss: 0.6921\n",
            "Epoch [ 1/100], Step [ 43/ 78], Loss: 0.6914\n",
            "Epoch [ 1/100], Step [ 44/ 78], Loss: 0.6919\n",
            "Epoch [ 1/100], Step [ 45/ 78], Loss: 0.6918\n",
            "Epoch [ 1/100], Step [ 46/ 78], Loss: 0.6925\n",
            "Epoch [ 1/100], Step [ 47/ 78], Loss: 0.6918\n",
            "Epoch [ 1/100], Step [ 48/ 78], Loss: 0.6912\n",
            "Epoch [ 1/100], Step [ 49/ 78], Loss: 0.6915\n",
            "Epoch [ 1/100], Step [ 50/ 78], Loss: 0.6921\n",
            "Epoch [ 1/100], Step [ 51/ 78], Loss: 0.6923\n",
            "Epoch [ 1/100], Step [ 52/ 78], Loss: 0.6914\n",
            "Epoch [ 1/100], Step [ 53/ 78], Loss: 0.6914\n",
            "Epoch [ 1/100], Step [ 54/ 78], Loss: 0.6920\n",
            "Epoch [ 1/100], Step [ 55/ 78], Loss: 0.6899\n",
            "Epoch [ 1/100], Step [ 56/ 78], Loss: 0.6916\n",
            "Epoch [ 1/100], Step [ 57/ 78], Loss: 0.6917\n",
            "Epoch [ 1/100], Step [ 58/ 78], Loss: 0.6915\n",
            "Epoch [ 1/100], Step [ 59/ 78], Loss: 0.6915\n",
            "Epoch [ 1/100], Step [ 60/ 78], Loss: 0.6920\n",
            "Epoch [ 1/100], Step [ 61/ 78], Loss: 0.6914\n",
            "Epoch [ 1/100], Step [ 62/ 78], Loss: 0.6910\n",
            "Epoch [ 1/100], Step [ 63/ 78], Loss: 0.6919\n",
            "Epoch [ 2/100], Step [  1/ 78], Loss: 0.6925\n",
            "Epoch [ 2/100], Step [  2/ 78], Loss: 0.6913\n",
            "Epoch [ 2/100], Step [  3/ 78], Loss: 0.6897\n",
            "Epoch [ 2/100], Step [  4/ 78], Loss: 0.6921\n",
            "Epoch [ 2/100], Step [  5/ 78], Loss: 0.6901\n",
            "Epoch [ 2/100], Step [  6/ 78], Loss: 0.6910\n",
            "Epoch [ 2/100], Step [  7/ 78], Loss: 0.6909\n",
            "Epoch [ 2/100], Step [  8/ 78], Loss: 0.6912\n",
            "Epoch [ 2/100], Step [  9/ 78], Loss: 0.6913\n",
            "Epoch [ 2/100], Step [ 10/ 78], Loss: 0.6897\n",
            "Epoch [ 2/100], Step [ 11/ 78], Loss: 0.6906\n",
            "Epoch [ 2/100], Step [ 12/ 78], Loss: 0.6897\n",
            "Epoch [ 2/100], Step [ 13/ 78], Loss: 0.6918\n",
            "Epoch [ 2/100], Step [ 14/ 78], Loss: 0.6903\n",
            "Epoch [ 2/100], Step [ 15/ 78], Loss: 0.6907\n",
            "Epoch [ 2/100], Step [ 16/ 78], Loss: 0.6914\n",
            "Epoch [ 2/100], Step [ 17/ 78], Loss: 0.6916\n",
            "Epoch [ 2/100], Step [ 18/ 78], Loss: 0.6901\n",
            "Epoch [ 2/100], Step [ 19/ 78], Loss: 0.6910\n",
            "Epoch [ 2/100], Step [ 20/ 78], Loss: 0.6892\n",
            "Epoch [ 2/100], Step [ 21/ 78], Loss: 0.6896\n",
            "Epoch [ 2/100], Step [ 22/ 78], Loss: 0.6901\n",
            "Epoch [ 2/100], Step [ 23/ 78], Loss: 0.6904\n",
            "Epoch [ 2/100], Step [ 24/ 78], Loss: 0.6912\n",
            "Epoch [ 2/100], Step [ 25/ 78], Loss: 0.6907\n",
            "Epoch [ 2/100], Step [ 26/ 78], Loss: 0.6925\n",
            "Epoch [ 2/100], Step [ 27/ 78], Loss: 0.6911\n",
            "Epoch [ 2/100], Step [ 28/ 78], Loss: 0.6911\n",
            "Epoch [ 2/100], Step [ 29/ 78], Loss: 0.6913\n",
            "Epoch [ 2/100], Step [ 30/ 78], Loss: 0.6911\n",
            "Epoch [ 2/100], Step [ 31/ 78], Loss: 0.6906\n",
            "Epoch [ 2/100], Step [ 32/ 78], Loss: 0.6895\n",
            "Epoch [ 2/100], Step [ 33/ 78], Loss: 0.6901\n",
            "Epoch [ 2/100], Step [ 34/ 78], Loss: 0.6885\n",
            "Epoch [ 2/100], Step [ 35/ 78], Loss: 0.6887\n",
            "Epoch [ 2/100], Step [ 36/ 78], Loss: 0.6920\n",
            "Epoch [ 2/100], Step [ 37/ 78], Loss: 0.6903\n",
            "Epoch [ 2/100], Step [ 38/ 78], Loss: 0.6897\n",
            "Epoch [ 2/100], Step [ 39/ 78], Loss: 0.6927\n",
            "Epoch [ 2/100], Step [ 40/ 78], Loss: 0.6897\n",
            "Epoch [ 2/100], Step [ 41/ 78], Loss: 0.6903\n",
            "Epoch [ 2/100], Step [ 42/ 78], Loss: 0.6905\n",
            "Epoch [ 2/100], Step [ 43/ 78], Loss: 0.6904\n",
            "Epoch [ 2/100], Step [ 44/ 78], Loss: 0.6913\n",
            "Epoch [ 2/100], Step [ 45/ 78], Loss: 0.6910\n",
            "Epoch [ 2/100], Step [ 46/ 78], Loss: 0.6890\n",
            "Epoch [ 2/100], Step [ 47/ 78], Loss: 0.6897\n",
            "Epoch [ 2/100], Step [ 48/ 78], Loss: 0.6915\n",
            "Epoch [ 2/100], Step [ 49/ 78], Loss: 0.6913\n",
            "Epoch [ 2/100], Step [ 50/ 78], Loss: 0.6904\n",
            "Epoch [ 2/100], Step [ 51/ 78], Loss: 0.6913\n",
            "Epoch [ 2/100], Step [ 52/ 78], Loss: 0.6895\n",
            "Epoch [ 2/100], Step [ 53/ 78], Loss: 0.6874\n",
            "Epoch [ 2/100], Step [ 54/ 78], Loss: 0.6872\n",
            "Epoch [ 2/100], Step [ 55/ 78], Loss: 0.6880\n",
            "Epoch [ 2/100], Step [ 56/ 78], Loss: 0.6908\n",
            "Epoch [ 2/100], Step [ 57/ 78], Loss: 0.6896\n",
            "Epoch [ 2/100], Step [ 58/ 78], Loss: 0.6893\n",
            "Epoch [ 2/100], Step [ 59/ 78], Loss: 0.6893\n",
            "Epoch [ 2/100], Step [ 60/ 78], Loss: 0.6884\n",
            "Epoch [ 2/100], Step [ 61/ 78], Loss: 0.6893\n",
            "Epoch [ 2/100], Step [ 62/ 78], Loss: 0.6889\n",
            "Epoch [ 2/100], Step [ 63/ 78], Loss: 0.6924\n",
            "Epoch [ 3/100], Step [  1/ 78], Loss: 0.6897\n",
            "Epoch [ 3/100], Step [  2/ 78], Loss: 0.6887\n",
            "Epoch [ 3/100], Step [  3/ 78], Loss: 0.6917\n",
            "Epoch [ 3/100], Step [  4/ 78], Loss: 0.6883\n",
            "Epoch [ 3/100], Step [  5/ 78], Loss: 0.6865\n",
            "Epoch [ 3/100], Step [  6/ 78], Loss: 0.6875\n",
            "Epoch [ 3/100], Step [  7/ 78], Loss: 0.6895\n",
            "Epoch [ 3/100], Step [  8/ 78], Loss: 0.6894\n",
            "Epoch [ 3/100], Step [  9/ 78], Loss: 0.6877\n",
            "Epoch [ 3/100], Step [ 10/ 78], Loss: 0.6877\n",
            "Epoch [ 3/100], Step [ 11/ 78], Loss: 0.6888\n",
            "Epoch [ 3/100], Step [ 12/ 78], Loss: 0.6908\n",
            "Epoch [ 3/100], Step [ 13/ 78], Loss: 0.6860\n",
            "Epoch [ 3/100], Step [ 14/ 78], Loss: 0.6887\n",
            "Epoch [ 3/100], Step [ 15/ 78], Loss: 0.6878\n",
            "Epoch [ 3/100], Step [ 16/ 78], Loss: 0.6869\n",
            "Epoch [ 3/100], Step [ 17/ 78], Loss: 0.6865\n",
            "Epoch [ 3/100], Step [ 18/ 78], Loss: 0.6897\n",
            "Epoch [ 3/100], Step [ 19/ 78], Loss: 0.6896\n",
            "Epoch [ 3/100], Step [ 20/ 78], Loss: 0.6901\n",
            "Epoch [ 3/100], Step [ 21/ 78], Loss: 0.6904\n",
            "Epoch [ 3/100], Step [ 22/ 78], Loss: 0.6852\n",
            "Epoch [ 3/100], Step [ 23/ 78], Loss: 0.6871\n",
            "Epoch [ 3/100], Step [ 24/ 78], Loss: 0.6895\n",
            "Epoch [ 3/100], Step [ 25/ 78], Loss: 0.6852\n",
            "Epoch [ 3/100], Step [ 26/ 78], Loss: 0.6878\n",
            "Epoch [ 3/100], Step [ 27/ 78], Loss: 0.6860\n",
            "Epoch [ 3/100], Step [ 28/ 78], Loss: 0.6892\n",
            "Epoch [ 3/100], Step [ 29/ 78], Loss: 0.6877\n",
            "Epoch [ 3/100], Step [ 30/ 78], Loss: 0.6900\n",
            "Epoch [ 3/100], Step [ 31/ 78], Loss: 0.6864\n",
            "Epoch [ 3/100], Step [ 32/ 78], Loss: 0.6892\n",
            "Epoch [ 3/100], Step [ 33/ 78], Loss: 0.6879\n",
            "Epoch [ 3/100], Step [ 34/ 78], Loss: 0.6859\n",
            "Epoch [ 3/100], Step [ 35/ 78], Loss: 0.6889\n",
            "Epoch [ 3/100], Step [ 36/ 78], Loss: 0.6863\n",
            "Epoch [ 3/100], Step [ 37/ 78], Loss: 0.6872\n",
            "Epoch [ 3/100], Step [ 38/ 78], Loss: 0.6857\n",
            "Epoch [ 3/100], Step [ 39/ 78], Loss: 0.6841\n",
            "Epoch [ 3/100], Step [ 40/ 78], Loss: 0.6883\n",
            "Epoch [ 3/100], Step [ 41/ 78], Loss: 0.6870\n",
            "Epoch [ 3/100], Step [ 42/ 78], Loss: 0.6893\n",
            "Epoch [ 3/100], Step [ 43/ 78], Loss: 0.6890\n",
            "Epoch [ 3/100], Step [ 44/ 78], Loss: 0.6852\n",
            "Epoch [ 3/100], Step [ 45/ 78], Loss: 0.6862\n",
            "Epoch [ 3/100], Step [ 46/ 78], Loss: 0.6869\n",
            "Epoch [ 3/100], Step [ 47/ 78], Loss: 0.6859\n",
            "Epoch [ 3/100], Step [ 48/ 78], Loss: 0.6860\n",
            "Epoch [ 3/100], Step [ 49/ 78], Loss: 0.6861\n",
            "Epoch [ 3/100], Step [ 50/ 78], Loss: 0.6870\n",
            "Epoch [ 3/100], Step [ 51/ 78], Loss: 0.6889\n",
            "Epoch [ 3/100], Step [ 52/ 78], Loss: 0.6953\n",
            "Epoch [ 3/100], Step [ 53/ 78], Loss: 0.6885\n",
            "Epoch [ 3/100], Step [ 54/ 78], Loss: 0.6869\n",
            "Epoch [ 3/100], Step [ 55/ 78], Loss: 0.6875\n",
            "Epoch [ 3/100], Step [ 56/ 78], Loss: 0.6875\n",
            "Epoch [ 3/100], Step [ 57/ 78], Loss: 0.6924\n",
            "Epoch [ 3/100], Step [ 58/ 78], Loss: 0.6874\n",
            "Epoch [ 3/100], Step [ 59/ 78], Loss: 0.6917\n",
            "Epoch [ 3/100], Step [ 60/ 78], Loss: 0.6836\n",
            "Epoch [ 3/100], Step [ 61/ 78], Loss: 0.6916\n",
            "Epoch [ 3/100], Step [ 62/ 78], Loss: 0.6872\n",
            "Epoch [ 3/100], Step [ 63/ 78], Loss: 0.6899\n",
            "Epoch [ 4/100], Step [  1/ 78], Loss: 0.6870\n",
            "Epoch [ 4/100], Step [  2/ 78], Loss: 0.6874\n",
            "Epoch [ 4/100], Step [  3/ 78], Loss: 0.6870\n",
            "Epoch [ 4/100], Step [  4/ 78], Loss: 0.6832\n",
            "Epoch [ 4/100], Step [  5/ 78], Loss: 0.6871\n",
            "Epoch [ 4/100], Step [  6/ 78], Loss: 0.6874\n",
            "Epoch [ 4/100], Step [  7/ 78], Loss: 0.6836\n",
            "Epoch [ 4/100], Step [  8/ 78], Loss: 0.6865\n",
            "Epoch [ 4/100], Step [  9/ 78], Loss: 0.6861\n",
            "Epoch [ 4/100], Step [ 10/ 78], Loss: 0.6888\n",
            "Epoch [ 4/100], Step [ 11/ 78], Loss: 0.6884\n",
            "Epoch [ 4/100], Step [ 12/ 78], Loss: 0.6854\n",
            "Epoch [ 4/100], Step [ 13/ 78], Loss: 0.6860\n",
            "Epoch [ 4/100], Step [ 14/ 78], Loss: 0.6835\n",
            "Epoch [ 4/100], Step [ 15/ 78], Loss: 0.6872\n",
            "Epoch [ 4/100], Step [ 16/ 78], Loss: 0.6775\n",
            "Epoch [ 4/100], Step [ 17/ 78], Loss: 0.6859\n",
            "Epoch [ 4/100], Step [ 18/ 78], Loss: 0.6870\n",
            "Epoch [ 4/100], Step [ 19/ 78], Loss: 0.6857\n",
            "Epoch [ 4/100], Step [ 20/ 78], Loss: 0.6864\n",
            "Epoch [ 4/100], Step [ 21/ 78], Loss: 0.6866\n",
            "Epoch [ 4/100], Step [ 22/ 78], Loss: 0.6847\n",
            "Epoch [ 4/100], Step [ 23/ 78], Loss: 0.6890\n",
            "Epoch [ 4/100], Step [ 24/ 78], Loss: 0.6917\n",
            "Epoch [ 4/100], Step [ 25/ 78], Loss: 0.6902\n",
            "Epoch [ 4/100], Step [ 26/ 78], Loss: 0.6845\n",
            "Epoch [ 4/100], Step [ 27/ 78], Loss: 0.6853\n",
            "Epoch [ 4/100], Step [ 28/ 78], Loss: 0.6792\n",
            "Epoch [ 4/100], Step [ 29/ 78], Loss: 0.6857\n",
            "Epoch [ 4/100], Step [ 30/ 78], Loss: 0.6797\n",
            "Epoch [ 4/100], Step [ 31/ 78], Loss: 0.6810\n",
            "Epoch [ 4/100], Step [ 32/ 78], Loss: 0.6917\n",
            "Epoch [ 4/100], Step [ 33/ 78], Loss: 0.6839\n",
            "Epoch [ 4/100], Step [ 34/ 78], Loss: 0.6872\n",
            "Epoch [ 4/100], Step [ 35/ 78], Loss: 0.6828\n",
            "Epoch [ 4/100], Step [ 36/ 78], Loss: 0.6847\n",
            "Epoch [ 4/100], Step [ 37/ 78], Loss: 0.6850\n",
            "Epoch [ 4/100], Step [ 38/ 78], Loss: 0.6809\n",
            "Epoch [ 4/100], Step [ 39/ 78], Loss: 0.6827\n",
            "Epoch [ 4/100], Step [ 40/ 78], Loss: 0.6840\n",
            "Epoch [ 4/100], Step [ 41/ 78], Loss: 0.6848\n",
            "Epoch [ 4/100], Step [ 42/ 78], Loss: 0.6897\n",
            "Epoch [ 4/100], Step [ 43/ 78], Loss: 0.6828\n",
            "Epoch [ 4/100], Step [ 44/ 78], Loss: 0.6886\n",
            "Epoch [ 4/100], Step [ 45/ 78], Loss: 0.6896\n",
            "Epoch [ 4/100], Step [ 46/ 78], Loss: 0.6827\n",
            "Epoch [ 4/100], Step [ 47/ 78], Loss: 0.6873\n",
            "Epoch [ 4/100], Step [ 48/ 78], Loss: 0.6842\n",
            "Epoch [ 4/100], Step [ 49/ 78], Loss: 0.6904\n",
            "Epoch [ 4/100], Step [ 50/ 78], Loss: 0.6858\n",
            "Epoch [ 4/100], Step [ 51/ 78], Loss: 0.6775\n",
            "Epoch [ 4/100], Step [ 52/ 78], Loss: 0.6825\n",
            "Epoch [ 4/100], Step [ 53/ 78], Loss: 0.6767\n",
            "Epoch [ 4/100], Step [ 54/ 78], Loss: 0.6780\n",
            "Epoch [ 4/100], Step [ 55/ 78], Loss: 0.6854\n",
            "Epoch [ 4/100], Step [ 56/ 78], Loss: 0.6829\n",
            "Epoch [ 4/100], Step [ 57/ 78], Loss: 0.6817\n",
            "Epoch [ 4/100], Step [ 58/ 78], Loss: 0.6853\n",
            "Epoch [ 4/100], Step [ 59/ 78], Loss: 0.6852\n",
            "Epoch [ 4/100], Step [ 60/ 78], Loss: 0.6913\n",
            "Epoch [ 4/100], Step [ 61/ 78], Loss: 0.6834\n",
            "Epoch [ 4/100], Step [ 62/ 78], Loss: 0.6912\n",
            "Epoch [ 4/100], Step [ 63/ 78], Loss: 0.6926\n",
            "Epoch [ 5/100], Step [  1/ 78], Loss: 0.6854\n",
            "Epoch [ 5/100], Step [  2/ 78], Loss: 0.6835\n",
            "Epoch [ 5/100], Step [  3/ 78], Loss: 0.6928\n",
            "Epoch [ 5/100], Step [  4/ 78], Loss: 0.6887\n",
            "Epoch [ 5/100], Step [  5/ 78], Loss: 0.6784\n",
            "Epoch [ 5/100], Step [  6/ 78], Loss: 0.6824\n",
            "Epoch [ 5/100], Step [  7/ 78], Loss: 0.6823\n",
            "Epoch [ 5/100], Step [  8/ 78], Loss: 0.6848\n",
            "Epoch [ 5/100], Step [  9/ 78], Loss: 0.6866\n",
            "Epoch [ 5/100], Step [ 10/ 78], Loss: 0.6813\n",
            "Epoch [ 5/100], Step [ 11/ 78], Loss: 0.6796\n",
            "Epoch [ 5/100], Step [ 12/ 78], Loss: 0.6786\n",
            "Epoch [ 5/100], Step [ 13/ 78], Loss: 0.6898\n",
            "Epoch [ 5/100], Step [ 14/ 78], Loss: 0.6829\n",
            "Epoch [ 5/100], Step [ 15/ 78], Loss: 0.6848\n",
            "Epoch [ 5/100], Step [ 16/ 78], Loss: 0.6816\n",
            "Epoch [ 5/100], Step [ 17/ 78], Loss: 0.6808\n",
            "Epoch [ 5/100], Step [ 18/ 78], Loss: 0.6774\n",
            "Epoch [ 5/100], Step [ 19/ 78], Loss: 0.6837\n",
            "Epoch [ 5/100], Step [ 20/ 78], Loss: 0.6841\n",
            "Epoch [ 5/100], Step [ 21/ 78], Loss: 0.6851\n",
            "Epoch [ 5/100], Step [ 22/ 78], Loss: 0.6861\n",
            "Epoch [ 5/100], Step [ 23/ 78], Loss: 0.6861\n",
            "Epoch [ 5/100], Step [ 24/ 78], Loss: 0.6855\n",
            "Epoch [ 5/100], Step [ 25/ 78], Loss: 0.6768\n",
            "Epoch [ 5/100], Step [ 26/ 78], Loss: 0.6824\n",
            "Epoch [ 5/100], Step [ 27/ 78], Loss: 0.6765\n",
            "Epoch [ 5/100], Step [ 28/ 78], Loss: 0.6871\n",
            "Epoch [ 5/100], Step [ 29/ 78], Loss: 0.6793\n",
            "Epoch [ 5/100], Step [ 30/ 78], Loss: 0.6816\n",
            "Epoch [ 5/100], Step [ 31/ 78], Loss: 0.6860\n",
            "Epoch [ 5/100], Step [ 32/ 78], Loss: 0.6717\n",
            "Epoch [ 5/100], Step [ 33/ 78], Loss: 0.6814\n",
            "Epoch [ 5/100], Step [ 34/ 78], Loss: 0.6801\n",
            "Epoch [ 5/100], Step [ 35/ 78], Loss: 0.6757\n",
            "Epoch [ 5/100], Step [ 36/ 78], Loss: 0.6821\n",
            "Epoch [ 5/100], Step [ 37/ 78], Loss: 0.6848\n",
            "Epoch [ 5/100], Step [ 38/ 78], Loss: 0.6873\n",
            "Epoch [ 5/100], Step [ 39/ 78], Loss: 0.6814\n",
            "Epoch [ 5/100], Step [ 40/ 78], Loss: 0.6817\n",
            "Epoch [ 5/100], Step [ 41/ 78], Loss: 0.6912\n",
            "Epoch [ 5/100], Step [ 42/ 78], Loss: 0.6846\n",
            "Epoch [ 5/100], Step [ 43/ 78], Loss: 0.6913\n",
            "Epoch [ 5/100], Step [ 44/ 78], Loss: 0.6777\n",
            "Epoch [ 5/100], Step [ 45/ 78], Loss: 0.6799\n",
            "Epoch [ 5/100], Step [ 46/ 78], Loss: 0.6855\n",
            "Epoch [ 5/100], Step [ 47/ 78], Loss: 0.6787\n",
            "Epoch [ 5/100], Step [ 48/ 78], Loss: 0.6850\n",
            "Epoch [ 5/100], Step [ 49/ 78], Loss: 0.6876\n",
            "Epoch [ 5/100], Step [ 50/ 78], Loss: 0.6728\n",
            "Epoch [ 5/100], Step [ 51/ 78], Loss: 0.6810\n",
            "Epoch [ 5/100], Step [ 52/ 78], Loss: 0.6853\n",
            "Epoch [ 5/100], Step [ 53/ 78], Loss: 0.6801\n",
            "Epoch [ 5/100], Step [ 54/ 78], Loss: 0.6807\n",
            "Epoch [ 5/100], Step [ 55/ 78], Loss: 0.6759\n",
            "Epoch [ 5/100], Step [ 56/ 78], Loss: 0.6778\n",
            "Epoch [ 5/100], Step [ 57/ 78], Loss: 0.6839\n",
            "Epoch [ 5/100], Step [ 58/ 78], Loss: 0.6768\n",
            "Epoch [ 5/100], Step [ 59/ 78], Loss: 0.6815\n",
            "Epoch [ 5/100], Step [ 60/ 78], Loss: 0.6834\n",
            "Epoch [ 5/100], Step [ 61/ 78], Loss: 0.6756\n",
            "Epoch [ 5/100], Step [ 62/ 78], Loss: 0.6839\n",
            "Epoch [ 5/100], Step [ 63/ 78], Loss: 0.6853\n",
            "Epoch [ 6/100], Step [  1/ 78], Loss: 0.6744\n",
            "Epoch [ 6/100], Step [  2/ 78], Loss: 0.6847\n",
            "Epoch [ 6/100], Step [  3/ 78], Loss: 0.6782\n",
            "Epoch [ 6/100], Step [  4/ 78], Loss: 0.6725\n",
            "Epoch [ 6/100], Step [  5/ 78], Loss: 0.6882\n",
            "Epoch [ 6/100], Step [  6/ 78], Loss: 0.6863\n",
            "Epoch [ 6/100], Step [  7/ 78], Loss: 0.6848\n",
            "Epoch [ 6/100], Step [  8/ 78], Loss: 0.6773\n",
            "Epoch [ 6/100], Step [  9/ 78], Loss: 0.6834\n",
            "Epoch [ 6/100], Step [ 10/ 78], Loss: 0.6773\n",
            "Epoch [ 6/100], Step [ 11/ 78], Loss: 0.6887\n",
            "Epoch [ 6/100], Step [ 12/ 78], Loss: 0.6779\n",
            "Epoch [ 6/100], Step [ 13/ 78], Loss: 0.6848\n",
            "Epoch [ 6/100], Step [ 14/ 78], Loss: 0.6819\n",
            "Epoch [ 6/100], Step [ 15/ 78], Loss: 0.6811\n",
            "Epoch [ 6/100], Step [ 16/ 78], Loss: 0.6811\n",
            "Epoch [ 6/100], Step [ 17/ 78], Loss: 0.6759\n",
            "Epoch [ 6/100], Step [ 18/ 78], Loss: 0.6832\n",
            "Epoch [ 6/100], Step [ 19/ 78], Loss: 0.6777\n",
            "Epoch [ 6/100], Step [ 20/ 78], Loss: 0.6855\n",
            "Epoch [ 6/100], Step [ 21/ 78], Loss: 0.6830\n",
            "Epoch [ 6/100], Step [ 22/ 78], Loss: 0.6772\n",
            "Epoch [ 6/100], Step [ 23/ 78], Loss: 0.6874\n",
            "Epoch [ 6/100], Step [ 24/ 78], Loss: 0.6798\n",
            "Epoch [ 6/100], Step [ 25/ 78], Loss: 0.6815\n",
            "Epoch [ 6/100], Step [ 26/ 78], Loss: 0.6801\n",
            "Epoch [ 6/100], Step [ 27/ 78], Loss: 0.6815\n",
            "Epoch [ 6/100], Step [ 28/ 78], Loss: 0.6876\n",
            "Epoch [ 6/100], Step [ 29/ 78], Loss: 0.6775\n",
            "Epoch [ 6/100], Step [ 30/ 78], Loss: 0.6759\n",
            "Epoch [ 6/100], Step [ 31/ 78], Loss: 0.6802\n",
            "Epoch [ 6/100], Step [ 32/ 78], Loss: 0.6799\n",
            "Epoch [ 6/100], Step [ 33/ 78], Loss: 0.6848\n",
            "Epoch [ 6/100], Step [ 34/ 78], Loss: 0.6699\n",
            "Epoch [ 6/100], Step [ 35/ 78], Loss: 0.6705\n",
            "Epoch [ 6/100], Step [ 36/ 78], Loss: 0.6753\n",
            "Epoch [ 6/100], Step [ 37/ 78], Loss: 0.6751\n",
            "Epoch [ 6/100], Step [ 38/ 78], Loss: 0.6839\n",
            "Epoch [ 6/100], Step [ 39/ 78], Loss: 0.6824\n",
            "Epoch [ 6/100], Step [ 40/ 78], Loss: 0.6849\n",
            "Epoch [ 6/100], Step [ 41/ 78], Loss: 0.6911\n",
            "Epoch [ 6/100], Step [ 42/ 78], Loss: 0.6833\n",
            "Epoch [ 6/100], Step [ 43/ 78], Loss: 0.6701\n",
            "Epoch [ 6/100], Step [ 44/ 78], Loss: 0.6823\n",
            "Epoch [ 6/100], Step [ 45/ 78], Loss: 0.6756\n",
            "Epoch [ 6/100], Step [ 46/ 78], Loss: 0.6806\n",
            "Epoch [ 6/100], Step [ 47/ 78], Loss: 0.6707\n",
            "Epoch [ 6/100], Step [ 48/ 78], Loss: 0.6840\n",
            "Epoch [ 6/100], Step [ 49/ 78], Loss: 0.6747\n",
            "Epoch [ 6/100], Step [ 50/ 78], Loss: 0.6645\n",
            "Epoch [ 6/100], Step [ 51/ 78], Loss: 0.6778\n",
            "Epoch [ 6/100], Step [ 52/ 78], Loss: 0.6767\n",
            "Epoch [ 6/100], Step [ 53/ 78], Loss: 0.6752\n",
            "Epoch [ 6/100], Step [ 54/ 78], Loss: 0.6781\n",
            "Epoch [ 6/100], Step [ 55/ 78], Loss: 0.6800\n",
            "Epoch [ 6/100], Step [ 56/ 78], Loss: 0.6805\n",
            "Epoch [ 6/100], Step [ 57/ 78], Loss: 0.6721\n",
            "Epoch [ 6/100], Step [ 58/ 78], Loss: 0.6773\n",
            "Epoch [ 6/100], Step [ 59/ 78], Loss: 0.6830\n",
            "Epoch [ 6/100], Step [ 60/ 78], Loss: 0.6912\n",
            "Epoch [ 6/100], Step [ 61/ 78], Loss: 0.6680\n",
            "Epoch [ 6/100], Step [ 62/ 78], Loss: 0.6706\n",
            "Epoch [ 6/100], Step [ 63/ 78], Loss: 0.6808\n",
            "Epoch [ 7/100], Step [  1/ 78], Loss: 0.6833\n",
            "Epoch [ 7/100], Step [  2/ 78], Loss: 0.6851\n",
            "Epoch [ 7/100], Step [  3/ 78], Loss: 0.6776\n",
            "Epoch [ 7/100], Step [  4/ 78], Loss: 0.6794\n",
            "Epoch [ 7/100], Step [  5/ 78], Loss: 0.6817\n",
            "Epoch [ 7/100], Step [  6/ 78], Loss: 0.6767\n",
            "Epoch [ 7/100], Step [  7/ 78], Loss: 0.6794\n",
            "Epoch [ 7/100], Step [  8/ 78], Loss: 0.6765\n",
            "Epoch [ 7/100], Step [  9/ 78], Loss: 0.6794\n",
            "Epoch [ 7/100], Step [ 10/ 78], Loss: 0.6678\n",
            "Epoch [ 7/100], Step [ 11/ 78], Loss: 0.6879\n",
            "Epoch [ 7/100], Step [ 12/ 78], Loss: 0.6807\n",
            "Epoch [ 7/100], Step [ 13/ 78], Loss: 0.6746\n",
            "Epoch [ 7/100], Step [ 14/ 78], Loss: 0.6959\n",
            "Epoch [ 7/100], Step [ 15/ 78], Loss: 0.6593\n",
            "Epoch [ 7/100], Step [ 16/ 78], Loss: 0.6695\n",
            "Epoch [ 7/100], Step [ 17/ 78], Loss: 0.6719\n",
            "Epoch [ 7/100], Step [ 18/ 78], Loss: 0.6766\n",
            "Epoch [ 7/100], Step [ 19/ 78], Loss: 0.6783\n",
            "Epoch [ 7/100], Step [ 20/ 78], Loss: 0.6711\n",
            "Epoch [ 7/100], Step [ 21/ 78], Loss: 0.6680\n",
            "Epoch [ 7/100], Step [ 22/ 78], Loss: 0.6749\n",
            "Epoch [ 7/100], Step [ 23/ 78], Loss: 0.6673\n",
            "Epoch [ 7/100], Step [ 24/ 78], Loss: 0.6740\n",
            "Epoch [ 7/100], Step [ 25/ 78], Loss: 0.6762\n",
            "Epoch [ 7/100], Step [ 26/ 78], Loss: 0.6861\n",
            "Epoch [ 7/100], Step [ 27/ 78], Loss: 0.6798\n",
            "Epoch [ 7/100], Step [ 28/ 78], Loss: 0.6731\n",
            "Epoch [ 7/100], Step [ 29/ 78], Loss: 0.6719\n",
            "Epoch [ 7/100], Step [ 30/ 78], Loss: 0.6732\n",
            "Epoch [ 7/100], Step [ 31/ 78], Loss: 0.6853\n",
            "Epoch [ 7/100], Step [ 32/ 78], Loss: 0.6677\n",
            "Epoch [ 7/100], Step [ 33/ 78], Loss: 0.6810\n",
            "Epoch [ 7/100], Step [ 34/ 78], Loss: 0.6706\n",
            "Epoch [ 7/100], Step [ 35/ 78], Loss: 0.6703\n",
            "Epoch [ 7/100], Step [ 36/ 78], Loss: 0.6696\n",
            "Epoch [ 7/100], Step [ 37/ 78], Loss: 0.6761\n",
            "Epoch [ 7/100], Step [ 38/ 78], Loss: 0.6788\n",
            "Epoch [ 7/100], Step [ 39/ 78], Loss: 0.6822\n",
            "Epoch [ 7/100], Step [ 40/ 78], Loss: 0.6706\n",
            "Epoch [ 7/100], Step [ 41/ 78], Loss: 0.6730\n",
            "Epoch [ 7/100], Step [ 42/ 78], Loss: 0.6816\n",
            "Epoch [ 7/100], Step [ 43/ 78], Loss: 0.6704\n",
            "Epoch [ 7/100], Step [ 44/ 78], Loss: 0.6778\n",
            "Epoch [ 7/100], Step [ 45/ 78], Loss: 0.6770\n",
            "Epoch [ 7/100], Step [ 46/ 78], Loss: 0.6815\n",
            "Epoch [ 7/100], Step [ 47/ 78], Loss: 0.6714\n",
            "Epoch [ 7/100], Step [ 48/ 78], Loss: 0.6706\n",
            "Epoch [ 7/100], Step [ 49/ 78], Loss: 0.6789\n",
            "Epoch [ 7/100], Step [ 50/ 78], Loss: 0.6921\n",
            "Epoch [ 7/100], Step [ 51/ 78], Loss: 0.6675\n",
            "Epoch [ 7/100], Step [ 52/ 78], Loss: 0.6733\n",
            "Epoch [ 7/100], Step [ 53/ 78], Loss: 0.6762\n",
            "Epoch [ 7/100], Step [ 54/ 78], Loss: 0.6941\n",
            "Epoch [ 7/100], Step [ 55/ 78], Loss: 0.6801\n",
            "Epoch [ 7/100], Step [ 56/ 78], Loss: 0.6624\n",
            "Epoch [ 7/100], Step [ 57/ 78], Loss: 0.6732\n",
            "Epoch [ 7/100], Step [ 58/ 78], Loss: 0.6817\n",
            "Epoch [ 7/100], Step [ 59/ 78], Loss: 0.6852\n",
            "Epoch [ 7/100], Step [ 60/ 78], Loss: 0.6704\n",
            "Epoch [ 7/100], Step [ 61/ 78], Loss: 0.6862\n",
            "Epoch [ 7/100], Step [ 62/ 78], Loss: 0.6723\n",
            "Epoch [ 7/100], Step [ 63/ 78], Loss: 0.6887\n",
            "Epoch [ 8/100], Step [  1/ 78], Loss: 0.6773\n",
            "Epoch [ 8/100], Step [  2/ 78], Loss: 0.6720\n",
            "Epoch [ 8/100], Step [  3/ 78], Loss: 0.6736\n",
            "Epoch [ 8/100], Step [  4/ 78], Loss: 0.6823\n",
            "Epoch [ 8/100], Step [  5/ 78], Loss: 0.7002\n",
            "Epoch [ 8/100], Step [  6/ 78], Loss: 0.6773\n",
            "Epoch [ 8/100], Step [  7/ 78], Loss: 0.6821\n",
            "Epoch [ 8/100], Step [  8/ 78], Loss: 0.6619\n",
            "Epoch [ 8/100], Step [  9/ 78], Loss: 0.6595\n",
            "Epoch [ 8/100], Step [ 10/ 78], Loss: 0.6697\n",
            "Epoch [ 8/100], Step [ 11/ 78], Loss: 0.6831\n",
            "Epoch [ 8/100], Step [ 12/ 78], Loss: 0.6835\n",
            "Epoch [ 8/100], Step [ 13/ 78], Loss: 0.6654\n",
            "Epoch [ 8/100], Step [ 14/ 78], Loss: 0.6723\n",
            "Epoch [ 8/100], Step [ 15/ 78], Loss: 0.6864\n",
            "Epoch [ 8/100], Step [ 16/ 78], Loss: 0.6665\n",
            "Epoch [ 8/100], Step [ 17/ 78], Loss: 0.6580\n",
            "Epoch [ 8/100], Step [ 18/ 78], Loss: 0.6805\n",
            "Epoch [ 8/100], Step [ 19/ 78], Loss: 0.6680\n",
            "Epoch [ 8/100], Step [ 20/ 78], Loss: 0.6807\n",
            "Epoch [ 8/100], Step [ 21/ 78], Loss: 0.6710\n",
            "Epoch [ 8/100], Step [ 22/ 78], Loss: 0.6783\n",
            "Epoch [ 8/100], Step [ 23/ 78], Loss: 0.6862\n",
            "Epoch [ 8/100], Step [ 24/ 78], Loss: 0.6806\n",
            "Epoch [ 8/100], Step [ 25/ 78], Loss: 0.6760\n",
            "Epoch [ 8/100], Step [ 26/ 78], Loss: 0.6769\n",
            "Epoch [ 8/100], Step [ 27/ 78], Loss: 0.6652\n",
            "Epoch [ 8/100], Step [ 28/ 78], Loss: 0.6564\n",
            "Epoch [ 8/100], Step [ 29/ 78], Loss: 0.6718\n",
            "Epoch [ 8/100], Step [ 30/ 78], Loss: 0.6705\n",
            "Epoch [ 8/100], Step [ 31/ 78], Loss: 0.6720\n",
            "Epoch [ 8/100], Step [ 32/ 78], Loss: 0.6683\n",
            "Epoch [ 8/100], Step [ 33/ 78], Loss: 0.6799\n",
            "Epoch [ 8/100], Step [ 34/ 78], Loss: 0.6735\n",
            "Epoch [ 8/100], Step [ 35/ 78], Loss: 0.6835\n",
            "Epoch [ 8/100], Step [ 36/ 78], Loss: 0.6721\n",
            "Epoch [ 8/100], Step [ 37/ 78], Loss: 0.6746\n",
            "Epoch [ 8/100], Step [ 38/ 78], Loss: 0.6642\n",
            "Epoch [ 8/100], Step [ 39/ 78], Loss: 0.6774\n",
            "Epoch [ 8/100], Step [ 40/ 78], Loss: 0.6657\n",
            "Epoch [ 8/100], Step [ 41/ 78], Loss: 0.6755\n",
            "Epoch [ 8/100], Step [ 42/ 78], Loss: 0.6667\n",
            "Epoch [ 8/100], Step [ 43/ 78], Loss: 0.6837\n",
            "Epoch [ 8/100], Step [ 44/ 78], Loss: 0.6738\n",
            "Epoch [ 8/100], Step [ 45/ 78], Loss: 0.6677\n",
            "Epoch [ 8/100], Step [ 46/ 78], Loss: 0.6696\n",
            "Epoch [ 8/100], Step [ 47/ 78], Loss: 0.6789\n",
            "Epoch [ 8/100], Step [ 48/ 78], Loss: 0.6709\n",
            "Epoch [ 8/100], Step [ 49/ 78], Loss: 0.6640\n",
            "Epoch [ 8/100], Step [ 50/ 78], Loss: 0.6581\n",
            "Epoch [ 8/100], Step [ 51/ 78], Loss: 0.6884\n",
            "Epoch [ 8/100], Step [ 52/ 78], Loss: 0.6676\n",
            "Epoch [ 8/100], Step [ 53/ 78], Loss: 0.6802\n",
            "Epoch [ 8/100], Step [ 54/ 78], Loss: 0.6732\n",
            "Epoch [ 8/100], Step [ 55/ 78], Loss: 0.6682\n",
            "Epoch [ 8/100], Step [ 56/ 78], Loss: 0.6635\n",
            "Epoch [ 8/100], Step [ 57/ 78], Loss: 0.6755\n",
            "Epoch [ 8/100], Step [ 58/ 78], Loss: 0.6672\n",
            "Epoch [ 8/100], Step [ 59/ 78], Loss: 0.6702\n",
            "Epoch [ 8/100], Step [ 60/ 78], Loss: 0.6772\n",
            "Epoch [ 8/100], Step [ 61/ 78], Loss: 0.6769\n",
            "Epoch [ 8/100], Step [ 62/ 78], Loss: 0.6676\n",
            "Epoch [ 8/100], Step [ 63/ 78], Loss: 0.6768\n",
            "Epoch [ 9/100], Step [  1/ 78], Loss: 0.6733\n",
            "Epoch [ 9/100], Step [  2/ 78], Loss: 0.6732\n",
            "Epoch [ 9/100], Step [  3/ 78], Loss: 0.6779\n",
            "Epoch [ 9/100], Step [  4/ 78], Loss: 0.6752\n",
            "Epoch [ 9/100], Step [  5/ 78], Loss: 0.6706\n",
            "Epoch [ 9/100], Step [  6/ 78], Loss: 0.6762\n",
            "Epoch [ 9/100], Step [  7/ 78], Loss: 0.6802\n",
            "Epoch [ 9/100], Step [  8/ 78], Loss: 0.6700\n",
            "Epoch [ 9/100], Step [  9/ 78], Loss: 0.6733\n",
            "Epoch [ 9/100], Step [ 10/ 78], Loss: 0.6687\n",
            "Epoch [ 9/100], Step [ 11/ 78], Loss: 0.6741\n",
            "Epoch [ 9/100], Step [ 12/ 78], Loss: 0.6828\n",
            "Epoch [ 9/100], Step [ 13/ 78], Loss: 0.6663\n",
            "Epoch [ 9/100], Step [ 14/ 78], Loss: 0.6843\n",
            "Epoch [ 9/100], Step [ 15/ 78], Loss: 0.6701\n",
            "Epoch [ 9/100], Step [ 16/ 78], Loss: 0.6688\n",
            "Epoch [ 9/100], Step [ 17/ 78], Loss: 0.6674\n",
            "Epoch [ 9/100], Step [ 18/ 78], Loss: 0.6706\n",
            "Epoch [ 9/100], Step [ 19/ 78], Loss: 0.6684\n",
            "Epoch [ 9/100], Step [ 20/ 78], Loss: 0.6495\n",
            "Epoch [ 9/100], Step [ 21/ 78], Loss: 0.6567\n",
            "Epoch [ 9/100], Step [ 22/ 78], Loss: 0.6777\n",
            "Epoch [ 9/100], Step [ 23/ 78], Loss: 0.6585\n",
            "Epoch [ 9/100], Step [ 24/ 78], Loss: 0.6630\n",
            "Epoch [ 9/100], Step [ 25/ 78], Loss: 0.6677\n",
            "Epoch [ 9/100], Step [ 26/ 78], Loss: 0.6649\n",
            "Epoch [ 9/100], Step [ 27/ 78], Loss: 0.6735\n",
            "Epoch [ 9/100], Step [ 28/ 78], Loss: 0.6569\n",
            "Epoch [ 9/100], Step [ 29/ 78], Loss: 0.6741\n",
            "Epoch [ 9/100], Step [ 30/ 78], Loss: 0.6543\n",
            "Epoch [ 9/100], Step [ 31/ 78], Loss: 0.6685\n",
            "Epoch [ 9/100], Step [ 32/ 78], Loss: 0.6630\n",
            "Epoch [ 9/100], Step [ 33/ 78], Loss: 0.6711\n",
            "Epoch [ 9/100], Step [ 34/ 78], Loss: 0.6735\n",
            "Epoch [ 9/100], Step [ 35/ 78], Loss: 0.6741\n",
            "Epoch [ 9/100], Step [ 36/ 78], Loss: 0.6659\n",
            "Epoch [ 9/100], Step [ 37/ 78], Loss: 0.6846\n",
            "Epoch [ 9/100], Step [ 38/ 78], Loss: 0.6587\n",
            "Epoch [ 9/100], Step [ 39/ 78], Loss: 0.6654\n",
            "Epoch [ 9/100], Step [ 40/ 78], Loss: 0.6743\n",
            "Epoch [ 9/100], Step [ 41/ 78], Loss: 0.6720\n",
            "Epoch [ 9/100], Step [ 42/ 78], Loss: 0.6815\n",
            "Epoch [ 9/100], Step [ 43/ 78], Loss: 0.6742\n",
            "Epoch [ 9/100], Step [ 44/ 78], Loss: 0.6801\n",
            "Epoch [ 9/100], Step [ 45/ 78], Loss: 0.6575\n",
            "Epoch [ 9/100], Step [ 46/ 78], Loss: 0.6781\n",
            "Epoch [ 9/100], Step [ 47/ 78], Loss: 0.6748\n",
            "Epoch [ 9/100], Step [ 48/ 78], Loss: 0.6675\n",
            "Epoch [ 9/100], Step [ 49/ 78], Loss: 0.6633\n",
            "Epoch [ 9/100], Step [ 50/ 78], Loss: 0.6588\n",
            "Epoch [ 9/100], Step [ 51/ 78], Loss: 0.6670\n",
            "Epoch [ 9/100], Step [ 52/ 78], Loss: 0.6850\n",
            "Epoch [ 9/100], Step [ 53/ 78], Loss: 0.6807\n",
            "Epoch [ 9/100], Step [ 54/ 78], Loss: 0.6738\n",
            "Epoch [ 9/100], Step [ 55/ 78], Loss: 0.6698\n",
            "Epoch [ 9/100], Step [ 56/ 78], Loss: 0.6613\n",
            "Epoch [ 9/100], Step [ 57/ 78], Loss: 0.6575\n",
            "Epoch [ 9/100], Step [ 58/ 78], Loss: 0.6605\n",
            "Epoch [ 9/100], Step [ 59/ 78], Loss: 0.6712\n",
            "Epoch [ 9/100], Step [ 60/ 78], Loss: 0.6542\n",
            "Epoch [ 9/100], Step [ 61/ 78], Loss: 0.6756\n",
            "Epoch [ 9/100], Step [ 62/ 78], Loss: 0.6578\n",
            "Epoch [ 9/100], Step [ 63/ 78], Loss: 0.6698\n",
            "Epoch [10/100], Step [  1/ 78], Loss: 0.6762\n",
            "Epoch [10/100], Step [  2/ 78], Loss: 0.6712\n",
            "Epoch [10/100], Step [  3/ 78], Loss: 0.6704\n",
            "Epoch [10/100], Step [  4/ 78], Loss: 0.6809\n",
            "Epoch [10/100], Step [  5/ 78], Loss: 0.6585\n",
            "Epoch [10/100], Step [  6/ 78], Loss: 0.6717\n",
            "Epoch [10/100], Step [  7/ 78], Loss: 0.6753\n",
            "Epoch [10/100], Step [  8/ 78], Loss: 0.6493\n",
            "Epoch [10/100], Step [  9/ 78], Loss: 0.6629\n",
            "Epoch [10/100], Step [ 10/ 78], Loss: 0.6662\n",
            "Epoch [10/100], Step [ 11/ 78], Loss: 0.6591\n",
            "Epoch [10/100], Step [ 12/ 78], Loss: 0.6698\n",
            "Epoch [10/100], Step [ 13/ 78], Loss: 0.6660\n",
            "Epoch [10/100], Step [ 14/ 78], Loss: 0.6676\n",
            "Epoch [10/100], Step [ 15/ 78], Loss: 0.6609\n",
            "Epoch [10/100], Step [ 16/ 78], Loss: 0.6569\n",
            "Epoch [10/100], Step [ 17/ 78], Loss: 0.6673\n",
            "Epoch [10/100], Step [ 18/ 78], Loss: 0.6639\n",
            "Epoch [10/100], Step [ 19/ 78], Loss: 0.6662\n",
            "Epoch [10/100], Step [ 20/ 78], Loss: 0.6606\n",
            "Epoch [10/100], Step [ 21/ 78], Loss: 0.6520\n",
            "Epoch [10/100], Step [ 22/ 78], Loss: 0.6706\n",
            "Epoch [10/100], Step [ 23/ 78], Loss: 0.6654\n",
            "Epoch [10/100], Step [ 24/ 78], Loss: 0.6799\n",
            "Epoch [10/100], Step [ 25/ 78], Loss: 0.6704\n",
            "Epoch [10/100], Step [ 26/ 78], Loss: 0.6517\n",
            "Epoch [10/100], Step [ 27/ 78], Loss: 0.6525\n",
            "Epoch [10/100], Step [ 28/ 78], Loss: 0.6495\n",
            "Epoch [10/100], Step [ 29/ 78], Loss: 0.6560\n",
            "Epoch [10/100], Step [ 30/ 78], Loss: 0.6576\n",
            "Epoch [10/100], Step [ 31/ 78], Loss: 0.6620\n",
            "Epoch [10/100], Step [ 32/ 78], Loss: 0.6648\n",
            "Epoch [10/100], Step [ 33/ 78], Loss: 0.6551\n",
            "Epoch [10/100], Step [ 34/ 78], Loss: 0.6748\n",
            "Epoch [10/100], Step [ 35/ 78], Loss: 0.6442\n",
            "Epoch [10/100], Step [ 36/ 78], Loss: 0.6720\n",
            "Epoch [10/100], Step [ 37/ 78], Loss: 0.6682\n",
            "Epoch [10/100], Step [ 38/ 78], Loss: 0.6598\n",
            "Epoch [10/100], Step [ 39/ 78], Loss: 0.6460\n",
            "Epoch [10/100], Step [ 40/ 78], Loss: 0.6587\n",
            "Epoch [10/100], Step [ 41/ 78], Loss: 0.6768\n",
            "Epoch [10/100], Step [ 42/ 78], Loss: 0.6710\n",
            "Epoch [10/100], Step [ 43/ 78], Loss: 0.6648\n",
            "Epoch [10/100], Step [ 44/ 78], Loss: 0.6451\n",
            "Epoch [10/100], Step [ 45/ 78], Loss: 0.6802\n",
            "Epoch [10/100], Step [ 46/ 78], Loss: 0.6475\n",
            "Epoch [10/100], Step [ 47/ 78], Loss: 0.6559\n",
            "Epoch [10/100], Step [ 48/ 78], Loss: 0.6657\n",
            "Epoch [10/100], Step [ 49/ 78], Loss: 0.6654\n",
            "Epoch [10/100], Step [ 50/ 78], Loss: 0.6746\n",
            "Epoch [10/100], Step [ 51/ 78], Loss: 0.6788\n",
            "Epoch [10/100], Step [ 52/ 78], Loss: 0.6515\n",
            "Epoch [10/100], Step [ 53/ 78], Loss: 0.6796\n",
            "Epoch [10/100], Step [ 54/ 78], Loss: 0.6765\n",
            "Epoch [10/100], Step [ 55/ 78], Loss: 0.6674\n",
            "Epoch [10/100], Step [ 56/ 78], Loss: 0.6699\n",
            "Epoch [10/100], Step [ 57/ 78], Loss: 0.6809\n",
            "Epoch [10/100], Step [ 58/ 78], Loss: 0.6826\n",
            "Epoch [10/100], Step [ 59/ 78], Loss: 0.6693\n",
            "Epoch [10/100], Step [ 60/ 78], Loss: 0.6707\n",
            "Epoch [10/100], Step [ 61/ 78], Loss: 0.6810\n",
            "Epoch [10/100], Step [ 62/ 78], Loss: 0.6584\n",
            "Epoch [10/100], Step [ 63/ 78], Loss: 0.6604\n",
            "Epoch [11/100], Step [  1/ 78], Loss: 0.6752\n",
            "Epoch [11/100], Step [  2/ 78], Loss: 0.6631\n",
            "Epoch [11/100], Step [  3/ 78], Loss: 0.6633\n",
            "Epoch [11/100], Step [  4/ 78], Loss: 0.6476\n",
            "Epoch [11/100], Step [  5/ 78], Loss: 0.6581\n",
            "Epoch [11/100], Step [  6/ 78], Loss: 0.6460\n",
            "Epoch [11/100], Step [  7/ 78], Loss: 0.6637\n",
            "Epoch [11/100], Step [  8/ 78], Loss: 0.6666\n",
            "Epoch [11/100], Step [  9/ 78], Loss: 0.6613\n",
            "Epoch [11/100], Step [ 10/ 78], Loss: 0.6703\n",
            "Epoch [11/100], Step [ 11/ 78], Loss: 0.6797\n",
            "Epoch [11/100], Step [ 12/ 78], Loss: 0.6628\n",
            "Epoch [11/100], Step [ 13/ 78], Loss: 0.6662\n",
            "Epoch [11/100], Step [ 14/ 78], Loss: 0.6642\n",
            "Epoch [11/100], Step [ 15/ 78], Loss: 0.6670\n",
            "Epoch [11/100], Step [ 16/ 78], Loss: 0.6534\n",
            "Epoch [11/100], Step [ 17/ 78], Loss: 0.6425\n",
            "Epoch [11/100], Step [ 18/ 78], Loss: 0.6702\n",
            "Epoch [11/100], Step [ 19/ 78], Loss: 0.6600\n",
            "Epoch [11/100], Step [ 20/ 78], Loss: 0.6579\n",
            "Epoch [11/100], Step [ 21/ 78], Loss: 0.6413\n",
            "Epoch [11/100], Step [ 22/ 78], Loss: 0.6577\n",
            "Epoch [11/100], Step [ 23/ 78], Loss: 0.6711\n",
            "Epoch [11/100], Step [ 24/ 78], Loss: 0.6615\n",
            "Epoch [11/100], Step [ 25/ 78], Loss: 0.6690\n",
            "Epoch [11/100], Step [ 26/ 78], Loss: 0.6747\n",
            "Epoch [11/100], Step [ 27/ 78], Loss: 0.6546\n",
            "Epoch [11/100], Step [ 28/ 78], Loss: 0.6739\n",
            "Epoch [11/100], Step [ 29/ 78], Loss: 0.6635\n",
            "Epoch [11/100], Step [ 30/ 78], Loss: 0.6575\n",
            "Epoch [11/100], Step [ 31/ 78], Loss: 0.6686\n",
            "Epoch [11/100], Step [ 32/ 78], Loss: 0.6588\n",
            "Epoch [11/100], Step [ 33/ 78], Loss: 0.6732\n",
            "Epoch [11/100], Step [ 34/ 78], Loss: 0.6651\n",
            "Epoch [11/100], Step [ 35/ 78], Loss: 0.6617\n",
            "Epoch [11/100], Step [ 36/ 78], Loss: 0.6734\n",
            "Epoch [11/100], Step [ 37/ 78], Loss: 0.6687\n",
            "Epoch [11/100], Step [ 38/ 78], Loss: 0.6482\n",
            "Epoch [11/100], Step [ 39/ 78], Loss: 0.6392\n",
            "Epoch [11/100], Step [ 40/ 78], Loss: 0.6670\n",
            "Epoch [11/100], Step [ 41/ 78], Loss: 0.6599\n",
            "Epoch [11/100], Step [ 42/ 78], Loss: 0.6561\n",
            "Epoch [11/100], Step [ 43/ 78], Loss: 0.6487\n",
            "Epoch [11/100], Step [ 44/ 78], Loss: 0.6728\n",
            "Epoch [11/100], Step [ 45/ 78], Loss: 0.6366\n",
            "Epoch [11/100], Step [ 46/ 78], Loss: 0.6461\n",
            "Epoch [11/100], Step [ 47/ 78], Loss: 0.6701\n",
            "Epoch [11/100], Step [ 48/ 78], Loss: 0.6993\n",
            "Epoch [11/100], Step [ 49/ 78], Loss: 0.6680\n",
            "Epoch [11/100], Step [ 50/ 78], Loss: 0.6658\n",
            "Epoch [11/100], Step [ 51/ 78], Loss: 0.6433\n",
            "Epoch [11/100], Step [ 52/ 78], Loss: 0.6609\n",
            "Epoch [11/100], Step [ 53/ 78], Loss: 0.6552\n",
            "Epoch [11/100], Step [ 54/ 78], Loss: 0.6549\n",
            "Epoch [11/100], Step [ 55/ 78], Loss: 0.6561\n",
            "Epoch [11/100], Step [ 56/ 78], Loss: 0.6511\n",
            "Epoch [11/100], Step [ 57/ 78], Loss: 0.6683\n",
            "Epoch [11/100], Step [ 58/ 78], Loss: 0.6548\n",
            "Epoch [11/100], Step [ 59/ 78], Loss: 0.6451\n",
            "Epoch [11/100], Step [ 60/ 78], Loss: 0.6541\n",
            "Epoch [11/100], Step [ 61/ 78], Loss: 0.6524\n",
            "Epoch [11/100], Step [ 62/ 78], Loss: 0.6574\n",
            "Epoch [11/100], Step [ 63/ 78], Loss: 0.6514\n",
            "Epoch [12/100], Step [  1/ 78], Loss: 0.6766\n",
            "Epoch [12/100], Step [  2/ 78], Loss: 0.6708\n",
            "Epoch [12/100], Step [  3/ 78], Loss: 0.6545\n",
            "Epoch [12/100], Step [  4/ 78], Loss: 0.6515\n",
            "Epoch [12/100], Step [  5/ 78], Loss: 0.6533\n",
            "Epoch [12/100], Step [  6/ 78], Loss: 0.6667\n",
            "Epoch [12/100], Step [  7/ 78], Loss: 0.6696\n",
            "Epoch [12/100], Step [  8/ 78], Loss: 0.6662\n",
            "Epoch [12/100], Step [  9/ 78], Loss: 0.6558\n",
            "Epoch [12/100], Step [ 10/ 78], Loss: 0.6476\n",
            "Epoch [12/100], Step [ 11/ 78], Loss: 0.6566\n",
            "Epoch [12/100], Step [ 12/ 78], Loss: 0.6661\n",
            "Epoch [12/100], Step [ 13/ 78], Loss: 0.6527\n",
            "Epoch [12/100], Step [ 14/ 78], Loss: 0.6453\n",
            "Epoch [12/100], Step [ 15/ 78], Loss: 0.6563\n",
            "Epoch [12/100], Step [ 16/ 78], Loss: 0.6672\n",
            "Epoch [12/100], Step [ 17/ 78], Loss: 0.6459\n",
            "Epoch [12/100], Step [ 18/ 78], Loss: 0.6612\n",
            "Epoch [12/100], Step [ 19/ 78], Loss: 0.6388\n",
            "Epoch [12/100], Step [ 20/ 78], Loss: 0.6566\n",
            "Epoch [12/100], Step [ 21/ 78], Loss: 0.6662\n",
            "Epoch [12/100], Step [ 22/ 78], Loss: 0.6609\n",
            "Epoch [12/100], Step [ 23/ 78], Loss: 0.6815\n",
            "Epoch [12/100], Step [ 24/ 78], Loss: 0.6563\n",
            "Epoch [12/100], Step [ 25/ 78], Loss: 0.6403\n",
            "Epoch [12/100], Step [ 26/ 78], Loss: 0.6514\n",
            "Epoch [12/100], Step [ 27/ 78], Loss: 0.6417\n",
            "Epoch [12/100], Step [ 28/ 78], Loss: 0.6581\n",
            "Epoch [12/100], Step [ 29/ 78], Loss: 0.6628\n",
            "Epoch [12/100], Step [ 30/ 78], Loss: 0.6707\n",
            "Epoch [12/100], Step [ 31/ 78], Loss: 0.6473\n",
            "Epoch [12/100], Step [ 32/ 78], Loss: 0.6682\n",
            "Epoch [12/100], Step [ 33/ 78], Loss: 0.6629\n",
            "Epoch [12/100], Step [ 34/ 78], Loss: 0.6778\n",
            "Epoch [12/100], Step [ 35/ 78], Loss: 0.6650\n",
            "Epoch [12/100], Step [ 36/ 78], Loss: 0.6499\n",
            "Epoch [12/100], Step [ 37/ 78], Loss: 0.6424\n",
            "Epoch [12/100], Step [ 38/ 78], Loss: 0.6354\n",
            "Epoch [12/100], Step [ 39/ 78], Loss: 0.6530\n",
            "Epoch [12/100], Step [ 40/ 78], Loss: 0.6613\n",
            "Epoch [12/100], Step [ 41/ 78], Loss: 0.6546\n",
            "Epoch [12/100], Step [ 42/ 78], Loss: 0.6410\n",
            "Epoch [12/100], Step [ 43/ 78], Loss: 0.6419\n",
            "Epoch [12/100], Step [ 44/ 78], Loss: 0.6572\n",
            "Epoch [12/100], Step [ 45/ 78], Loss: 0.6494\n",
            "Epoch [12/100], Step [ 46/ 78], Loss: 0.6541\n",
            "Epoch [12/100], Step [ 47/ 78], Loss: 0.6507\n",
            "Epoch [12/100], Step [ 48/ 78], Loss: 0.6456\n",
            "Epoch [12/100], Step [ 49/ 78], Loss: 0.6394\n",
            "Epoch [12/100], Step [ 50/ 78], Loss: 0.6563\n",
            "Epoch [12/100], Step [ 51/ 78], Loss: 0.6547\n",
            "Epoch [12/100], Step [ 52/ 78], Loss: 0.6613\n",
            "Epoch [12/100], Step [ 53/ 78], Loss: 0.6596\n",
            "Epoch [12/100], Step [ 54/ 78], Loss: 0.6544\n",
            "Epoch [12/100], Step [ 55/ 78], Loss: 0.6656\n",
            "Epoch [12/100], Step [ 56/ 78], Loss: 0.6509\n",
            "Epoch [12/100], Step [ 57/ 78], Loss: 0.6486\n",
            "Epoch [12/100], Step [ 58/ 78], Loss: 0.6472\n",
            "Epoch [12/100], Step [ 59/ 78], Loss: 0.6567\n",
            "Epoch [12/100], Step [ 60/ 78], Loss: 0.6472\n",
            "Epoch [12/100], Step [ 61/ 78], Loss: 0.6695\n",
            "Epoch [12/100], Step [ 62/ 78], Loss: 0.6522\n",
            "Epoch [12/100], Step [ 63/ 78], Loss: 0.6707\n",
            "Epoch [13/100], Step [  1/ 78], Loss: 0.6431\n",
            "Epoch [13/100], Step [  2/ 78], Loss: 0.6505\n",
            "Epoch [13/100], Step [  3/ 78], Loss: 0.6527\n",
            "Epoch [13/100], Step [  4/ 78], Loss: 0.6679\n",
            "Epoch [13/100], Step [  5/ 78], Loss: 0.6678\n",
            "Epoch [13/100], Step [  6/ 78], Loss: 0.6560\n",
            "Epoch [13/100], Step [  7/ 78], Loss: 0.6452\n",
            "Epoch [13/100], Step [  8/ 78], Loss: 0.6640\n",
            "Epoch [13/100], Step [  9/ 78], Loss: 0.6554\n",
            "Epoch [13/100], Step [ 10/ 78], Loss: 0.6716\n",
            "Epoch [13/100], Step [ 11/ 78], Loss: 0.6530\n",
            "Epoch [13/100], Step [ 12/ 78], Loss: 0.6558\n",
            "Epoch [13/100], Step [ 13/ 78], Loss: 0.6567\n",
            "Epoch [13/100], Step [ 14/ 78], Loss: 0.6480\n",
            "Epoch [13/100], Step [ 15/ 78], Loss: 0.6378\n",
            "Epoch [13/100], Step [ 16/ 78], Loss: 0.6409\n",
            "Epoch [13/100], Step [ 17/ 78], Loss: 0.6439\n",
            "Epoch [13/100], Step [ 18/ 78], Loss: 0.6630\n",
            "Epoch [13/100], Step [ 19/ 78], Loss: 0.6457\n",
            "Epoch [13/100], Step [ 20/ 78], Loss: 0.6478\n",
            "Epoch [13/100], Step [ 21/ 78], Loss: 0.6592\n",
            "Epoch [13/100], Step [ 22/ 78], Loss: 0.6236\n",
            "Epoch [13/100], Step [ 23/ 78], Loss: 0.6525\n",
            "Epoch [13/100], Step [ 24/ 78], Loss: 0.6554\n",
            "Epoch [13/100], Step [ 25/ 78], Loss: 0.6472\n",
            "Epoch [13/100], Step [ 26/ 78], Loss: 0.6519\n",
            "Epoch [13/100], Step [ 27/ 78], Loss: 0.6634\n",
            "Epoch [13/100], Step [ 28/ 78], Loss: 0.6616\n",
            "Epoch [13/100], Step [ 29/ 78], Loss: 0.6401\n",
            "Epoch [13/100], Step [ 30/ 78], Loss: 0.6538\n",
            "Epoch [13/100], Step [ 31/ 78], Loss: 0.6516\n",
            "Epoch [13/100], Step [ 32/ 78], Loss: 0.6518\n",
            "Epoch [13/100], Step [ 33/ 78], Loss: 0.6343\n",
            "Epoch [13/100], Step [ 34/ 78], Loss: 0.6390\n",
            "Epoch [13/100], Step [ 35/ 78], Loss: 0.6433\n",
            "Epoch [13/100], Step [ 36/ 78], Loss: 0.6553\n",
            "Epoch [13/100], Step [ 37/ 78], Loss: 0.6670\n",
            "Epoch [13/100], Step [ 38/ 78], Loss: 0.6490\n",
            "Epoch [13/100], Step [ 39/ 78], Loss: 0.6465\n",
            "Epoch [13/100], Step [ 40/ 78], Loss: 0.6498\n",
            "Epoch [13/100], Step [ 41/ 78], Loss: 0.6546\n",
            "Epoch [13/100], Step [ 42/ 78], Loss: 0.6561\n",
            "Epoch [13/100], Step [ 43/ 78], Loss: 0.6814\n",
            "Epoch [13/100], Step [ 44/ 78], Loss: 0.6522\n",
            "Epoch [13/100], Step [ 45/ 78], Loss: 0.6468\n",
            "Epoch [13/100], Step [ 46/ 78], Loss: 0.6682\n",
            "Epoch [13/100], Step [ 47/ 78], Loss: 0.6439\n",
            "Epoch [13/100], Step [ 48/ 78], Loss: 0.6605\n",
            "Epoch [13/100], Step [ 49/ 78], Loss: 0.6628\n",
            "Epoch [13/100], Step [ 50/ 78], Loss: 0.6284\n",
            "Epoch [13/100], Step [ 51/ 78], Loss: 0.6519\n",
            "Epoch [13/100], Step [ 52/ 78], Loss: 0.6636\n",
            "Epoch [13/100], Step [ 53/ 78], Loss: 0.6401\n",
            "Epoch [13/100], Step [ 54/ 78], Loss: 0.6592\n",
            "Epoch [13/100], Step [ 55/ 78], Loss: 0.6387\n",
            "Epoch [13/100], Step [ 56/ 78], Loss: 0.6581\n",
            "Epoch [13/100], Step [ 57/ 78], Loss: 0.6389\n",
            "Epoch [13/100], Step [ 58/ 78], Loss: 0.6473\n",
            "Epoch [13/100], Step [ 59/ 78], Loss: 0.6563\n",
            "Epoch [13/100], Step [ 60/ 78], Loss: 0.6507\n",
            "Epoch [13/100], Step [ 61/ 78], Loss: 0.6158\n",
            "Epoch [13/100], Step [ 62/ 78], Loss: 0.6378\n",
            "Epoch [13/100], Step [ 63/ 78], Loss: 0.6354\n",
            "Epoch [14/100], Step [  1/ 78], Loss: 0.6327\n",
            "Epoch [14/100], Step [  2/ 78], Loss: 0.6370\n",
            "Epoch [14/100], Step [  3/ 78], Loss: 0.6532\n",
            "Epoch [14/100], Step [  4/ 78], Loss: 0.6461\n",
            "Epoch [14/100], Step [  5/ 78], Loss: 0.6258\n",
            "Epoch [14/100], Step [  6/ 78], Loss: 0.6386\n",
            "Epoch [14/100], Step [  7/ 78], Loss: 0.6373\n",
            "Epoch [14/100], Step [  8/ 78], Loss: 0.6613\n",
            "Epoch [14/100], Step [  9/ 78], Loss: 0.6800\n",
            "Epoch [14/100], Step [ 10/ 78], Loss: 0.6371\n",
            "Epoch [14/100], Step [ 11/ 78], Loss: 0.6718\n",
            "Epoch [14/100], Step [ 12/ 78], Loss: 0.6491\n",
            "Epoch [14/100], Step [ 13/ 78], Loss: 0.6553\n",
            "Epoch [14/100], Step [ 14/ 78], Loss: 0.6461\n",
            "Epoch [14/100], Step [ 15/ 78], Loss: 0.6456\n",
            "Epoch [14/100], Step [ 16/ 78], Loss: 0.6384\n",
            "Epoch [14/100], Step [ 17/ 78], Loss: 0.6429\n",
            "Epoch [14/100], Step [ 18/ 78], Loss: 0.6586\n",
            "Epoch [14/100], Step [ 19/ 78], Loss: 0.6573\n",
            "Epoch [14/100], Step [ 20/ 78], Loss: 0.6357\n",
            "Epoch [14/100], Step [ 21/ 78], Loss: 0.6525\n",
            "Epoch [14/100], Step [ 22/ 78], Loss: 0.6235\n",
            "Epoch [14/100], Step [ 23/ 78], Loss: 0.6622\n",
            "Epoch [14/100], Step [ 24/ 78], Loss: 0.6404\n",
            "Epoch [14/100], Step [ 25/ 78], Loss: 0.6576\n",
            "Epoch [14/100], Step [ 26/ 78], Loss: 0.6486\n",
            "Epoch [14/100], Step [ 27/ 78], Loss: 0.6468\n",
            "Epoch [14/100], Step [ 28/ 78], Loss: 0.6294\n",
            "Epoch [14/100], Step [ 29/ 78], Loss: 0.6488\n",
            "Epoch [14/100], Step [ 30/ 78], Loss: 0.6548\n",
            "Epoch [14/100], Step [ 31/ 78], Loss: 0.6299\n",
            "Epoch [14/100], Step [ 32/ 78], Loss: 0.6313\n",
            "Epoch [14/100], Step [ 33/ 78], Loss: 0.6617\n",
            "Epoch [14/100], Step [ 34/ 78], Loss: 0.6444\n",
            "Epoch [14/100], Step [ 35/ 78], Loss: 0.6623\n",
            "Epoch [14/100], Step [ 36/ 78], Loss: 0.6335\n",
            "Epoch [14/100], Step [ 37/ 78], Loss: 0.6355\n",
            "Epoch [14/100], Step [ 38/ 78], Loss: 0.6514\n",
            "Epoch [14/100], Step [ 39/ 78], Loss: 0.6528\n",
            "Epoch [14/100], Step [ 40/ 78], Loss: 0.6529\n",
            "Epoch [14/100], Step [ 41/ 78], Loss: 0.6508\n",
            "Epoch [14/100], Step [ 42/ 78], Loss: 0.6620\n",
            "Epoch [14/100], Step [ 43/ 78], Loss: 0.6759\n",
            "Epoch [14/100], Step [ 44/ 78], Loss: 0.6365\n",
            "Epoch [14/100], Step [ 45/ 78], Loss: 0.6351\n",
            "Epoch [14/100], Step [ 46/ 78], Loss: 0.6507\n",
            "Epoch [14/100], Step [ 47/ 78], Loss: 0.6578\n",
            "Epoch [14/100], Step [ 48/ 78], Loss: 0.6457\n",
            "Epoch [14/100], Step [ 49/ 78], Loss: 0.6253\n",
            "Epoch [14/100], Step [ 50/ 78], Loss: 0.6460\n",
            "Epoch [14/100], Step [ 51/ 78], Loss: 0.6541\n",
            "Epoch [14/100], Step [ 52/ 78], Loss: 0.6373\n",
            "Epoch [14/100], Step [ 53/ 78], Loss: 0.6654\n",
            "Epoch [14/100], Step [ 54/ 78], Loss: 0.6560\n",
            "Epoch [14/100], Step [ 55/ 78], Loss: 0.6503\n",
            "Epoch [14/100], Step [ 56/ 78], Loss: 0.6552\n",
            "Epoch [14/100], Step [ 57/ 78], Loss: 0.6365\n",
            "Epoch [14/100], Step [ 58/ 78], Loss: 0.6263\n",
            "Epoch [14/100], Step [ 59/ 78], Loss: 0.6398\n",
            "Epoch [14/100], Step [ 60/ 78], Loss: 0.6305\n",
            "Epoch [14/100], Step [ 61/ 78], Loss: 0.6547\n",
            "Epoch [14/100], Step [ 62/ 78], Loss: 0.6540\n",
            "Epoch [14/100], Step [ 63/ 78], Loss: 0.6580\n",
            "Epoch [15/100], Step [  1/ 78], Loss: 0.6540\n",
            "Epoch [15/100], Step [  2/ 78], Loss: 0.6536\n",
            "Epoch [15/100], Step [  3/ 78], Loss: 0.6318\n",
            "Epoch [15/100], Step [  4/ 78], Loss: 0.6551\n",
            "Epoch [15/100], Step [  5/ 78], Loss: 0.6402\n",
            "Epoch [15/100], Step [  6/ 78], Loss: 0.6516\n",
            "Epoch [15/100], Step [  7/ 78], Loss: 0.6552\n",
            "Epoch [15/100], Step [  8/ 78], Loss: 0.6510\n",
            "Epoch [15/100], Step [  9/ 78], Loss: 0.6408\n",
            "Epoch [15/100], Step [ 10/ 78], Loss: 0.6347\n",
            "Epoch [15/100], Step [ 11/ 78], Loss: 0.6365\n",
            "Epoch [15/100], Step [ 12/ 78], Loss: 0.6352\n",
            "Epoch [15/100], Step [ 13/ 78], Loss: 0.6602\n",
            "Epoch [15/100], Step [ 14/ 78], Loss: 0.6356\n",
            "Epoch [15/100], Step [ 15/ 78], Loss: 0.6459\n",
            "Epoch [15/100], Step [ 16/ 78], Loss: 0.6675\n",
            "Epoch [15/100], Step [ 17/ 78], Loss: 0.6352\n",
            "Epoch [15/100], Step [ 18/ 78], Loss: 0.6445\n",
            "Epoch [15/100], Step [ 19/ 78], Loss: 0.6529\n",
            "Epoch [15/100], Step [ 20/ 78], Loss: 0.6543\n",
            "Epoch [15/100], Step [ 21/ 78], Loss: 0.6297\n",
            "Epoch [15/100], Step [ 22/ 78], Loss: 0.6547\n",
            "Epoch [15/100], Step [ 23/ 78], Loss: 0.6420\n",
            "Epoch [15/100], Step [ 24/ 78], Loss: 0.6624\n",
            "Epoch [15/100], Step [ 25/ 78], Loss: 0.6306\n",
            "Epoch [15/100], Step [ 26/ 78], Loss: 0.6445\n",
            "Epoch [15/100], Step [ 27/ 78], Loss: 0.6460\n",
            "Epoch [15/100], Step [ 28/ 78], Loss: 0.6537\n",
            "Epoch [15/100], Step [ 29/ 78], Loss: 0.6311\n",
            "Epoch [15/100], Step [ 30/ 78], Loss: 0.6384\n",
            "Epoch [15/100], Step [ 31/ 78], Loss: 0.6256\n",
            "Epoch [15/100], Step [ 32/ 78], Loss: 0.6601\n",
            "Epoch [15/100], Step [ 33/ 78], Loss: 0.6270\n",
            "Epoch [15/100], Step [ 34/ 78], Loss: 0.6328\n",
            "Epoch [15/100], Step [ 35/ 78], Loss: 0.6543\n",
            "Epoch [15/100], Step [ 36/ 78], Loss: 0.6467\n",
            "Epoch [15/100], Step [ 37/ 78], Loss: 0.6235\n",
            "Epoch [15/100], Step [ 38/ 78], Loss: 0.6429\n",
            "Epoch [15/100], Step [ 39/ 78], Loss: 0.6373\n",
            "Epoch [15/100], Step [ 40/ 78], Loss: 0.6433\n",
            "Epoch [15/100], Step [ 41/ 78], Loss: 0.6424\n",
            "Epoch [15/100], Step [ 42/ 78], Loss: 0.6421\n",
            "Epoch [15/100], Step [ 43/ 78], Loss: 0.6333\n",
            "Epoch [15/100], Step [ 44/ 78], Loss: 0.6627\n",
            "Epoch [15/100], Step [ 45/ 78], Loss: 0.6171\n",
            "Epoch [15/100], Step [ 46/ 78], Loss: 0.6583\n",
            "Epoch [15/100], Step [ 47/ 78], Loss: 0.6351\n",
            "Epoch [15/100], Step [ 48/ 78], Loss: 0.6553\n",
            "Epoch [15/100], Step [ 49/ 78], Loss: 0.6514\n",
            "Epoch [15/100], Step [ 50/ 78], Loss: 0.6262\n",
            "Epoch [15/100], Step [ 51/ 78], Loss: 0.6480\n",
            "Epoch [15/100], Step [ 52/ 78], Loss: 0.6573\n",
            "Epoch [15/100], Step [ 53/ 78], Loss: 0.6615\n",
            "Epoch [15/100], Step [ 54/ 78], Loss: 0.6313\n",
            "Epoch [15/100], Step [ 55/ 78], Loss: 0.6683\n",
            "Epoch [15/100], Step [ 56/ 78], Loss: 0.6419\n",
            "Epoch [15/100], Step [ 57/ 78], Loss: 0.6282\n",
            "Epoch [15/100], Step [ 58/ 78], Loss: 0.6334\n",
            "Epoch [15/100], Step [ 59/ 78], Loss: 0.6423\n",
            "Epoch [15/100], Step [ 60/ 78], Loss: 0.6432\n",
            "Epoch [15/100], Step [ 61/ 78], Loss: 0.6479\n",
            "Epoch [15/100], Step [ 62/ 78], Loss: 0.6477\n",
            "Epoch [15/100], Step [ 63/ 78], Loss: 0.6469\n",
            "Epoch [16/100], Step [  1/ 78], Loss: 0.6196\n",
            "Epoch [16/100], Step [  2/ 78], Loss: 0.6279\n",
            "Epoch [16/100], Step [  3/ 78], Loss: 0.6628\n",
            "Epoch [16/100], Step [  4/ 78], Loss: 0.6354\n",
            "Epoch [16/100], Step [  5/ 78], Loss: 0.6570\n",
            "Epoch [16/100], Step [  6/ 78], Loss: 0.6501\n",
            "Epoch [16/100], Step [  7/ 78], Loss: 0.6373\n",
            "Epoch [16/100], Step [  8/ 78], Loss: 0.6403\n",
            "Epoch [16/100], Step [  9/ 78], Loss: 0.6358\n",
            "Epoch [16/100], Step [ 10/ 78], Loss: 0.6540\n",
            "Epoch [16/100], Step [ 11/ 78], Loss: 0.6454\n",
            "Epoch [16/100], Step [ 12/ 78], Loss: 0.6316\n",
            "Epoch [16/100], Step [ 13/ 78], Loss: 0.6282\n",
            "Epoch [16/100], Step [ 14/ 78], Loss: 0.6314\n",
            "Epoch [16/100], Step [ 15/ 78], Loss: 0.6295\n",
            "Epoch [16/100], Step [ 16/ 78], Loss: 0.6453\n",
            "Epoch [16/100], Step [ 17/ 78], Loss: 0.6699\n",
            "Epoch [16/100], Step [ 18/ 78], Loss: 0.6268\n",
            "Epoch [16/100], Step [ 19/ 78], Loss: 0.6285\n",
            "Epoch [16/100], Step [ 20/ 78], Loss: 0.6392\n",
            "Epoch [16/100], Step [ 21/ 78], Loss: 0.6439\n",
            "Epoch [16/100], Step [ 22/ 78], Loss: 0.6360\n",
            "Epoch [16/100], Step [ 23/ 78], Loss: 0.6554\n",
            "Epoch [16/100], Step [ 24/ 78], Loss: 0.6254\n",
            "Epoch [16/100], Step [ 25/ 78], Loss: 0.6527\n",
            "Epoch [16/100], Step [ 26/ 78], Loss: 0.6404\n",
            "Epoch [16/100], Step [ 27/ 78], Loss: 0.6323\n",
            "Epoch [16/100], Step [ 28/ 78], Loss: 0.6353\n",
            "Epoch [16/100], Step [ 29/ 78], Loss: 0.6367\n",
            "Epoch [16/100], Step [ 30/ 78], Loss: 0.6417\n",
            "Epoch [16/100], Step [ 31/ 78], Loss: 0.6557\n",
            "Epoch [16/100], Step [ 32/ 78], Loss: 0.6441\n",
            "Epoch [16/100], Step [ 33/ 78], Loss: 0.6460\n",
            "Epoch [16/100], Step [ 34/ 78], Loss: 0.6380\n",
            "Epoch [16/100], Step [ 35/ 78], Loss: 0.6446\n",
            "Epoch [16/100], Step [ 36/ 78], Loss: 0.6385\n",
            "Epoch [16/100], Step [ 37/ 78], Loss: 0.6439\n",
            "Epoch [16/100], Step [ 38/ 78], Loss: 0.6502\n",
            "Epoch [16/100], Step [ 39/ 78], Loss: 0.6602\n",
            "Epoch [16/100], Step [ 40/ 78], Loss: 0.6107\n",
            "Epoch [16/100], Step [ 41/ 78], Loss: 0.6195\n",
            "Epoch [16/100], Step [ 42/ 78], Loss: 0.6453\n",
            "Epoch [16/100], Step [ 43/ 78], Loss: 0.6300\n",
            "Epoch [16/100], Step [ 44/ 78], Loss: 0.6274\n",
            "Epoch [16/100], Step [ 45/ 78], Loss: 0.6287\n",
            "Epoch [16/100], Step [ 46/ 78], Loss: 0.6364\n",
            "Epoch [16/100], Step [ 47/ 78], Loss: 0.6183\n",
            "Epoch [16/100], Step [ 48/ 78], Loss: 0.6269\n",
            "Epoch [16/100], Step [ 49/ 78], Loss: 0.6495\n",
            "Epoch [16/100], Step [ 50/ 78], Loss: 0.6288\n",
            "Epoch [16/100], Step [ 51/ 78], Loss: 0.6137\n",
            "Epoch [16/100], Step [ 52/ 78], Loss: 0.6318\n",
            "Epoch [16/100], Step [ 53/ 78], Loss: 0.6385\n",
            "Epoch [16/100], Step [ 54/ 78], Loss: 0.6614\n",
            "Epoch [16/100], Step [ 55/ 78], Loss: 0.6363\n",
            "Epoch [16/100], Step [ 56/ 78], Loss: 0.6402\n",
            "Epoch [16/100], Step [ 57/ 78], Loss: 0.6207\n",
            "Epoch [16/100], Step [ 58/ 78], Loss: 0.6460\n",
            "Epoch [16/100], Step [ 59/ 78], Loss: 0.6499\n",
            "Epoch [16/100], Step [ 60/ 78], Loss: 0.6435\n",
            "Epoch [16/100], Step [ 61/ 78], Loss: 0.6063\n",
            "Epoch [16/100], Step [ 62/ 78], Loss: 0.6442\n",
            "Epoch [16/100], Step [ 63/ 78], Loss: 0.6840\n",
            "Epoch [17/100], Step [  1/ 78], Loss: 0.6466\n",
            "Epoch [17/100], Step [  2/ 78], Loss: 0.6368\n",
            "Epoch [17/100], Step [  3/ 78], Loss: 0.6691\n",
            "Epoch [17/100], Step [  4/ 78], Loss: 0.6341\n",
            "Epoch [17/100], Step [  5/ 78], Loss: 0.6492\n",
            "Epoch [17/100], Step [  6/ 78], Loss: 0.6343\n",
            "Epoch [17/100], Step [  7/ 78], Loss: 0.6335\n",
            "Epoch [17/100], Step [  8/ 78], Loss: 0.6444\n",
            "Epoch [17/100], Step [  9/ 78], Loss: 0.6258\n",
            "Epoch [17/100], Step [ 10/ 78], Loss: 0.6447\n",
            "Epoch [17/100], Step [ 11/ 78], Loss: 0.6236\n",
            "Epoch [17/100], Step [ 12/ 78], Loss: 0.6632\n",
            "Epoch [17/100], Step [ 13/ 78], Loss: 0.6412\n",
            "Epoch [17/100], Step [ 14/ 78], Loss: 0.6185\n",
            "Epoch [17/100], Step [ 15/ 78], Loss: 0.6194\n",
            "Epoch [17/100], Step [ 16/ 78], Loss: 0.6468\n",
            "Epoch [17/100], Step [ 17/ 78], Loss: 0.6527\n",
            "Epoch [17/100], Step [ 18/ 78], Loss: 0.6502\n",
            "Epoch [17/100], Step [ 19/ 78], Loss: 0.6154\n",
            "Epoch [17/100], Step [ 20/ 78], Loss: 0.6265\n",
            "Epoch [17/100], Step [ 21/ 78], Loss: 0.6527\n",
            "Epoch [17/100], Step [ 22/ 78], Loss: 0.6038\n",
            "Epoch [17/100], Step [ 23/ 78], Loss: 0.6595\n",
            "Epoch [17/100], Step [ 24/ 78], Loss: 0.6469\n",
            "Epoch [17/100], Step [ 25/ 78], Loss: 0.6189\n",
            "Epoch [17/100], Step [ 26/ 78], Loss: 0.6561\n",
            "Epoch [17/100], Step [ 27/ 78], Loss: 0.6495\n",
            "Epoch [17/100], Step [ 28/ 78], Loss: 0.6112\n",
            "Epoch [17/100], Step [ 29/ 78], Loss: 0.6334\n",
            "Epoch [17/100], Step [ 30/ 78], Loss: 0.6304\n",
            "Epoch [17/100], Step [ 31/ 78], Loss: 0.6289\n",
            "Epoch [17/100], Step [ 32/ 78], Loss: 0.6450\n",
            "Epoch [17/100], Step [ 33/ 78], Loss: 0.6346\n",
            "Epoch [17/100], Step [ 34/ 78], Loss: 0.6701\n",
            "Epoch [17/100], Step [ 35/ 78], Loss: 0.6145\n",
            "Epoch [17/100], Step [ 36/ 78], Loss: 0.6144\n",
            "Epoch [17/100], Step [ 37/ 78], Loss: 0.6378\n",
            "Epoch [17/100], Step [ 38/ 78], Loss: 0.6244\n",
            "Epoch [17/100], Step [ 39/ 78], Loss: 0.6272\n",
            "Epoch [17/100], Step [ 40/ 78], Loss: 0.6429\n",
            "Epoch [17/100], Step [ 41/ 78], Loss: 0.6580\n",
            "Epoch [17/100], Step [ 42/ 78], Loss: 0.6558\n",
            "Epoch [17/100], Step [ 43/ 78], Loss: 0.6203\n",
            "Epoch [17/100], Step [ 44/ 78], Loss: 0.6348\n",
            "Epoch [17/100], Step [ 45/ 78], Loss: 0.6333\n",
            "Epoch [17/100], Step [ 46/ 78], Loss: 0.6282\n",
            "Epoch [17/100], Step [ 47/ 78], Loss: 0.6204\n",
            "Epoch [17/100], Step [ 48/ 78], Loss: 0.6130\n",
            "Epoch [17/100], Step [ 49/ 78], Loss: 0.6709\n",
            "Epoch [17/100], Step [ 50/ 78], Loss: 0.6332\n",
            "Epoch [17/100], Step [ 51/ 78], Loss: 0.6048\n",
            "Epoch [17/100], Step [ 52/ 78], Loss: 0.6375\n",
            "Epoch [17/100], Step [ 53/ 78], Loss: 0.6321\n",
            "Epoch [17/100], Step [ 54/ 78], Loss: 0.6412\n",
            "Epoch [17/100], Step [ 55/ 78], Loss: 0.6546\n",
            "Epoch [17/100], Step [ 56/ 78], Loss: 0.6115\n",
            "Epoch [17/100], Step [ 57/ 78], Loss: 0.6384\n",
            "Epoch [17/100], Step [ 58/ 78], Loss: 0.6634\n",
            "Epoch [17/100], Step [ 59/ 78], Loss: 0.6408\n",
            "Epoch [17/100], Step [ 60/ 78], Loss: 0.6338\n",
            "Epoch [17/100], Step [ 61/ 78], Loss: 0.6752\n",
            "Epoch [17/100], Step [ 62/ 78], Loss: 0.6297\n",
            "Epoch [17/100], Step [ 63/ 78], Loss: 0.6513\n",
            "Epoch [18/100], Step [  1/ 78], Loss: 0.6241\n",
            "Epoch [18/100], Step [  2/ 78], Loss: 0.6270\n",
            "Epoch [18/100], Step [  3/ 78], Loss: 0.6419\n",
            "Epoch [18/100], Step [  4/ 78], Loss: 0.6323\n",
            "Epoch [18/100], Step [  5/ 78], Loss: 0.6175\n",
            "Epoch [18/100], Step [  6/ 78], Loss: 0.6379\n",
            "Epoch [18/100], Step [  7/ 78], Loss: 0.6516\n",
            "Epoch [18/100], Step [  8/ 78], Loss: 0.6676\n",
            "Epoch [18/100], Step [  9/ 78], Loss: 0.6441\n",
            "Epoch [18/100], Step [ 10/ 78], Loss: 0.6426\n",
            "Epoch [18/100], Step [ 11/ 78], Loss: 0.6256\n",
            "Epoch [18/100], Step [ 12/ 78], Loss: 0.6277\n",
            "Epoch [18/100], Step [ 13/ 78], Loss: 0.6295\n",
            "Epoch [18/100], Step [ 14/ 78], Loss: 0.6358\n",
            "Epoch [18/100], Step [ 15/ 78], Loss: 0.6093\n",
            "Epoch [18/100], Step [ 16/ 78], Loss: 0.6261\n",
            "Epoch [18/100], Step [ 17/ 78], Loss: 0.6299\n",
            "Epoch [18/100], Step [ 18/ 78], Loss: 0.6229\n",
            "Epoch [18/100], Step [ 19/ 78], Loss: 0.6239\n",
            "Epoch [18/100], Step [ 20/ 78], Loss: 0.6329\n",
            "Epoch [18/100], Step [ 21/ 78], Loss: 0.6090\n",
            "Epoch [18/100], Step [ 22/ 78], Loss: 0.6496\n",
            "Epoch [18/100], Step [ 23/ 78], Loss: 0.6481\n",
            "Epoch [18/100], Step [ 24/ 78], Loss: 0.6714\n",
            "Epoch [18/100], Step [ 25/ 78], Loss: 0.6706\n",
            "Epoch [18/100], Step [ 26/ 78], Loss: 0.6600\n",
            "Epoch [18/100], Step [ 27/ 78], Loss: 0.6432\n",
            "Epoch [18/100], Step [ 28/ 78], Loss: 0.6402\n",
            "Epoch [18/100], Step [ 29/ 78], Loss: 0.6131\n",
            "Epoch [18/100], Step [ 30/ 78], Loss: 0.6277\n",
            "Epoch [18/100], Step [ 31/ 78], Loss: 0.6153\n",
            "Epoch [18/100], Step [ 32/ 78], Loss: 0.6129\n",
            "Epoch [18/100], Step [ 33/ 78], Loss: 0.6166\n",
            "Epoch [18/100], Step [ 34/ 78], Loss: 0.6476\n",
            "Epoch [18/100], Step [ 35/ 78], Loss: 0.6343\n",
            "Epoch [18/100], Step [ 36/ 78], Loss: 0.6270\n",
            "Epoch [18/100], Step [ 37/ 78], Loss: 0.6546\n",
            "Epoch [18/100], Step [ 38/ 78], Loss: 0.6539\n",
            "Epoch [18/100], Step [ 39/ 78], Loss: 0.6167\n",
            "Epoch [18/100], Step [ 40/ 78], Loss: 0.6348\n",
            "Epoch [18/100], Step [ 41/ 78], Loss: 0.6411\n",
            "Epoch [18/100], Step [ 42/ 78], Loss: 0.6339\n",
            "Epoch [18/100], Step [ 43/ 78], Loss: 0.6155\n",
            "Epoch [18/100], Step [ 44/ 78], Loss: 0.6242\n",
            "Epoch [18/100], Step [ 45/ 78], Loss: 0.6282\n",
            "Epoch [18/100], Step [ 46/ 78], Loss: 0.6090\n",
            "Epoch [18/100], Step [ 47/ 78], Loss: 0.6298\n",
            "Epoch [18/100], Step [ 48/ 78], Loss: 0.6486\n",
            "Epoch [18/100], Step [ 49/ 78], Loss: 0.6545\n",
            "Epoch [18/100], Step [ 50/ 78], Loss: 0.5860\n",
            "Epoch [18/100], Step [ 51/ 78], Loss: 0.6266\n",
            "Epoch [18/100], Step [ 52/ 78], Loss: 0.6373\n",
            "Epoch [18/100], Step [ 53/ 78], Loss: 0.6394\n",
            "Epoch [18/100], Step [ 54/ 78], Loss: 0.6324\n",
            "Epoch [18/100], Step [ 55/ 78], Loss: 0.6142\n",
            "Epoch [18/100], Step [ 56/ 78], Loss: 0.6184\n",
            "Epoch [18/100], Step [ 57/ 78], Loss: 0.6521\n",
            "Epoch [18/100], Step [ 58/ 78], Loss: 0.6311\n",
            "Epoch [18/100], Step [ 59/ 78], Loss: 0.6241\n",
            "Epoch [18/100], Step [ 60/ 78], Loss: 0.6272\n",
            "Epoch [18/100], Step [ 61/ 78], Loss: 0.6298\n",
            "Epoch [18/100], Step [ 62/ 78], Loss: 0.6132\n",
            "Epoch [18/100], Step [ 63/ 78], Loss: 0.6146\n",
            "Epoch [19/100], Step [  1/ 78], Loss: 0.6011\n",
            "Epoch [19/100], Step [  2/ 78], Loss: 0.6497\n",
            "Epoch [19/100], Step [  3/ 78], Loss: 0.6733\n",
            "Epoch [19/100], Step [  4/ 78], Loss: 0.6393\n",
            "Epoch [19/100], Step [  5/ 78], Loss: 0.6101\n",
            "Epoch [19/100], Step [  6/ 78], Loss: 0.6250\n",
            "Epoch [19/100], Step [  7/ 78], Loss: 0.6213\n",
            "Epoch [19/100], Step [  8/ 78], Loss: 0.6238\n",
            "Epoch [19/100], Step [  9/ 78], Loss: 0.6299\n",
            "Epoch [19/100], Step [ 10/ 78], Loss: 0.6350\n",
            "Epoch [19/100], Step [ 11/ 78], Loss: 0.6224\n",
            "Epoch [19/100], Step [ 12/ 78], Loss: 0.6339\n",
            "Epoch [19/100], Step [ 13/ 78], Loss: 0.6317\n",
            "Epoch [19/100], Step [ 14/ 78], Loss: 0.6106\n",
            "Epoch [19/100], Step [ 15/ 78], Loss: 0.6401\n",
            "Epoch [19/100], Step [ 16/ 78], Loss: 0.6136\n",
            "Epoch [19/100], Step [ 17/ 78], Loss: 0.6301\n",
            "Epoch [19/100], Step [ 18/ 78], Loss: 0.6438\n",
            "Epoch [19/100], Step [ 19/ 78], Loss: 0.6456\n",
            "Epoch [19/100], Step [ 20/ 78], Loss: 0.6344\n",
            "Epoch [19/100], Step [ 21/ 78], Loss: 0.6241\n",
            "Epoch [19/100], Step [ 22/ 78], Loss: 0.6073\n",
            "Epoch [19/100], Step [ 23/ 78], Loss: 0.6088\n",
            "Epoch [19/100], Step [ 24/ 78], Loss: 0.6429\n",
            "Epoch [19/100], Step [ 25/ 78], Loss: 0.6290\n",
            "Epoch [19/100], Step [ 26/ 78], Loss: 0.6574\n",
            "Epoch [19/100], Step [ 27/ 78], Loss: 0.6117\n",
            "Epoch [19/100], Step [ 28/ 78], Loss: 0.6353\n",
            "Epoch [19/100], Step [ 29/ 78], Loss: 0.6140\n",
            "Epoch [19/100], Step [ 30/ 78], Loss: 0.6415\n",
            "Epoch [19/100], Step [ 31/ 78], Loss: 0.6388\n",
            "Epoch [19/100], Step [ 32/ 78], Loss: 0.6332\n",
            "Epoch [19/100], Step [ 33/ 78], Loss: 0.6382\n",
            "Epoch [19/100], Step [ 34/ 78], Loss: 0.6164\n",
            "Epoch [19/100], Step [ 35/ 78], Loss: 0.6241\n",
            "Epoch [19/100], Step [ 36/ 78], Loss: 0.6226\n",
            "Epoch [19/100], Step [ 37/ 78], Loss: 0.6433\n",
            "Epoch [19/100], Step [ 38/ 78], Loss: 0.6480\n",
            "Epoch [19/100], Step [ 39/ 78], Loss: 0.6218\n",
            "Epoch [19/100], Step [ 40/ 78], Loss: 0.6370\n",
            "Epoch [19/100], Step [ 41/ 78], Loss: 0.6220\n",
            "Epoch [19/100], Step [ 42/ 78], Loss: 0.5946\n",
            "Epoch [19/100], Step [ 43/ 78], Loss: 0.6107\n",
            "Epoch [19/100], Step [ 44/ 78], Loss: 0.6274\n",
            "Epoch [19/100], Step [ 45/ 78], Loss: 0.6439\n",
            "Epoch [19/100], Step [ 46/ 78], Loss: 0.6395\n",
            "Epoch [19/100], Step [ 47/ 78], Loss: 0.5971\n",
            "Epoch [19/100], Step [ 48/ 78], Loss: 0.6302\n",
            "Epoch [19/100], Step [ 49/ 78], Loss: 0.6258\n",
            "Epoch [19/100], Step [ 50/ 78], Loss: 0.6197\n",
            "Epoch [19/100], Step [ 51/ 78], Loss: 0.6103\n",
            "Epoch [19/100], Step [ 52/ 78], Loss: 0.6549\n",
            "Epoch [19/100], Step [ 53/ 78], Loss: 0.6484\n",
            "Epoch [19/100], Step [ 54/ 78], Loss: 0.6406\n",
            "Epoch [19/100], Step [ 55/ 78], Loss: 0.6234\n",
            "Epoch [19/100], Step [ 56/ 78], Loss: 0.6276\n",
            "Epoch [19/100], Step [ 57/ 78], Loss: 0.6317\n",
            "Epoch [19/100], Step [ 58/ 78], Loss: 0.6425\n",
            "Epoch [19/100], Step [ 59/ 78], Loss: 0.6178\n",
            "Epoch [19/100], Step [ 60/ 78], Loss: 0.6265\n",
            "Epoch [19/100], Step [ 61/ 78], Loss: 0.6188\n",
            "Epoch [19/100], Step [ 62/ 78], Loss: 0.6483\n",
            "Epoch [19/100], Step [ 63/ 78], Loss: 0.6301\n",
            "Epoch [20/100], Step [  1/ 78], Loss: 0.6413\n",
            "Epoch [20/100], Step [  2/ 78], Loss: 0.6149\n",
            "Epoch [20/100], Step [  3/ 78], Loss: 0.6575\n",
            "Epoch [20/100], Step [  4/ 78], Loss: 0.6188\n",
            "Epoch [20/100], Step [  5/ 78], Loss: 0.6212\n",
            "Epoch [20/100], Step [  6/ 78], Loss: 0.6189\n",
            "Epoch [20/100], Step [  7/ 78], Loss: 0.6328\n",
            "Epoch [20/100], Step [  8/ 78], Loss: 0.6594\n",
            "Epoch [20/100], Step [  9/ 78], Loss: 0.6173\n",
            "Epoch [20/100], Step [ 10/ 78], Loss: 0.6340\n",
            "Epoch [20/100], Step [ 11/ 78], Loss: 0.6247\n",
            "Epoch [20/100], Step [ 12/ 78], Loss: 0.6169\n",
            "Epoch [20/100], Step [ 13/ 78], Loss: 0.6137\n",
            "Epoch [20/100], Step [ 14/ 78], Loss: 0.6011\n",
            "Epoch [20/100], Step [ 15/ 78], Loss: 0.5969\n",
            "Epoch [20/100], Step [ 16/ 78], Loss: 0.6346\n",
            "Epoch [20/100], Step [ 17/ 78], Loss: 0.6485\n",
            "Epoch [20/100], Step [ 18/ 78], Loss: 0.6341\n",
            "Epoch [20/100], Step [ 19/ 78], Loss: 0.6567\n",
            "Epoch [20/100], Step [ 20/ 78], Loss: 0.6188\n",
            "Epoch [20/100], Step [ 21/ 78], Loss: 0.6421\n",
            "Epoch [20/100], Step [ 22/ 78], Loss: 0.6271\n",
            "Epoch [20/100], Step [ 23/ 78], Loss: 0.6354\n",
            "Epoch [20/100], Step [ 24/ 78], Loss: 0.6187\n",
            "Epoch [20/100], Step [ 25/ 78], Loss: 0.6111\n",
            "Epoch [20/100], Step [ 26/ 78], Loss: 0.6073\n",
            "Epoch [20/100], Step [ 27/ 78], Loss: 0.6603\n",
            "Epoch [20/100], Step [ 28/ 78], Loss: 0.6365\n",
            "Epoch [20/100], Step [ 29/ 78], Loss: 0.6318\n",
            "Epoch [20/100], Step [ 30/ 78], Loss: 0.6438\n",
            "Epoch [20/100], Step [ 31/ 78], Loss: 0.6350\n",
            "Epoch [20/100], Step [ 32/ 78], Loss: 0.6348\n",
            "Epoch [20/100], Step [ 33/ 78], Loss: 0.6540\n",
            "Epoch [20/100], Step [ 34/ 78], Loss: 0.6057\n",
            "Epoch [20/100], Step [ 35/ 78], Loss: 0.5915\n",
            "Epoch [20/100], Step [ 36/ 78], Loss: 0.6214\n",
            "Epoch [20/100], Step [ 37/ 78], Loss: 0.6434\n",
            "Epoch [20/100], Step [ 38/ 78], Loss: 0.6397\n",
            "Epoch [20/100], Step [ 39/ 78], Loss: 0.6312\n",
            "Epoch [20/100], Step [ 40/ 78], Loss: 0.6895\n",
            "Epoch [20/100], Step [ 41/ 78], Loss: 0.6334\n",
            "Epoch [20/100], Step [ 42/ 78], Loss: 0.6558\n",
            "Epoch [20/100], Step [ 43/ 78], Loss: 0.6165\n",
            "Epoch [20/100], Step [ 44/ 78], Loss: 0.6210\n",
            "Epoch [20/100], Step [ 45/ 78], Loss: 0.6071\n",
            "Epoch [20/100], Step [ 46/ 78], Loss: 0.6088\n",
            "Epoch [20/100], Step [ 47/ 78], Loss: 0.6229\n",
            "Epoch [20/100], Step [ 48/ 78], Loss: 0.6584\n",
            "Epoch [20/100], Step [ 49/ 78], Loss: 0.6264\n",
            "Epoch [20/100], Step [ 50/ 78], Loss: 0.6041\n",
            "Epoch [20/100], Step [ 51/ 78], Loss: 0.6049\n",
            "Epoch [20/100], Step [ 52/ 78], Loss: 0.6463\n",
            "Epoch [20/100], Step [ 53/ 78], Loss: 0.6189\n",
            "Epoch [20/100], Step [ 54/ 78], Loss: 0.6129\n",
            "Epoch [20/100], Step [ 55/ 78], Loss: 0.6296\n",
            "Epoch [20/100], Step [ 56/ 78], Loss: 0.6176\n",
            "Epoch [20/100], Step [ 57/ 78], Loss: 0.6238\n",
            "Epoch [20/100], Step [ 58/ 78], Loss: 0.6329\n",
            "Epoch [20/100], Step [ 59/ 78], Loss: 0.6577\n",
            "Epoch [20/100], Step [ 60/ 78], Loss: 0.6246\n",
            "Epoch [20/100], Step [ 61/ 78], Loss: 0.6540\n",
            "Epoch [20/100], Step [ 62/ 78], Loss: 0.6451\n",
            "Epoch [20/100], Step [ 63/ 78], Loss: 0.6512\n",
            "Epoch [21/100], Step [  1/ 78], Loss: 0.6346\n",
            "Epoch [21/100], Step [  2/ 78], Loss: 0.6201\n",
            "Epoch [21/100], Step [  3/ 78], Loss: 0.6427\n",
            "Epoch [21/100], Step [  4/ 78], Loss: 0.6033\n",
            "Epoch [21/100], Step [  5/ 78], Loss: 0.5974\n",
            "Epoch [21/100], Step [  6/ 78], Loss: 0.6252\n",
            "Epoch [21/100], Step [  7/ 78], Loss: 0.6471\n",
            "Epoch [21/100], Step [  8/ 78], Loss: 0.6267\n",
            "Epoch [21/100], Step [  9/ 78], Loss: 0.6275\n",
            "Epoch [21/100], Step [ 10/ 78], Loss: 0.6330\n",
            "Epoch [21/100], Step [ 11/ 78], Loss: 0.6244\n",
            "Epoch [21/100], Step [ 12/ 78], Loss: 0.6231\n",
            "Epoch [21/100], Step [ 13/ 78], Loss: 0.6125\n",
            "Epoch [21/100], Step [ 14/ 78], Loss: 0.6445\n",
            "Epoch [21/100], Step [ 15/ 78], Loss: 0.6288\n",
            "Epoch [21/100], Step [ 16/ 78], Loss: 0.6071\n",
            "Epoch [21/100], Step [ 17/ 78], Loss: 0.6523\n",
            "Epoch [21/100], Step [ 18/ 78], Loss: 0.6555\n",
            "Epoch [21/100], Step [ 19/ 78], Loss: 0.6151\n",
            "Epoch [21/100], Step [ 20/ 78], Loss: 0.6324\n",
            "Epoch [21/100], Step [ 21/ 78], Loss: 0.6242\n",
            "Epoch [21/100], Step [ 22/ 78], Loss: 0.6251\n",
            "Epoch [21/100], Step [ 23/ 78], Loss: 0.6279\n",
            "Epoch [21/100], Step [ 24/ 78], Loss: 0.5939\n",
            "Epoch [21/100], Step [ 25/ 78], Loss: 0.6256\n",
            "Epoch [21/100], Step [ 26/ 78], Loss: 0.6382\n",
            "Epoch [21/100], Step [ 27/ 78], Loss: 0.6349\n",
            "Epoch [21/100], Step [ 28/ 78], Loss: 0.6609\n",
            "Epoch [21/100], Step [ 29/ 78], Loss: 0.6296\n",
            "Epoch [21/100], Step [ 30/ 78], Loss: 0.6196\n",
            "Epoch [21/100], Step [ 31/ 78], Loss: 0.6063\n",
            "Epoch [21/100], Step [ 32/ 78], Loss: 0.6046\n",
            "Epoch [21/100], Step [ 33/ 78], Loss: 0.6268\n",
            "Epoch [21/100], Step [ 34/ 78], Loss: 0.6387\n",
            "Epoch [21/100], Step [ 35/ 78], Loss: 0.6354\n",
            "Epoch [21/100], Step [ 36/ 78], Loss: 0.6317\n",
            "Epoch [21/100], Step [ 37/ 78], Loss: 0.6378\n",
            "Epoch [21/100], Step [ 38/ 78], Loss: 0.6076\n",
            "Epoch [21/100], Step [ 39/ 78], Loss: 0.5946\n",
            "Epoch [21/100], Step [ 40/ 78], Loss: 0.5918\n",
            "Epoch [21/100], Step [ 41/ 78], Loss: 0.6286\n",
            "Epoch [21/100], Step [ 42/ 78], Loss: 0.6178\n",
            "Epoch [21/100], Step [ 43/ 78], Loss: 0.6086\n",
            "Epoch [21/100], Step [ 44/ 78], Loss: 0.6383\n",
            "Epoch [21/100], Step [ 45/ 78], Loss: 0.6425\n",
            "Epoch [21/100], Step [ 46/ 78], Loss: 0.6255\n",
            "Epoch [21/100], Step [ 47/ 78], Loss: 0.6709\n",
            "Epoch [21/100], Step [ 48/ 78], Loss: 0.6197\n",
            "Epoch [21/100], Step [ 49/ 78], Loss: 0.6308\n",
            "Epoch [21/100], Step [ 50/ 78], Loss: 0.6165\n",
            "Epoch [21/100], Step [ 51/ 78], Loss: 0.6077\n",
            "Epoch [21/100], Step [ 52/ 78], Loss: 0.6057\n",
            "Epoch [21/100], Step [ 53/ 78], Loss: 0.6119\n",
            "Epoch [21/100], Step [ 54/ 78], Loss: 0.6253\n",
            "Epoch [21/100], Step [ 55/ 78], Loss: 0.6055\n",
            "Epoch [21/100], Step [ 56/ 78], Loss: 0.6458\n",
            "Epoch [21/100], Step [ 57/ 78], Loss: 0.6548\n",
            "Epoch [21/100], Step [ 58/ 78], Loss: 0.6309\n",
            "Epoch [21/100], Step [ 59/ 78], Loss: 0.6365\n",
            "Epoch [21/100], Step [ 60/ 78], Loss: 0.6400\n",
            "Epoch [21/100], Step [ 61/ 78], Loss: 0.6017\n",
            "Epoch [21/100], Step [ 62/ 78], Loss: 0.6396\n",
            "Epoch [21/100], Step [ 63/ 78], Loss: 0.6273\n",
            "Epoch [22/100], Step [  1/ 78], Loss: 0.6172\n",
            "Epoch [22/100], Step [  2/ 78], Loss: 0.6360\n",
            "Epoch [22/100], Step [  3/ 78], Loss: 0.6363\n",
            "Epoch [22/100], Step [  4/ 78], Loss: 0.5864\n",
            "Epoch [22/100], Step [  5/ 78], Loss: 0.6048\n",
            "Epoch [22/100], Step [  6/ 78], Loss: 0.6150\n",
            "Epoch [22/100], Step [  7/ 78], Loss: 0.5987\n",
            "Epoch [22/100], Step [  8/ 78], Loss: 0.6318\n",
            "Epoch [22/100], Step [  9/ 78], Loss: 0.6210\n",
            "Epoch [22/100], Step [ 10/ 78], Loss: 0.6237\n",
            "Epoch [22/100], Step [ 11/ 78], Loss: 0.6248\n",
            "Epoch [22/100], Step [ 12/ 78], Loss: 0.6276\n",
            "Epoch [22/100], Step [ 13/ 78], Loss: 0.6393\n",
            "Epoch [22/100], Step [ 14/ 78], Loss: 0.5907\n",
            "Epoch [22/100], Step [ 15/ 78], Loss: 0.6213\n",
            "Epoch [22/100], Step [ 16/ 78], Loss: 0.6320\n",
            "Epoch [22/100], Step [ 17/ 78], Loss: 0.5998\n",
            "Epoch [22/100], Step [ 18/ 78], Loss: 0.6273\n",
            "Epoch [22/100], Step [ 19/ 78], Loss: 0.6412\n",
            "Epoch [22/100], Step [ 20/ 78], Loss: 0.6920\n",
            "Epoch [22/100], Step [ 21/ 78], Loss: 0.6180\n",
            "Epoch [22/100], Step [ 22/ 78], Loss: 0.6646\n",
            "Epoch [22/100], Step [ 23/ 78], Loss: 0.6103\n",
            "Epoch [22/100], Step [ 24/ 78], Loss: 0.6303\n",
            "Epoch [22/100], Step [ 25/ 78], Loss: 0.6860\n",
            "Epoch [22/100], Step [ 26/ 78], Loss: 0.6143\n",
            "Epoch [22/100], Step [ 27/ 78], Loss: 0.6129\n",
            "Epoch [22/100], Step [ 28/ 78], Loss: 0.6033\n",
            "Epoch [22/100], Step [ 29/ 78], Loss: 0.6792\n",
            "Epoch [22/100], Step [ 30/ 78], Loss: 0.6031\n",
            "Epoch [22/100], Step [ 31/ 78], Loss: 0.6113\n",
            "Epoch [22/100], Step [ 32/ 78], Loss: 0.5979\n",
            "Epoch [22/100], Step [ 33/ 78], Loss: 0.6470\n",
            "Epoch [22/100], Step [ 34/ 78], Loss: 0.6419\n",
            "Epoch [22/100], Step [ 35/ 78], Loss: 0.6516\n",
            "Epoch [22/100], Step [ 36/ 78], Loss: 0.6305\n",
            "Epoch [22/100], Step [ 37/ 78], Loss: 0.6590\n",
            "Epoch [22/100], Step [ 38/ 78], Loss: 0.6699\n",
            "Epoch [22/100], Step [ 39/ 78], Loss: 0.6746\n",
            "Epoch [22/100], Step [ 40/ 78], Loss: 0.6304\n",
            "Epoch [22/100], Step [ 41/ 78], Loss: 0.6360\n",
            "Epoch [22/100], Step [ 42/ 78], Loss: 0.6507\n",
            "Epoch [22/100], Step [ 43/ 78], Loss: 0.6128\n",
            "Epoch [22/100], Step [ 44/ 78], Loss: 0.6143\n",
            "Epoch [22/100], Step [ 45/ 78], Loss: 0.6260\n",
            "Epoch [22/100], Step [ 46/ 78], Loss: 0.6173\n",
            "Epoch [22/100], Step [ 47/ 78], Loss: 0.6168\n",
            "Epoch [22/100], Step [ 48/ 78], Loss: 0.6192\n",
            "Epoch [22/100], Step [ 49/ 78], Loss: 0.6099\n",
            "Epoch [22/100], Step [ 50/ 78], Loss: 0.6229\n",
            "Epoch [22/100], Step [ 51/ 78], Loss: 0.6479\n",
            "Epoch [22/100], Step [ 52/ 78], Loss: 0.6611\n",
            "Epoch [22/100], Step [ 53/ 78], Loss: 0.6471\n",
            "Epoch [22/100], Step [ 54/ 78], Loss: 0.5999\n",
            "Epoch [22/100], Step [ 55/ 78], Loss: 0.6286\n",
            "Epoch [22/100], Step [ 56/ 78], Loss: 0.6185\n",
            "Epoch [22/100], Step [ 57/ 78], Loss: 0.6148\n",
            "Epoch [22/100], Step [ 58/ 78], Loss: 0.6111\n",
            "Epoch [22/100], Step [ 59/ 78], Loss: 0.6254\n",
            "Epoch [22/100], Step [ 60/ 78], Loss: 0.6188\n",
            "Epoch [22/100], Step [ 61/ 78], Loss: 0.6263\n",
            "Epoch [22/100], Step [ 62/ 78], Loss: 0.6129\n",
            "Epoch [22/100], Step [ 63/ 78], Loss: 0.6249\n",
            "Epoch [23/100], Step [  1/ 78], Loss: 0.6195\n",
            "Epoch [23/100], Step [  2/ 78], Loss: 0.6580\n",
            "Epoch [23/100], Step [  3/ 78], Loss: 0.6128\n",
            "Epoch [23/100], Step [  4/ 78], Loss: 0.5993\n",
            "Epoch [23/100], Step [  5/ 78], Loss: 0.6135\n",
            "Epoch [23/100], Step [  6/ 78], Loss: 0.6087\n",
            "Epoch [23/100], Step [  7/ 78], Loss: 0.6358\n",
            "Epoch [23/100], Step [  8/ 78], Loss: 0.6074\n",
            "Epoch [23/100], Step [  9/ 78], Loss: 0.6203\n",
            "Epoch [23/100], Step [ 10/ 78], Loss: 0.5983\n",
            "Epoch [23/100], Step [ 11/ 78], Loss: 0.6013\n",
            "Epoch [23/100], Step [ 12/ 78], Loss: 0.6349\n",
            "Epoch [23/100], Step [ 13/ 78], Loss: 0.6140\n",
            "Epoch [23/100], Step [ 14/ 78], Loss: 0.5969\n",
            "Epoch [23/100], Step [ 15/ 78], Loss: 0.6328\n",
            "Epoch [23/100], Step [ 16/ 78], Loss: 0.6162\n",
            "Epoch [23/100], Step [ 17/ 78], Loss: 0.6232\n",
            "Epoch [23/100], Step [ 18/ 78], Loss: 0.6196\n",
            "Epoch [23/100], Step [ 19/ 78], Loss: 0.6259\n",
            "Epoch [23/100], Step [ 20/ 78], Loss: 0.6244\n",
            "Epoch [23/100], Step [ 21/ 78], Loss: 0.6194\n",
            "Epoch [23/100], Step [ 22/ 78], Loss: 0.6280\n",
            "Epoch [23/100], Step [ 23/ 78], Loss: 0.6372\n",
            "Epoch [23/100], Step [ 24/ 78], Loss: 0.6221\n",
            "Epoch [23/100], Step [ 25/ 78], Loss: 0.6326\n",
            "Epoch [23/100], Step [ 26/ 78], Loss: 0.6114\n",
            "Epoch [23/100], Step [ 27/ 78], Loss: 0.6232\n",
            "Epoch [23/100], Step [ 28/ 78], Loss: 0.6272\n",
            "Epoch [23/100], Step [ 29/ 78], Loss: 0.6081\n",
            "Epoch [23/100], Step [ 30/ 78], Loss: 0.6112\n",
            "Epoch [23/100], Step [ 31/ 78], Loss: 0.6394\n",
            "Epoch [23/100], Step [ 32/ 78], Loss: 0.6242\n",
            "Epoch [23/100], Step [ 33/ 78], Loss: 0.6217\n",
            "Epoch [23/100], Step [ 34/ 78], Loss: 0.6158\n",
            "Epoch [23/100], Step [ 35/ 78], Loss: 0.6369\n",
            "Epoch [23/100], Step [ 36/ 78], Loss: 0.6120\n",
            "Epoch [23/100], Step [ 37/ 78], Loss: 0.6407\n",
            "Epoch [23/100], Step [ 38/ 78], Loss: 0.6028\n",
            "Epoch [23/100], Step [ 39/ 78], Loss: 0.6364\n",
            "Epoch [23/100], Step [ 40/ 78], Loss: 0.6057\n",
            "Epoch [23/100], Step [ 41/ 78], Loss: 0.6641\n",
            "Epoch [23/100], Step [ 42/ 78], Loss: 0.6023\n",
            "Epoch [23/100], Step [ 43/ 78], Loss: 0.6424\n",
            "Epoch [23/100], Step [ 44/ 78], Loss: 0.6115\n",
            "Epoch [23/100], Step [ 45/ 78], Loss: 0.6189\n",
            "Epoch [23/100], Step [ 46/ 78], Loss: 0.6240\n",
            "Epoch [23/100], Step [ 47/ 78], Loss: 0.5918\n",
            "Epoch [23/100], Step [ 48/ 78], Loss: 0.6101\n",
            "Epoch [23/100], Step [ 49/ 78], Loss: 0.6106\n",
            "Epoch [23/100], Step [ 50/ 78], Loss: 0.5897\n",
            "Epoch [23/100], Step [ 51/ 78], Loss: 0.6373\n",
            "Epoch [23/100], Step [ 52/ 78], Loss: 0.5982\n",
            "Epoch [23/100], Step [ 53/ 78], Loss: 0.6098\n",
            "Epoch [23/100], Step [ 54/ 78], Loss: 0.6477\n",
            "Epoch [23/100], Step [ 55/ 78], Loss: 0.6529\n",
            "Epoch [23/100], Step [ 56/ 78], Loss: 0.6262\n",
            "Epoch [23/100], Step [ 57/ 78], Loss: 0.6075\n",
            "Epoch [23/100], Step [ 58/ 78], Loss: 0.6345\n",
            "Epoch [23/100], Step [ 59/ 78], Loss: 0.6102\n",
            "Epoch [23/100], Step [ 60/ 78], Loss: 0.6393\n",
            "Epoch [23/100], Step [ 61/ 78], Loss: 0.6227\n",
            "Epoch [23/100], Step [ 62/ 78], Loss: 0.5975\n",
            "Epoch [23/100], Step [ 63/ 78], Loss: 0.6624\n",
            "Epoch [24/100], Step [  1/ 78], Loss: 0.6041\n",
            "Epoch [24/100], Step [  2/ 78], Loss: 0.6241\n",
            "Epoch [24/100], Step [  3/ 78], Loss: 0.6099\n",
            "Epoch [24/100], Step [  4/ 78], Loss: 0.5986\n",
            "Epoch [24/100], Step [  5/ 78], Loss: 0.6456\n",
            "Epoch [24/100], Step [  6/ 78], Loss: 0.6361\n",
            "Epoch [24/100], Step [  7/ 78], Loss: 0.6027\n",
            "Epoch [24/100], Step [  8/ 78], Loss: 0.6241\n",
            "Epoch [24/100], Step [  9/ 78], Loss: 0.5936\n",
            "Epoch [24/100], Step [ 10/ 78], Loss: 0.6018\n",
            "Epoch [24/100], Step [ 11/ 78], Loss: 0.6194\n",
            "Epoch [24/100], Step [ 12/ 78], Loss: 0.6126\n",
            "Epoch [24/100], Step [ 13/ 78], Loss: 0.6017\n",
            "Epoch [24/100], Step [ 14/ 78], Loss: 0.6301\n",
            "Epoch [24/100], Step [ 15/ 78], Loss: 0.6105\n",
            "Epoch [24/100], Step [ 16/ 78], Loss: 0.6174\n",
            "Epoch [24/100], Step [ 17/ 78], Loss: 0.6391\n",
            "Epoch [24/100], Step [ 18/ 78], Loss: 0.6144\n",
            "Epoch [24/100], Step [ 19/ 78], Loss: 0.6270\n",
            "Epoch [24/100], Step [ 20/ 78], Loss: 0.6316\n",
            "Epoch [24/100], Step [ 21/ 78], Loss: 0.6013\n",
            "Epoch [24/100], Step [ 22/ 78], Loss: 0.5821\n",
            "Epoch [24/100], Step [ 23/ 78], Loss: 0.6001\n",
            "Epoch [24/100], Step [ 24/ 78], Loss: 0.6375\n",
            "Epoch [24/100], Step [ 25/ 78], Loss: 0.6239\n",
            "Epoch [24/100], Step [ 26/ 78], Loss: 0.6023\n",
            "Epoch [24/100], Step [ 27/ 78], Loss: 0.6329\n",
            "Epoch [24/100], Step [ 28/ 78], Loss: 0.6133\n",
            "Epoch [24/100], Step [ 29/ 78], Loss: 0.6474\n",
            "Epoch [24/100], Step [ 30/ 78], Loss: 0.6240\n",
            "Epoch [24/100], Step [ 31/ 78], Loss: 0.6040\n",
            "Epoch [24/100], Step [ 32/ 78], Loss: 0.5770\n",
            "Epoch [24/100], Step [ 33/ 78], Loss: 0.6272\n",
            "Epoch [24/100], Step [ 34/ 78], Loss: 0.6179\n",
            "Epoch [24/100], Step [ 35/ 78], Loss: 0.6157\n",
            "Epoch [24/100], Step [ 36/ 78], Loss: 0.6006\n",
            "Epoch [24/100], Step [ 37/ 78], Loss: 0.6361\n",
            "Epoch [24/100], Step [ 38/ 78], Loss: 0.6371\n",
            "Epoch [24/100], Step [ 39/ 78], Loss: 0.6513\n",
            "Epoch [24/100], Step [ 40/ 78], Loss: 0.6182\n",
            "Epoch [24/100], Step [ 41/ 78], Loss: 0.5988\n",
            "Epoch [24/100], Step [ 42/ 78], Loss: 0.6189\n",
            "Epoch [24/100], Step [ 43/ 78], Loss: 0.6246\n",
            "Epoch [24/100], Step [ 44/ 78], Loss: 0.6183\n",
            "Epoch [24/100], Step [ 45/ 78], Loss: 0.6215\n",
            "Epoch [24/100], Step [ 46/ 78], Loss: 0.6325\n",
            "Epoch [24/100], Step [ 47/ 78], Loss: 0.6418\n",
            "Epoch [24/100], Step [ 48/ 78], Loss: 0.6546\n",
            "Epoch [24/100], Step [ 49/ 78], Loss: 0.6242\n",
            "Epoch [24/100], Step [ 50/ 78], Loss: 0.6262\n",
            "Epoch [24/100], Step [ 51/ 78], Loss: 0.6221\n",
            "Epoch [24/100], Step [ 52/ 78], Loss: 0.6063\n",
            "Epoch [24/100], Step [ 53/ 78], Loss: 0.5796\n",
            "Epoch [24/100], Step [ 54/ 78], Loss: 0.6032\n",
            "Epoch [24/100], Step [ 55/ 78], Loss: 0.6282\n",
            "Epoch [24/100], Step [ 56/ 78], Loss: 0.6443\n",
            "Epoch [24/100], Step [ 57/ 78], Loss: 0.6385\n",
            "Epoch [24/100], Step [ 58/ 78], Loss: 0.6110\n",
            "Epoch [24/100], Step [ 59/ 78], Loss: 0.5986\n",
            "Epoch [24/100], Step [ 60/ 78], Loss: 0.5876\n",
            "Epoch [24/100], Step [ 61/ 78], Loss: 0.5997\n",
            "Epoch [24/100], Step [ 62/ 78], Loss: 0.6135\n",
            "Epoch [24/100], Step [ 63/ 78], Loss: 0.6111\n",
            "Epoch [25/100], Step [  1/ 78], Loss: 0.6165\n",
            "Epoch [25/100], Step [  2/ 78], Loss: 0.6099\n",
            "Epoch [25/100], Step [  3/ 78], Loss: 0.5980\n",
            "Epoch [25/100], Step [  4/ 78], Loss: 0.5976\n",
            "Epoch [25/100], Step [  5/ 78], Loss: 0.6134\n",
            "Epoch [25/100], Step [  6/ 78], Loss: 0.6574\n",
            "Epoch [25/100], Step [  7/ 78], Loss: 0.6594\n",
            "Epoch [25/100], Step [  8/ 78], Loss: 0.6327\n",
            "Epoch [25/100], Step [  9/ 78], Loss: 0.6087\n",
            "Epoch [25/100], Step [ 10/ 78], Loss: 0.6246\n",
            "Epoch [25/100], Step [ 11/ 78], Loss: 0.6092\n",
            "Epoch [25/100], Step [ 12/ 78], Loss: 0.6362\n",
            "Epoch [25/100], Step [ 13/ 78], Loss: 0.6162\n",
            "Epoch [25/100], Step [ 14/ 78], Loss: 0.6399\n",
            "Epoch [25/100], Step [ 15/ 78], Loss: 0.6169\n",
            "Epoch [25/100], Step [ 16/ 78], Loss: 0.6237\n",
            "Epoch [25/100], Step [ 17/ 78], Loss: 0.6271\n",
            "Epoch [25/100], Step [ 18/ 78], Loss: 0.6261\n",
            "Epoch [25/100], Step [ 19/ 78], Loss: 0.6347\n",
            "Epoch [25/100], Step [ 20/ 78], Loss: 0.6172\n",
            "Epoch [25/100], Step [ 21/ 78], Loss: 0.6373\n",
            "Epoch [25/100], Step [ 22/ 78], Loss: 0.5913\n",
            "Epoch [25/100], Step [ 23/ 78], Loss: 0.6179\n",
            "Epoch [25/100], Step [ 24/ 78], Loss: 0.6402\n",
            "Epoch [25/100], Step [ 25/ 78], Loss: 0.6461\n",
            "Epoch [25/100], Step [ 26/ 78], Loss: 0.6219\n",
            "Epoch [25/100], Step [ 27/ 78], Loss: 0.6272\n",
            "Epoch [25/100], Step [ 28/ 78], Loss: 0.6193\n",
            "Epoch [25/100], Step [ 29/ 78], Loss: 0.6033\n",
            "Epoch [25/100], Step [ 30/ 78], Loss: 0.6317\n",
            "Epoch [25/100], Step [ 31/ 78], Loss: 0.6064\n",
            "Epoch [25/100], Step [ 32/ 78], Loss: 0.6220\n",
            "Epoch [25/100], Step [ 33/ 78], Loss: 0.6147\n",
            "Epoch [25/100], Step [ 34/ 78], Loss: 0.5831\n",
            "Epoch [25/100], Step [ 35/ 78], Loss: 0.6010\n",
            "Epoch [25/100], Step [ 36/ 78], Loss: 0.6252\n",
            "Epoch [25/100], Step [ 37/ 78], Loss: 0.6108\n",
            "Epoch [25/100], Step [ 38/ 78], Loss: 0.6244\n",
            "Epoch [25/100], Step [ 39/ 78], Loss: 0.6068\n",
            "Epoch [25/100], Step [ 40/ 78], Loss: 0.6142\n",
            "Epoch [25/100], Step [ 41/ 78], Loss: 0.5874\n",
            "Epoch [25/100], Step [ 42/ 78], Loss: 0.6192\n",
            "Epoch [25/100], Step [ 43/ 78], Loss: 0.6250\n",
            "Epoch [25/100], Step [ 44/ 78], Loss: 0.5938\n",
            "Epoch [25/100], Step [ 45/ 78], Loss: 0.6538\n",
            "Epoch [25/100], Step [ 46/ 78], Loss: 0.6138\n",
            "Epoch [25/100], Step [ 47/ 78], Loss: 0.6301\n",
            "Epoch [25/100], Step [ 48/ 78], Loss: 0.6085\n",
            "Epoch [25/100], Step [ 49/ 78], Loss: 0.6304\n",
            "Epoch [25/100], Step [ 50/ 78], Loss: 0.5848\n",
            "Epoch [25/100], Step [ 51/ 78], Loss: 0.5954\n",
            "Epoch [25/100], Step [ 52/ 78], Loss: 0.6096\n",
            "Epoch [25/100], Step [ 53/ 78], Loss: 0.6300\n",
            "Epoch [25/100], Step [ 54/ 78], Loss: 0.6007\n",
            "Epoch [25/100], Step [ 55/ 78], Loss: 0.5935\n",
            "Epoch [25/100], Step [ 56/ 78], Loss: 0.5926\n",
            "Epoch [25/100], Step [ 57/ 78], Loss: 0.5726\n",
            "Epoch [25/100], Step [ 58/ 78], Loss: 0.5985\n",
            "Epoch [25/100], Step [ 59/ 78], Loss: 0.6413\n",
            "Epoch [25/100], Step [ 60/ 78], Loss: 0.6109\n",
            "Epoch [25/100], Step [ 61/ 78], Loss: 0.6651\n",
            "Epoch [25/100], Step [ 62/ 78], Loss: 0.6211\n",
            "Epoch [25/100], Step [ 63/ 78], Loss: 0.5993\n",
            "Epoch [26/100], Step [  1/ 78], Loss: 0.6158\n",
            "Epoch [26/100], Step [  2/ 78], Loss: 0.6153\n",
            "Epoch [26/100], Step [  3/ 78], Loss: 0.5877\n",
            "Epoch [26/100], Step [  4/ 78], Loss: 0.5948\n",
            "Epoch [26/100], Step [  5/ 78], Loss: 0.5912\n",
            "Epoch [26/100], Step [  6/ 78], Loss: 0.6312\n",
            "Epoch [26/100], Step [  7/ 78], Loss: 0.6056\n",
            "Epoch [26/100], Step [  8/ 78], Loss: 0.6214\n",
            "Epoch [26/100], Step [  9/ 78], Loss: 0.5762\n",
            "Epoch [26/100], Step [ 10/ 78], Loss: 0.5928\n",
            "Epoch [26/100], Step [ 11/ 78], Loss: 0.5903\n",
            "Epoch [26/100], Step [ 12/ 78], Loss: 0.6217\n",
            "Epoch [26/100], Step [ 13/ 78], Loss: 0.5893\n",
            "Epoch [26/100], Step [ 14/ 78], Loss: 0.6401\n",
            "Epoch [26/100], Step [ 15/ 78], Loss: 0.5976\n",
            "Epoch [26/100], Step [ 16/ 78], Loss: 0.6450\n",
            "Epoch [26/100], Step [ 17/ 78], Loss: 0.6375\n",
            "Epoch [26/100], Step [ 18/ 78], Loss: 0.6145\n",
            "Epoch [26/100], Step [ 19/ 78], Loss: 0.6081\n",
            "Epoch [26/100], Step [ 20/ 78], Loss: 0.6072\n",
            "Epoch [26/100], Step [ 21/ 78], Loss: 0.6174\n",
            "Epoch [26/100], Step [ 22/ 78], Loss: 0.5967\n",
            "Epoch [26/100], Step [ 23/ 78], Loss: 0.6457\n",
            "Epoch [26/100], Step [ 24/ 78], Loss: 0.6643\n",
            "Epoch [26/100], Step [ 25/ 78], Loss: 0.6338\n",
            "Epoch [26/100], Step [ 26/ 78], Loss: 0.6280\n",
            "Epoch [26/100], Step [ 27/ 78], Loss: 0.6172\n",
            "Epoch [26/100], Step [ 28/ 78], Loss: 0.5957\n",
            "Epoch [26/100], Step [ 29/ 78], Loss: 0.5851\n",
            "Epoch [26/100], Step [ 30/ 78], Loss: 0.6138\n",
            "Epoch [26/100], Step [ 31/ 78], Loss: 0.5983\n",
            "Epoch [26/100], Step [ 32/ 78], Loss: 0.6024\n",
            "Epoch [26/100], Step [ 33/ 78], Loss: 0.6410\n",
            "Epoch [26/100], Step [ 34/ 78], Loss: 0.5982\n",
            "Epoch [26/100], Step [ 35/ 78], Loss: 0.6072\n",
            "Epoch [26/100], Step [ 36/ 78], Loss: 0.5952\n",
            "Epoch [26/100], Step [ 37/ 78], Loss: 0.6061\n",
            "Epoch [26/100], Step [ 38/ 78], Loss: 0.6328\n",
            "Epoch [26/100], Step [ 39/ 78], Loss: 0.6213\n",
            "Epoch [26/100], Step [ 40/ 78], Loss: 0.6555\n",
            "Epoch [26/100], Step [ 41/ 78], Loss: 0.6385\n",
            "Epoch [26/100], Step [ 42/ 78], Loss: 0.6504\n",
            "Epoch [26/100], Step [ 43/ 78], Loss: 0.6130\n",
            "Epoch [26/100], Step [ 44/ 78], Loss: 0.6365\n",
            "Epoch [26/100], Step [ 45/ 78], Loss: 0.6080\n",
            "Epoch [26/100], Step [ 46/ 78], Loss: 0.6210\n",
            "Epoch [26/100], Step [ 47/ 78], Loss: 0.6203\n",
            "Epoch [26/100], Step [ 48/ 78], Loss: 0.5929\n",
            "Epoch [26/100], Step [ 49/ 78], Loss: 0.6185\n",
            "Epoch [26/100], Step [ 50/ 78], Loss: 0.6283\n",
            "Epoch [26/100], Step [ 51/ 78], Loss: 0.5835\n",
            "Epoch [26/100], Step [ 52/ 78], Loss: 0.6163\n",
            "Epoch [26/100], Step [ 53/ 78], Loss: 0.6030\n",
            "Epoch [26/100], Step [ 54/ 78], Loss: 0.6338\n",
            "Epoch [26/100], Step [ 55/ 78], Loss: 0.6432\n",
            "Epoch [26/100], Step [ 56/ 78], Loss: 0.6282\n",
            "Epoch [26/100], Step [ 57/ 78], Loss: 0.5751\n",
            "Epoch [26/100], Step [ 58/ 78], Loss: 0.5998\n",
            "Epoch [26/100], Step [ 59/ 78], Loss: 0.6193\n",
            "Epoch [26/100], Step [ 60/ 78], Loss: 0.5971\n",
            "Epoch [26/100], Step [ 61/ 78], Loss: 0.6212\n",
            "Epoch [26/100], Step [ 62/ 78], Loss: 0.6243\n",
            "Epoch [26/100], Step [ 63/ 78], Loss: 0.6173\n",
            "Epoch [27/100], Step [  1/ 78], Loss: 0.5982\n",
            "Epoch [27/100], Step [  2/ 78], Loss: 0.5870\n",
            "Epoch [27/100], Step [  3/ 78], Loss: 0.6105\n",
            "Epoch [27/100], Step [  4/ 78], Loss: 0.6153\n",
            "Epoch [27/100], Step [  5/ 78], Loss: 0.6187\n",
            "Epoch [27/100], Step [  6/ 78], Loss: 0.6120\n",
            "Epoch [27/100], Step [  7/ 78], Loss: 0.5925\n",
            "Epoch [27/100], Step [  8/ 78], Loss: 0.5947\n",
            "Epoch [27/100], Step [  9/ 78], Loss: 0.6168\n",
            "Epoch [27/100], Step [ 10/ 78], Loss: 0.5877\n",
            "Epoch [27/100], Step [ 11/ 78], Loss: 0.6042\n",
            "Epoch [27/100], Step [ 12/ 78], Loss: 0.5997\n",
            "Epoch [27/100], Step [ 13/ 78], Loss: 0.6163\n",
            "Epoch [27/100], Step [ 14/ 78], Loss: 0.5825\n",
            "Epoch [27/100], Step [ 15/ 78], Loss: 0.6067\n",
            "Epoch [27/100], Step [ 16/ 78], Loss: 0.6272\n",
            "Epoch [27/100], Step [ 17/ 78], Loss: 0.6196\n",
            "Epoch [27/100], Step [ 18/ 78], Loss: 0.6331\n",
            "Epoch [27/100], Step [ 19/ 78], Loss: 0.6114\n",
            "Epoch [27/100], Step [ 20/ 78], Loss: 0.6091\n",
            "Epoch [27/100], Step [ 21/ 78], Loss: 0.6181\n",
            "Epoch [27/100], Step [ 22/ 78], Loss: 0.6205\n",
            "Epoch [27/100], Step [ 23/ 78], Loss: 0.5879\n",
            "Epoch [27/100], Step [ 24/ 78], Loss: 0.5935\n",
            "Epoch [27/100], Step [ 25/ 78], Loss: 0.5791\n",
            "Epoch [27/100], Step [ 26/ 78], Loss: 0.5925\n",
            "Epoch [27/100], Step [ 27/ 78], Loss: 0.5905\n",
            "Epoch [27/100], Step [ 28/ 78], Loss: 0.6040\n",
            "Epoch [27/100], Step [ 29/ 78], Loss: 0.5896\n",
            "Epoch [27/100], Step [ 30/ 78], Loss: 0.6063\n",
            "Epoch [27/100], Step [ 31/ 78], Loss: 0.6395\n",
            "Epoch [27/100], Step [ 32/ 78], Loss: 0.6275\n",
            "Epoch [27/100], Step [ 33/ 78], Loss: 0.5962\n",
            "Epoch [27/100], Step [ 34/ 78], Loss: 0.6012\n",
            "Epoch [27/100], Step [ 35/ 78], Loss: 0.5963\n",
            "Epoch [27/100], Step [ 36/ 78], Loss: 0.6243\n",
            "Epoch [27/100], Step [ 37/ 78], Loss: 0.5923\n",
            "Epoch [27/100], Step [ 38/ 78], Loss: 0.6207\n",
            "Epoch [27/100], Step [ 39/ 78], Loss: 0.6043\n",
            "Epoch [27/100], Step [ 40/ 78], Loss: 0.6023\n",
            "Epoch [27/100], Step [ 41/ 78], Loss: 0.5937\n",
            "Epoch [27/100], Step [ 42/ 78], Loss: 0.6025\n",
            "Epoch [27/100], Step [ 43/ 78], Loss: 0.5941\n",
            "Epoch [27/100], Step [ 44/ 78], Loss: 0.6037\n",
            "Epoch [27/100], Step [ 45/ 78], Loss: 0.6350\n",
            "Epoch [27/100], Step [ 46/ 78], Loss: 0.6509\n",
            "Epoch [27/100], Step [ 47/ 78], Loss: 0.6206\n",
            "Epoch [27/100], Step [ 48/ 78], Loss: 0.5999\n",
            "Epoch [27/100], Step [ 49/ 78], Loss: 0.6236\n",
            "Epoch [27/100], Step [ 50/ 78], Loss: 0.6399\n",
            "Epoch [27/100], Step [ 51/ 78], Loss: 0.6066\n",
            "Epoch [27/100], Step [ 52/ 78], Loss: 0.6622\n",
            "Epoch [27/100], Step [ 53/ 78], Loss: 0.6138\n",
            "Epoch [27/100], Step [ 54/ 78], Loss: 0.6071\n",
            "Epoch [27/100], Step [ 55/ 78], Loss: 0.6007\n",
            "Epoch [27/100], Step [ 56/ 78], Loss: 0.5936\n",
            "Epoch [27/100], Step [ 57/ 78], Loss: 0.5841\n",
            "Epoch [27/100], Step [ 58/ 78], Loss: 0.6447\n",
            "Epoch [27/100], Step [ 59/ 78], Loss: 0.6567\n",
            "Epoch [27/100], Step [ 60/ 78], Loss: 0.6229\n",
            "Epoch [27/100], Step [ 61/ 78], Loss: 0.5914\n",
            "Epoch [27/100], Step [ 62/ 78], Loss: 0.6232\n",
            "Epoch [27/100], Step [ 63/ 78], Loss: 0.6342\n",
            "Epoch [28/100], Step [  1/ 78], Loss: 0.6259\n",
            "Epoch [28/100], Step [  2/ 78], Loss: 0.5968\n",
            "Epoch [28/100], Step [  3/ 78], Loss: 0.5678\n",
            "Epoch [28/100], Step [  4/ 78], Loss: 0.6190\n",
            "Epoch [28/100], Step [  5/ 78], Loss: 0.6119\n",
            "Epoch [28/100], Step [  6/ 78], Loss: 0.6209\n",
            "Epoch [28/100], Step [  7/ 78], Loss: 0.6011\n",
            "Epoch [28/100], Step [  8/ 78], Loss: 0.5794\n",
            "Epoch [28/100], Step [  9/ 78], Loss: 0.6260\n",
            "Epoch [28/100], Step [ 10/ 78], Loss: 0.5988\n",
            "Epoch [28/100], Step [ 11/ 78], Loss: 0.5827\n",
            "Epoch [28/100], Step [ 12/ 78], Loss: 0.6033\n",
            "Epoch [28/100], Step [ 13/ 78], Loss: 0.6002\n",
            "Epoch [28/100], Step [ 14/ 78], Loss: 0.6201\n",
            "Epoch [28/100], Step [ 15/ 78], Loss: 0.6297\n",
            "Epoch [28/100], Step [ 16/ 78], Loss: 0.6000\n",
            "Epoch [28/100], Step [ 17/ 78], Loss: 0.6139\n",
            "Epoch [28/100], Step [ 18/ 78], Loss: 0.5932\n",
            "Epoch [28/100], Step [ 19/ 78], Loss: 0.5962\n",
            "Epoch [28/100], Step [ 20/ 78], Loss: 0.6144\n",
            "Epoch [28/100], Step [ 21/ 78], Loss: 0.5862\n",
            "Epoch [28/100], Step [ 22/ 78], Loss: 0.6452\n",
            "Epoch [28/100], Step [ 23/ 78], Loss: 0.6326\n",
            "Epoch [28/100], Step [ 24/ 78], Loss: 0.6075\n",
            "Epoch [28/100], Step [ 25/ 78], Loss: 0.5697\n",
            "Epoch [28/100], Step [ 26/ 78], Loss: 0.6094\n",
            "Epoch [28/100], Step [ 27/ 78], Loss: 0.5971\n",
            "Epoch [28/100], Step [ 28/ 78], Loss: 0.6036\n",
            "Epoch [28/100], Step [ 29/ 78], Loss: 0.5865\n",
            "Epoch [28/100], Step [ 30/ 78], Loss: 0.6478\n",
            "Epoch [28/100], Step [ 31/ 78], Loss: 0.6346\n",
            "Epoch [28/100], Step [ 32/ 78], Loss: 0.5845\n",
            "Epoch [28/100], Step [ 33/ 78], Loss: 0.5848\n",
            "Epoch [28/100], Step [ 34/ 78], Loss: 0.6249\n",
            "Epoch [28/100], Step [ 35/ 78], Loss: 0.5690\n",
            "Epoch [28/100], Step [ 36/ 78], Loss: 0.5924\n",
            "Epoch [28/100], Step [ 37/ 78], Loss: 0.5949\n",
            "Epoch [28/100], Step [ 38/ 78], Loss: 0.5849\n",
            "Epoch [28/100], Step [ 39/ 78], Loss: 0.5999\n",
            "Epoch [28/100], Step [ 40/ 78], Loss: 0.6456\n",
            "Epoch [28/100], Step [ 41/ 78], Loss: 0.6571\n",
            "Epoch [28/100], Step [ 42/ 78], Loss: 0.6519\n",
            "Epoch [28/100], Step [ 43/ 78], Loss: 0.6004\n",
            "Epoch [28/100], Step [ 44/ 78], Loss: 0.6240\n",
            "Epoch [28/100], Step [ 45/ 78], Loss: 0.6406\n",
            "Epoch [28/100], Step [ 46/ 78], Loss: 0.6139\n",
            "Epoch [28/100], Step [ 47/ 78], Loss: 0.6236\n",
            "Epoch [28/100], Step [ 48/ 78], Loss: 0.6066\n",
            "Epoch [28/100], Step [ 49/ 78], Loss: 0.6134\n",
            "Epoch [28/100], Step [ 50/ 78], Loss: 0.6150\n",
            "Epoch [28/100], Step [ 51/ 78], Loss: 0.6184\n",
            "Epoch [28/100], Step [ 52/ 78], Loss: 0.6047\n",
            "Epoch [28/100], Step [ 53/ 78], Loss: 0.6182\n",
            "Epoch [28/100], Step [ 54/ 78], Loss: 0.6282\n",
            "Epoch [28/100], Step [ 55/ 78], Loss: 0.5964\n",
            "Epoch [28/100], Step [ 56/ 78], Loss: 0.5785\n",
            "Epoch [28/100], Step [ 57/ 78], Loss: 0.5803\n",
            "Epoch [28/100], Step [ 58/ 78], Loss: 0.5850\n",
            "Epoch [28/100], Step [ 59/ 78], Loss: 0.6284\n",
            "Epoch [28/100], Step [ 60/ 78], Loss: 0.6213\n",
            "Epoch [28/100], Step [ 61/ 78], Loss: 0.5936\n",
            "Epoch [28/100], Step [ 62/ 78], Loss: 0.6401\n",
            "Epoch [28/100], Step [ 63/ 78], Loss: 0.6454\n",
            "Epoch [29/100], Step [  1/ 78], Loss: 0.5819\n",
            "Epoch [29/100], Step [  2/ 78], Loss: 0.6360\n",
            "Epoch [29/100], Step [  3/ 78], Loss: 0.6042\n",
            "Epoch [29/100], Step [  4/ 78], Loss: 0.6442\n",
            "Epoch [29/100], Step [  5/ 78], Loss: 0.6270\n",
            "Epoch [29/100], Step [  6/ 78], Loss: 0.5906\n",
            "Epoch [29/100], Step [  7/ 78], Loss: 0.5997\n",
            "Epoch [29/100], Step [  8/ 78], Loss: 0.5987\n",
            "Epoch [29/100], Step [  9/ 78], Loss: 0.6268\n",
            "Epoch [29/100], Step [ 10/ 78], Loss: 0.6385\n",
            "Epoch [29/100], Step [ 11/ 78], Loss: 0.6028\n",
            "Epoch [29/100], Step [ 12/ 78], Loss: 0.6045\n",
            "Epoch [29/100], Step [ 13/ 78], Loss: 0.5902\n",
            "Epoch [29/100], Step [ 14/ 78], Loss: 0.5855\n",
            "Epoch [29/100], Step [ 15/ 78], Loss: 0.5987\n",
            "Epoch [29/100], Step [ 16/ 78], Loss: 0.5939\n",
            "Epoch [29/100], Step [ 17/ 78], Loss: 0.6063\n",
            "Epoch [29/100], Step [ 18/ 78], Loss: 0.6004\n",
            "Epoch [29/100], Step [ 19/ 78], Loss: 0.5938\n",
            "Epoch [29/100], Step [ 20/ 78], Loss: 0.6292\n",
            "Epoch [29/100], Step [ 21/ 78], Loss: 0.5634\n",
            "Epoch [29/100], Step [ 22/ 78], Loss: 0.6402\n",
            "Epoch [29/100], Step [ 23/ 78], Loss: 0.5942\n",
            "Epoch [29/100], Step [ 24/ 78], Loss: 0.6176\n",
            "Epoch [29/100], Step [ 25/ 78], Loss: 0.6039\n",
            "Epoch [29/100], Step [ 26/ 78], Loss: 0.6236\n",
            "Epoch [29/100], Step [ 27/ 78], Loss: 0.6097\n",
            "Epoch [29/100], Step [ 28/ 78], Loss: 0.5995\n",
            "Epoch [29/100], Step [ 29/ 78], Loss: 0.6008\n",
            "Epoch [29/100], Step [ 30/ 78], Loss: 0.5685\n",
            "Epoch [29/100], Step [ 31/ 78], Loss: 0.5718\n",
            "Epoch [29/100], Step [ 32/ 78], Loss: 0.6472\n",
            "Epoch [29/100], Step [ 33/ 78], Loss: 0.6236\n",
            "Epoch [29/100], Step [ 34/ 78], Loss: 0.5656\n",
            "Epoch [29/100], Step [ 35/ 78], Loss: 0.5980\n",
            "Epoch [29/100], Step [ 36/ 78], Loss: 0.6113\n",
            "Epoch [29/100], Step [ 37/ 78], Loss: 0.6188\n",
            "Epoch [29/100], Step [ 38/ 78], Loss: 0.6370\n",
            "Epoch [29/100], Step [ 39/ 78], Loss: 0.5991\n",
            "Epoch [29/100], Step [ 40/ 78], Loss: 0.6354\n",
            "Epoch [29/100], Step [ 41/ 78], Loss: 0.6150\n",
            "Epoch [29/100], Step [ 42/ 78], Loss: 0.6334\n",
            "Epoch [29/100], Step [ 43/ 78], Loss: 0.5878\n",
            "Epoch [29/100], Step [ 44/ 78], Loss: 0.6329\n",
            "Epoch [29/100], Step [ 45/ 78], Loss: 0.6225\n",
            "Epoch [29/100], Step [ 46/ 78], Loss: 0.6070\n",
            "Epoch [29/100], Step [ 47/ 78], Loss: 0.5916\n",
            "Epoch [29/100], Step [ 48/ 78], Loss: 0.6132\n",
            "Epoch [29/100], Step [ 49/ 78], Loss: 0.6052\n",
            "Epoch [29/100], Step [ 50/ 78], Loss: 0.6281\n",
            "Epoch [29/100], Step [ 51/ 78], Loss: 0.6296\n",
            "Epoch [29/100], Step [ 52/ 78], Loss: 0.5811\n",
            "Epoch [29/100], Step [ 53/ 78], Loss: 0.6029\n",
            "Epoch [29/100], Step [ 54/ 78], Loss: 0.6038\n",
            "Epoch [29/100], Step [ 55/ 78], Loss: 0.6125\n",
            "Epoch [29/100], Step [ 56/ 78], Loss: 0.6134\n",
            "Epoch [29/100], Step [ 57/ 78], Loss: 0.6117\n",
            "Epoch [29/100], Step [ 58/ 78], Loss: 0.6015\n",
            "Epoch [29/100], Step [ 59/ 78], Loss: 0.5792\n",
            "Epoch [29/100], Step [ 60/ 78], Loss: 0.5792\n",
            "Epoch [29/100], Step [ 61/ 78], Loss: 0.5965\n",
            "Epoch [29/100], Step [ 62/ 78], Loss: 0.6173\n",
            "Epoch [29/100], Step [ 63/ 78], Loss: 0.5590\n",
            "Epoch [30/100], Step [  1/ 78], Loss: 0.5910\n",
            "Epoch [30/100], Step [  2/ 78], Loss: 0.6087\n",
            "Epoch [30/100], Step [  3/ 78], Loss: 0.6046\n",
            "Epoch [30/100], Step [  4/ 78], Loss: 0.6073\n",
            "Epoch [30/100], Step [  5/ 78], Loss: 0.5990\n",
            "Epoch [30/100], Step [  6/ 78], Loss: 0.5670\n",
            "Epoch [30/100], Step [  7/ 78], Loss: 0.5977\n",
            "Epoch [30/100], Step [  8/ 78], Loss: 0.6303\n",
            "Epoch [30/100], Step [  9/ 78], Loss: 0.6296\n",
            "Epoch [30/100], Step [ 10/ 78], Loss: 0.6120\n",
            "Epoch [30/100], Step [ 11/ 78], Loss: 0.6194\n",
            "Epoch [30/100], Step [ 12/ 78], Loss: 0.6211\n",
            "Epoch [30/100], Step [ 13/ 78], Loss: 0.5866\n",
            "Epoch [30/100], Step [ 14/ 78], Loss: 0.6191\n",
            "Epoch [30/100], Step [ 15/ 78], Loss: 0.6035\n",
            "Epoch [30/100], Step [ 16/ 78], Loss: 0.6427\n",
            "Epoch [30/100], Step [ 17/ 78], Loss: 0.5939\n",
            "Epoch [30/100], Step [ 18/ 78], Loss: 0.5847\n",
            "Epoch [30/100], Step [ 19/ 78], Loss: 0.6010\n",
            "Epoch [30/100], Step [ 20/ 78], Loss: 0.6065\n",
            "Epoch [30/100], Step [ 21/ 78], Loss: 0.5935\n",
            "Epoch [30/100], Step [ 22/ 78], Loss: 0.5941\n",
            "Epoch [30/100], Step [ 23/ 78], Loss: 0.6077\n",
            "Epoch [30/100], Step [ 24/ 78], Loss: 0.5891\n",
            "Epoch [30/100], Step [ 25/ 78], Loss: 0.6029\n",
            "Epoch [30/100], Step [ 26/ 78], Loss: 0.5937\n",
            "Epoch [30/100], Step [ 27/ 78], Loss: 0.5920\n",
            "Epoch [30/100], Step [ 28/ 78], Loss: 0.6324\n",
            "Epoch [30/100], Step [ 29/ 78], Loss: 0.6316\n",
            "Epoch [30/100], Step [ 30/ 78], Loss: 0.6092\n",
            "Epoch [30/100], Step [ 31/ 78], Loss: 0.6139\n",
            "Epoch [30/100], Step [ 32/ 78], Loss: 0.5738\n",
            "Epoch [30/100], Step [ 33/ 78], Loss: 0.5877\n",
            "Epoch [30/100], Step [ 34/ 78], Loss: 0.5972\n",
            "Epoch [30/100], Step [ 35/ 78], Loss: 0.5829\n",
            "Epoch [30/100], Step [ 36/ 78], Loss: 0.5747\n",
            "Epoch [30/100], Step [ 37/ 78], Loss: 0.6289\n",
            "Epoch [30/100], Step [ 38/ 78], Loss: 0.5779\n",
            "Epoch [30/100], Step [ 39/ 78], Loss: 0.6154\n",
            "Epoch [30/100], Step [ 40/ 78], Loss: 0.5583\n",
            "Epoch [30/100], Step [ 41/ 78], Loss: 0.6084\n",
            "Epoch [30/100], Step [ 42/ 78], Loss: 0.6242\n",
            "Epoch [30/100], Step [ 43/ 78], Loss: 0.6147\n",
            "Epoch [30/100], Step [ 44/ 78], Loss: 0.5979\n",
            "Epoch [30/100], Step [ 45/ 78], Loss: 0.5918\n",
            "Epoch [30/100], Step [ 46/ 78], Loss: 0.5778\n",
            "Epoch [30/100], Step [ 47/ 78], Loss: 0.5951\n",
            "Epoch [30/100], Step [ 48/ 78], Loss: 0.6146\n",
            "Epoch [30/100], Step [ 49/ 78], Loss: 0.6190\n",
            "Epoch [30/100], Step [ 50/ 78], Loss: 0.6200\n",
            "Epoch [30/100], Step [ 51/ 78], Loss: 0.6026\n",
            "Epoch [30/100], Step [ 52/ 78], Loss: 0.6240\n",
            "Epoch [30/100], Step [ 53/ 78], Loss: 0.5875\n",
            "Epoch [30/100], Step [ 54/ 78], Loss: 0.6269\n",
            "Epoch [30/100], Step [ 55/ 78], Loss: 0.6485\n",
            "Epoch [30/100], Step [ 56/ 78], Loss: 0.5927\n",
            "Epoch [30/100], Step [ 57/ 78], Loss: 0.6059\n",
            "Epoch [30/100], Step [ 58/ 78], Loss: 0.6057\n",
            "Epoch [30/100], Step [ 59/ 78], Loss: 0.5979\n",
            "Epoch [30/100], Step [ 60/ 78], Loss: 0.5846\n",
            "Epoch [30/100], Step [ 61/ 78], Loss: 0.6075\n",
            "Epoch [30/100], Step [ 62/ 78], Loss: 0.6159\n",
            "Epoch [30/100], Step [ 63/ 78], Loss: 0.6451\n",
            "Epoch [31/100], Step [  1/ 78], Loss: 0.6259\n",
            "Epoch [31/100], Step [  2/ 78], Loss: 0.6294\n",
            "Epoch [31/100], Step [  3/ 78], Loss: 0.5842\n",
            "Epoch [31/100], Step [  4/ 78], Loss: 0.5620\n",
            "Epoch [31/100], Step [  5/ 78], Loss: 0.5959\n",
            "Epoch [31/100], Step [  6/ 78], Loss: 0.6419\n",
            "Epoch [31/100], Step [  7/ 78], Loss: 0.6191\n",
            "Epoch [31/100], Step [  8/ 78], Loss: 0.5767\n",
            "Epoch [31/100], Step [  9/ 78], Loss: 0.6066\n",
            "Epoch [31/100], Step [ 10/ 78], Loss: 0.6533\n",
            "Epoch [31/100], Step [ 11/ 78], Loss: 0.5770\n",
            "Epoch [31/100], Step [ 12/ 78], Loss: 0.5814\n",
            "Epoch [31/100], Step [ 13/ 78], Loss: 0.6078\n",
            "Epoch [31/100], Step [ 14/ 78], Loss: 0.6305\n",
            "Epoch [31/100], Step [ 15/ 78], Loss: 0.6164\n",
            "Epoch [31/100], Step [ 16/ 78], Loss: 0.5944\n",
            "Epoch [31/100], Step [ 17/ 78], Loss: 0.6006\n",
            "Epoch [31/100], Step [ 18/ 78], Loss: 0.5826\n",
            "Epoch [31/100], Step [ 19/ 78], Loss: 0.6038\n",
            "Epoch [31/100], Step [ 20/ 78], Loss: 0.6192\n",
            "Epoch [31/100], Step [ 21/ 78], Loss: 0.6239\n",
            "Epoch [31/100], Step [ 22/ 78], Loss: 0.6225\n",
            "Epoch [31/100], Step [ 23/ 78], Loss: 0.6431\n",
            "Epoch [31/100], Step [ 24/ 78], Loss: 0.6203\n",
            "Epoch [31/100], Step [ 25/ 78], Loss: 0.6111\n",
            "Epoch [31/100], Step [ 26/ 78], Loss: 0.6047\n",
            "Epoch [31/100], Step [ 27/ 78], Loss: 0.5897\n",
            "Epoch [31/100], Step [ 28/ 78], Loss: 0.5832\n",
            "Epoch [31/100], Step [ 29/ 78], Loss: 0.6020\n",
            "Epoch [31/100], Step [ 30/ 78], Loss: 0.5810\n",
            "Epoch [31/100], Step [ 31/ 78], Loss: 0.5906\n",
            "Epoch [31/100], Step [ 32/ 78], Loss: 0.5466\n",
            "Epoch [31/100], Step [ 33/ 78], Loss: 0.6073\n",
            "Epoch [31/100], Step [ 34/ 78], Loss: 0.6038\n",
            "Epoch [31/100], Step [ 35/ 78], Loss: 0.5958\n",
            "Epoch [31/100], Step [ 36/ 78], Loss: 0.6218\n",
            "Epoch [31/100], Step [ 37/ 78], Loss: 0.6411\n",
            "Epoch [31/100], Step [ 38/ 78], Loss: 0.6618\n",
            "Epoch [31/100], Step [ 39/ 78], Loss: 0.5624\n",
            "Epoch [31/100], Step [ 40/ 78], Loss: 0.6436\n",
            "Epoch [31/100], Step [ 41/ 78], Loss: 0.5947\n",
            "Epoch [31/100], Step [ 42/ 78], Loss: 0.5897\n",
            "Epoch [31/100], Step [ 43/ 78], Loss: 0.6426\n",
            "Epoch [31/100], Step [ 44/ 78], Loss: 0.5837\n",
            "Epoch [31/100], Step [ 45/ 78], Loss: 0.6004\n",
            "Epoch [31/100], Step [ 46/ 78], Loss: 0.6184\n",
            "Epoch [31/100], Step [ 47/ 78], Loss: 0.6087\n",
            "Epoch [31/100], Step [ 48/ 78], Loss: 0.5848\n",
            "Epoch [31/100], Step [ 49/ 78], Loss: 0.5910\n",
            "Epoch [31/100], Step [ 50/ 78], Loss: 0.6084\n",
            "Epoch [31/100], Step [ 51/ 78], Loss: 0.5861\n",
            "Epoch [31/100], Step [ 52/ 78], Loss: 0.6340\n",
            "Epoch [31/100], Step [ 53/ 78], Loss: 0.6173\n",
            "Epoch [31/100], Step [ 54/ 78], Loss: 0.6124\n",
            "Epoch [31/100], Step [ 55/ 78], Loss: 0.6255\n",
            "Epoch [31/100], Step [ 56/ 78], Loss: 0.5908\n",
            "Epoch [31/100], Step [ 57/ 78], Loss: 0.5962\n",
            "Epoch [31/100], Step [ 58/ 78], Loss: 0.5887\n",
            "Epoch [31/100], Step [ 59/ 78], Loss: 0.5920\n",
            "Epoch [31/100], Step [ 60/ 78], Loss: 0.5779\n",
            "Epoch [31/100], Step [ 61/ 78], Loss: 0.5876\n",
            "Epoch [31/100], Step [ 62/ 78], Loss: 0.6106\n",
            "Epoch [31/100], Step [ 63/ 78], Loss: 0.6063\n",
            "Epoch [32/100], Step [  1/ 78], Loss: 0.6486\n",
            "Epoch [32/100], Step [  2/ 78], Loss: 0.6077\n",
            "Epoch [32/100], Step [  3/ 78], Loss: 0.5911\n",
            "Epoch [32/100], Step [  4/ 78], Loss: 0.6293\n",
            "Epoch [32/100], Step [  5/ 78], Loss: 0.5747\n",
            "Epoch [32/100], Step [  6/ 78], Loss: 0.5745\n",
            "Epoch [32/100], Step [  7/ 78], Loss: 0.6007\n",
            "Epoch [32/100], Step [  8/ 78], Loss: 0.5939\n",
            "Epoch [32/100], Step [  9/ 78], Loss: 0.5723\n",
            "Epoch [32/100], Step [ 10/ 78], Loss: 0.6364\n",
            "Epoch [32/100], Step [ 11/ 78], Loss: 0.5953\n",
            "Epoch [32/100], Step [ 12/ 78], Loss: 0.5971\n",
            "Epoch [32/100], Step [ 13/ 78], Loss: 0.6346\n",
            "Epoch [32/100], Step [ 14/ 78], Loss: 0.5959\n",
            "Epoch [32/100], Step [ 15/ 78], Loss: 0.6142\n",
            "Epoch [32/100], Step [ 16/ 78], Loss: 0.5919\n",
            "Epoch [32/100], Step [ 17/ 78], Loss: 0.6231\n",
            "Epoch [32/100], Step [ 18/ 78], Loss: 0.6052\n",
            "Epoch [32/100], Step [ 19/ 78], Loss: 0.5698\n",
            "Epoch [32/100], Step [ 20/ 78], Loss: 0.6030\n",
            "Epoch [32/100], Step [ 21/ 78], Loss: 0.5892\n",
            "Epoch [32/100], Step [ 22/ 78], Loss: 0.5898\n",
            "Epoch [32/100], Step [ 23/ 78], Loss: 0.6017\n",
            "Epoch [32/100], Step [ 24/ 78], Loss: 0.5709\n",
            "Epoch [32/100], Step [ 25/ 78], Loss: 0.5758\n",
            "Epoch [32/100], Step [ 26/ 78], Loss: 0.5915\n",
            "Epoch [32/100], Step [ 27/ 78], Loss: 0.6168\n",
            "Epoch [32/100], Step [ 28/ 78], Loss: 0.5708\n",
            "Epoch [32/100], Step [ 29/ 78], Loss: 0.5801\n",
            "Epoch [32/100], Step [ 30/ 78], Loss: 0.5741\n",
            "Epoch [32/100], Step [ 31/ 78], Loss: 0.6117\n",
            "Epoch [32/100], Step [ 32/ 78], Loss: 0.6134\n",
            "Epoch [32/100], Step [ 33/ 78], Loss: 0.6047\n",
            "Epoch [32/100], Step [ 34/ 78], Loss: 0.5859\n",
            "Epoch [32/100], Step [ 35/ 78], Loss: 0.6099\n",
            "Epoch [32/100], Step [ 36/ 78], Loss: 0.5932\n",
            "Epoch [32/100], Step [ 37/ 78], Loss: 0.6390\n",
            "Epoch [32/100], Step [ 38/ 78], Loss: 0.6104\n",
            "Epoch [32/100], Step [ 39/ 78], Loss: 0.6049\n",
            "Epoch [32/100], Step [ 40/ 78], Loss: 0.6001\n",
            "Epoch [32/100], Step [ 41/ 78], Loss: 0.6344\n",
            "Epoch [32/100], Step [ 42/ 78], Loss: 0.6018\n",
            "Epoch [32/100], Step [ 43/ 78], Loss: 0.6211\n",
            "Epoch [32/100], Step [ 44/ 78], Loss: 0.5875\n",
            "Epoch [32/100], Step [ 45/ 78], Loss: 0.5553\n",
            "Epoch [32/100], Step [ 46/ 78], Loss: 0.5973\n",
            "Epoch [32/100], Step [ 47/ 78], Loss: 0.6089\n",
            "Epoch [32/100], Step [ 48/ 78], Loss: 0.6350\n",
            "Epoch [32/100], Step [ 49/ 78], Loss: 0.6067\n",
            "Epoch [32/100], Step [ 50/ 78], Loss: 0.6493\n",
            "Epoch [32/100], Step [ 51/ 78], Loss: 0.6043\n",
            "Epoch [32/100], Step [ 52/ 78], Loss: 0.6059\n",
            "Epoch [32/100], Step [ 53/ 78], Loss: 0.6388\n",
            "Epoch [32/100], Step [ 54/ 78], Loss: 0.6095\n",
            "Epoch [32/100], Step [ 55/ 78], Loss: 0.6158\n",
            "Epoch [32/100], Step [ 56/ 78], Loss: 0.6003\n",
            "Epoch [32/100], Step [ 57/ 78], Loss: 0.5659\n",
            "Epoch [32/100], Step [ 58/ 78], Loss: 0.6067\n",
            "Epoch [32/100], Step [ 59/ 78], Loss: 0.6068\n",
            "Epoch [32/100], Step [ 60/ 78], Loss: 0.6009\n",
            "Epoch [32/100], Step [ 61/ 78], Loss: 0.6035\n",
            "Epoch [32/100], Step [ 62/ 78], Loss: 0.5964\n",
            "Epoch [32/100], Step [ 63/ 78], Loss: 0.5973\n",
            "Epoch [33/100], Step [  1/ 78], Loss: 0.6176\n",
            "Epoch [33/100], Step [  2/ 78], Loss: 0.6053\n",
            "Epoch [33/100], Step [  3/ 78], Loss: 0.6022\n",
            "Epoch [33/100], Step [  4/ 78], Loss: 0.6298\n",
            "Epoch [33/100], Step [  5/ 78], Loss: 0.6284\n",
            "Epoch [33/100], Step [  6/ 78], Loss: 0.6110\n",
            "Epoch [33/100], Step [  7/ 78], Loss: 0.5700\n",
            "Epoch [33/100], Step [  8/ 78], Loss: 0.5802\n",
            "Epoch [33/100], Step [  9/ 78], Loss: 0.5702\n",
            "Epoch [33/100], Step [ 10/ 78], Loss: 0.6208\n",
            "Epoch [33/100], Step [ 11/ 78], Loss: 0.5964\n",
            "Epoch [33/100], Step [ 12/ 78], Loss: 0.6369\n",
            "Epoch [33/100], Step [ 13/ 78], Loss: 0.5730\n",
            "Epoch [33/100], Step [ 14/ 78], Loss: 0.5895\n",
            "Epoch [33/100], Step [ 15/ 78], Loss: 0.5859\n",
            "Epoch [33/100], Step [ 16/ 78], Loss: 0.5950\n",
            "Epoch [33/100], Step [ 17/ 78], Loss: 0.5799\n",
            "Epoch [33/100], Step [ 18/ 78], Loss: 0.6329\n",
            "Epoch [33/100], Step [ 19/ 78], Loss: 0.5970\n",
            "Epoch [33/100], Step [ 20/ 78], Loss: 0.5964\n",
            "Epoch [33/100], Step [ 21/ 78], Loss: 0.5704\n",
            "Epoch [33/100], Step [ 22/ 78], Loss: 0.5739\n",
            "Epoch [33/100], Step [ 23/ 78], Loss: 0.5731\n",
            "Epoch [33/100], Step [ 24/ 78], Loss: 0.5840\n",
            "Epoch [33/100], Step [ 25/ 78], Loss: 0.5977\n",
            "Epoch [33/100], Step [ 26/ 78], Loss: 0.6215\n",
            "Epoch [33/100], Step [ 27/ 78], Loss: 0.5874\n",
            "Epoch [33/100], Step [ 28/ 78], Loss: 0.6078\n",
            "Epoch [33/100], Step [ 29/ 78], Loss: 0.6136\n",
            "Epoch [33/100], Step [ 30/ 78], Loss: 0.5881\n",
            "Epoch [33/100], Step [ 31/ 78], Loss: 0.5874\n",
            "Epoch [33/100], Step [ 32/ 78], Loss: 0.6014\n",
            "Epoch [33/100], Step [ 33/ 78], Loss: 0.6052\n",
            "Epoch [33/100], Step [ 34/ 78], Loss: 0.6082\n",
            "Epoch [33/100], Step [ 35/ 78], Loss: 0.5849\n",
            "Epoch [33/100], Step [ 36/ 78], Loss: 0.5743\n",
            "Epoch [33/100], Step [ 37/ 78], Loss: 0.5946\n",
            "Epoch [33/100], Step [ 38/ 78], Loss: 0.5655\n",
            "Epoch [33/100], Step [ 39/ 78], Loss: 0.5885\n",
            "Epoch [33/100], Step [ 40/ 78], Loss: 0.5942\n",
            "Epoch [33/100], Step [ 41/ 78], Loss: 0.5570\n",
            "Epoch [33/100], Step [ 42/ 78], Loss: 0.5903\n",
            "Epoch [33/100], Step [ 43/ 78], Loss: 0.5613\n",
            "Epoch [33/100], Step [ 44/ 78], Loss: 0.5677\n",
            "Epoch [33/100], Step [ 45/ 78], Loss: 0.6082\n",
            "Epoch [33/100], Step [ 46/ 78], Loss: 0.5853\n",
            "Epoch [33/100], Step [ 47/ 78], Loss: 0.5750\n",
            "Epoch [33/100], Step [ 48/ 78], Loss: 0.6011\n",
            "Epoch [33/100], Step [ 49/ 78], Loss: 0.5722\n",
            "Epoch [33/100], Step [ 50/ 78], Loss: 0.5852\n",
            "Epoch [33/100], Step [ 51/ 78], Loss: 0.5819\n",
            "Epoch [33/100], Step [ 52/ 78], Loss: 0.6230\n",
            "Epoch [33/100], Step [ 53/ 78], Loss: 0.6398\n",
            "Epoch [33/100], Step [ 54/ 78], Loss: 0.5831\n",
            "Epoch [33/100], Step [ 55/ 78], Loss: 0.5921\n",
            "Epoch [33/100], Step [ 56/ 78], Loss: 0.6133\n",
            "Epoch [33/100], Step [ 57/ 78], Loss: 0.6032\n",
            "Epoch [33/100], Step [ 58/ 78], Loss: 0.5658\n",
            "Epoch [33/100], Step [ 59/ 78], Loss: 0.5828\n",
            "Epoch [33/100], Step [ 60/ 78], Loss: 0.5957\n",
            "Epoch [33/100], Step [ 61/ 78], Loss: 0.5994\n",
            "Epoch [33/100], Step [ 62/ 78], Loss: 0.5806\n",
            "Epoch [33/100], Step [ 63/ 78], Loss: 0.5423\n",
            "Epoch [34/100], Step [  1/ 78], Loss: 0.6063\n",
            "Epoch [34/100], Step [  2/ 78], Loss: 0.5810\n",
            "Epoch [34/100], Step [  3/ 78], Loss: 0.5798\n",
            "Epoch [34/100], Step [  4/ 78], Loss: 0.5754\n",
            "Epoch [34/100], Step [  5/ 78], Loss: 0.6096\n",
            "Epoch [34/100], Step [  6/ 78], Loss: 0.5710\n",
            "Epoch [34/100], Step [  7/ 78], Loss: 0.5587\n",
            "Epoch [34/100], Step [  8/ 78], Loss: 0.6313\n",
            "Epoch [34/100], Step [  9/ 78], Loss: 0.5841\n",
            "Epoch [34/100], Step [ 10/ 78], Loss: 0.5726\n",
            "Epoch [34/100], Step [ 11/ 78], Loss: 0.6057\n",
            "Epoch [34/100], Step [ 12/ 78], Loss: 0.6126\n",
            "Epoch [34/100], Step [ 13/ 78], Loss: 0.6445\n",
            "Epoch [34/100], Step [ 14/ 78], Loss: 0.6787\n",
            "Epoch [34/100], Step [ 15/ 78], Loss: 0.5493\n",
            "Epoch [34/100], Step [ 16/ 78], Loss: 0.6063\n",
            "Epoch [34/100], Step [ 17/ 78], Loss: 0.5967\n",
            "Epoch [34/100], Step [ 18/ 78], Loss: 0.5966\n",
            "Epoch [34/100], Step [ 19/ 78], Loss: 0.5986\n",
            "Epoch [34/100], Step [ 20/ 78], Loss: 0.6024\n",
            "Epoch [34/100], Step [ 21/ 78], Loss: 0.5875\n",
            "Epoch [34/100], Step [ 22/ 78], Loss: 0.5967\n",
            "Epoch [34/100], Step [ 23/ 78], Loss: 0.5678\n",
            "Epoch [34/100], Step [ 24/ 78], Loss: 0.5733\n",
            "Epoch [34/100], Step [ 25/ 78], Loss: 0.6063\n",
            "Epoch [34/100], Step [ 26/ 78], Loss: 0.5806\n",
            "Epoch [34/100], Step [ 27/ 78], Loss: 0.6243\n",
            "Epoch [34/100], Step [ 28/ 78], Loss: 0.6173\n",
            "Epoch [34/100], Step [ 29/ 78], Loss: 0.5703\n",
            "Epoch [34/100], Step [ 30/ 78], Loss: 0.5750\n",
            "Epoch [34/100], Step [ 31/ 78], Loss: 0.5869\n",
            "Epoch [34/100], Step [ 32/ 78], Loss: 0.5745\n",
            "Epoch [34/100], Step [ 33/ 78], Loss: 0.5726\n",
            "Epoch [34/100], Step [ 34/ 78], Loss: 0.5755\n",
            "Epoch [34/100], Step [ 35/ 78], Loss: 0.5986\n",
            "Epoch [34/100], Step [ 36/ 78], Loss: 0.5697\n",
            "Epoch [34/100], Step [ 37/ 78], Loss: 0.5954\n",
            "Epoch [34/100], Step [ 38/ 78], Loss: 0.6187\n",
            "Epoch [34/100], Step [ 39/ 78], Loss: 0.5859\n",
            "Epoch [34/100], Step [ 40/ 78], Loss: 0.5887\n",
            "Epoch [34/100], Step [ 41/ 78], Loss: 0.6290\n",
            "Epoch [34/100], Step [ 42/ 78], Loss: 0.5864\n",
            "Epoch [34/100], Step [ 43/ 78], Loss: 0.5733\n",
            "Epoch [34/100], Step [ 44/ 78], Loss: 0.5966\n",
            "Epoch [34/100], Step [ 45/ 78], Loss: 0.5928\n",
            "Epoch [34/100], Step [ 46/ 78], Loss: 0.5885\n",
            "Epoch [34/100], Step [ 47/ 78], Loss: 0.6067\n",
            "Epoch [34/100], Step [ 48/ 78], Loss: 0.5962\n",
            "Epoch [34/100], Step [ 49/ 78], Loss: 0.5860\n",
            "Epoch [34/100], Step [ 50/ 78], Loss: 0.5614\n",
            "Epoch [34/100], Step [ 51/ 78], Loss: 0.5685\n",
            "Epoch [34/100], Step [ 52/ 78], Loss: 0.6106\n",
            "Epoch [34/100], Step [ 53/ 78], Loss: 0.5883\n",
            "Epoch [34/100], Step [ 54/ 78], Loss: 0.6066\n",
            "Epoch [34/100], Step [ 55/ 78], Loss: 0.5976\n",
            "Epoch [34/100], Step [ 56/ 78], Loss: 0.5719\n",
            "Epoch [34/100], Step [ 57/ 78], Loss: 0.6063\n",
            "Epoch [34/100], Step [ 58/ 78], Loss: 0.5847\n",
            "Epoch [34/100], Step [ 59/ 78], Loss: 0.6515\n",
            "Epoch [34/100], Step [ 60/ 78], Loss: 0.6343\n",
            "Epoch [34/100], Step [ 61/ 78], Loss: 0.6166\n",
            "Epoch [34/100], Step [ 62/ 78], Loss: 0.6106\n",
            "Epoch [34/100], Step [ 63/ 78], Loss: 0.5498\n",
            "Epoch [35/100], Step [  1/ 78], Loss: 0.5731\n",
            "Epoch [35/100], Step [  2/ 78], Loss: 0.5880\n",
            "Epoch [35/100], Step [  3/ 78], Loss: 0.5822\n",
            "Epoch [35/100], Step [  4/ 78], Loss: 0.5744\n",
            "Epoch [35/100], Step [  5/ 78], Loss: 0.5818\n",
            "Epoch [35/100], Step [  6/ 78], Loss: 0.5917\n",
            "Epoch [35/100], Step [  7/ 78], Loss: 0.6006\n",
            "Epoch [35/100], Step [  8/ 78], Loss: 0.5634\n",
            "Epoch [35/100], Step [  9/ 78], Loss: 0.5814\n",
            "Epoch [35/100], Step [ 10/ 78], Loss: 0.5885\n",
            "Epoch [35/100], Step [ 11/ 78], Loss: 0.6183\n",
            "Epoch [35/100], Step [ 12/ 78], Loss: 0.6207\n",
            "Epoch [35/100], Step [ 13/ 78], Loss: 0.5978\n",
            "Epoch [35/100], Step [ 14/ 78], Loss: 0.5830\n",
            "Epoch [35/100], Step [ 15/ 78], Loss: 0.5856\n",
            "Epoch [35/100], Step [ 16/ 78], Loss: 0.5790\n",
            "Epoch [35/100], Step [ 17/ 78], Loss: 0.6133\n",
            "Epoch [35/100], Step [ 18/ 78], Loss: 0.6335\n",
            "Epoch [35/100], Step [ 19/ 78], Loss: 0.5894\n",
            "Epoch [35/100], Step [ 20/ 78], Loss: 0.5490\n",
            "Epoch [35/100], Step [ 21/ 78], Loss: 0.5951\n",
            "Epoch [35/100], Step [ 22/ 78], Loss: 0.5926\n",
            "Epoch [35/100], Step [ 23/ 78], Loss: 0.6259\n",
            "Epoch [35/100], Step [ 24/ 78], Loss: 0.6086\n",
            "Epoch [35/100], Step [ 25/ 78], Loss: 0.5639\n",
            "Epoch [35/100], Step [ 26/ 78], Loss: 0.6025\n",
            "Epoch [35/100], Step [ 27/ 78], Loss: 0.5490\n",
            "Epoch [35/100], Step [ 28/ 78], Loss: 0.6066\n",
            "Epoch [35/100], Step [ 29/ 78], Loss: 0.5731\n",
            "Epoch [35/100], Step [ 30/ 78], Loss: 0.6016\n",
            "Epoch [35/100], Step [ 31/ 78], Loss: 0.5593\n",
            "Epoch [35/100], Step [ 32/ 78], Loss: 0.5901\n",
            "Epoch [35/100], Step [ 33/ 78], Loss: 0.6223\n",
            "Epoch [35/100], Step [ 34/ 78], Loss: 0.5707\n",
            "Epoch [35/100], Step [ 35/ 78], Loss: 0.5776\n",
            "Epoch [35/100], Step [ 36/ 78], Loss: 0.5524\n",
            "Epoch [35/100], Step [ 37/ 78], Loss: 0.6065\n",
            "Epoch [35/100], Step [ 38/ 78], Loss: 0.5993\n",
            "Epoch [35/100], Step [ 39/ 78], Loss: 0.5969\n",
            "Epoch [35/100], Step [ 40/ 78], Loss: 0.6257\n",
            "Epoch [35/100], Step [ 41/ 78], Loss: 0.6299\n",
            "Epoch [35/100], Step [ 42/ 78], Loss: 0.5813\n",
            "Epoch [35/100], Step [ 43/ 78], Loss: 0.6149\n",
            "Epoch [35/100], Step [ 44/ 78], Loss: 0.5671\n",
            "Epoch [35/100], Step [ 45/ 78], Loss: 0.5837\n",
            "Epoch [35/100], Step [ 46/ 78], Loss: 0.6039\n",
            "Epoch [35/100], Step [ 47/ 78], Loss: 0.6318\n",
            "Epoch [35/100], Step [ 48/ 78], Loss: 0.6109\n",
            "Epoch [35/100], Step [ 49/ 78], Loss: 0.5844\n",
            "Epoch [35/100], Step [ 50/ 78], Loss: 0.6112\n",
            "Epoch [35/100], Step [ 51/ 78], Loss: 0.5873\n",
            "Epoch [35/100], Step [ 52/ 78], Loss: 0.5828\n",
            "Epoch [35/100], Step [ 53/ 78], Loss: 0.5951\n",
            "Epoch [35/100], Step [ 54/ 78], Loss: 0.5342\n",
            "Epoch [35/100], Step [ 55/ 78], Loss: 0.6084\n",
            "Epoch [35/100], Step [ 56/ 78], Loss: 0.5923\n",
            "Epoch [35/100], Step [ 57/ 78], Loss: 0.6517\n",
            "Epoch [35/100], Step [ 58/ 78], Loss: 0.6036\n",
            "Epoch [35/100], Step [ 59/ 78], Loss: 0.5759\n",
            "Epoch [35/100], Step [ 60/ 78], Loss: 0.5939\n",
            "Epoch [35/100], Step [ 61/ 78], Loss: 0.5991\n",
            "Epoch [35/100], Step [ 62/ 78], Loss: 0.5858\n",
            "Epoch [35/100], Step [ 63/ 78], Loss: 0.5770\n",
            "Epoch [36/100], Step [  1/ 78], Loss: 0.5815\n",
            "Epoch [36/100], Step [  2/ 78], Loss: 0.5836\n",
            "Epoch [36/100], Step [  3/ 78], Loss: 0.5886\n",
            "Epoch [36/100], Step [  4/ 78], Loss: 0.5719\n",
            "Epoch [36/100], Step [  5/ 78], Loss: 0.5938\n",
            "Epoch [36/100], Step [  6/ 78], Loss: 0.6029\n",
            "Epoch [36/100], Step [  7/ 78], Loss: 0.5868\n",
            "Epoch [36/100], Step [  8/ 78], Loss: 0.5720\n",
            "Epoch [36/100], Step [  9/ 78], Loss: 0.5779\n",
            "Epoch [36/100], Step [ 10/ 78], Loss: 0.5857\n",
            "Epoch [36/100], Step [ 11/ 78], Loss: 0.6301\n",
            "Epoch [36/100], Step [ 12/ 78], Loss: 0.6055\n",
            "Epoch [36/100], Step [ 13/ 78], Loss: 0.5803\n",
            "Epoch [36/100], Step [ 14/ 78], Loss: 0.5726\n",
            "Epoch [36/100], Step [ 15/ 78], Loss: 0.5604\n",
            "Epoch [36/100], Step [ 16/ 78], Loss: 0.5705\n",
            "Epoch [36/100], Step [ 17/ 78], Loss: 0.6098\n",
            "Epoch [36/100], Step [ 18/ 78], Loss: 0.6117\n",
            "Epoch [36/100], Step [ 19/ 78], Loss: 0.6008\n",
            "Epoch [36/100], Step [ 20/ 78], Loss: 0.6147\n",
            "Epoch [36/100], Step [ 21/ 78], Loss: 0.5770\n",
            "Epoch [36/100], Step [ 22/ 78], Loss: 0.5807\n",
            "Epoch [36/100], Step [ 23/ 78], Loss: 0.5807\n",
            "Epoch [36/100], Step [ 24/ 78], Loss: 0.5958\n",
            "Epoch [36/100], Step [ 25/ 78], Loss: 0.5870\n",
            "Epoch [36/100], Step [ 26/ 78], Loss: 0.6093\n",
            "Epoch [36/100], Step [ 27/ 78], Loss: 0.5935\n",
            "Epoch [36/100], Step [ 28/ 78], Loss: 0.5865\n",
            "Epoch [36/100], Step [ 29/ 78], Loss: 0.5872\n",
            "Epoch [36/100], Step [ 30/ 78], Loss: 0.5832\n",
            "Epoch [36/100], Step [ 31/ 78], Loss: 0.5746\n",
            "Epoch [36/100], Step [ 32/ 78], Loss: 0.5731\n",
            "Epoch [36/100], Step [ 33/ 78], Loss: 0.6148\n",
            "Epoch [36/100], Step [ 34/ 78], Loss: 0.5760\n",
            "Epoch [36/100], Step [ 35/ 78], Loss: 0.5818\n",
            "Epoch [36/100], Step [ 36/ 78], Loss: 0.5688\n",
            "Epoch [36/100], Step [ 37/ 78], Loss: 0.5784\n",
            "Epoch [36/100], Step [ 38/ 78], Loss: 0.5876\n",
            "Epoch [36/100], Step [ 39/ 78], Loss: 0.5865\n",
            "Epoch [36/100], Step [ 40/ 78], Loss: 0.5678\n",
            "Epoch [36/100], Step [ 41/ 78], Loss: 0.5723\n",
            "Epoch [36/100], Step [ 42/ 78], Loss: 0.5763\n",
            "Epoch [36/100], Step [ 43/ 78], Loss: 0.5698\n",
            "Epoch [36/100], Step [ 44/ 78], Loss: 0.5905\n",
            "Epoch [36/100], Step [ 45/ 78], Loss: 0.6149\n",
            "Epoch [36/100], Step [ 46/ 78], Loss: 0.5787\n",
            "Epoch [36/100], Step [ 47/ 78], Loss: 0.6094\n",
            "Epoch [36/100], Step [ 48/ 78], Loss: 0.5752\n",
            "Epoch [36/100], Step [ 49/ 78], Loss: 0.6087\n",
            "Epoch [36/100], Step [ 50/ 78], Loss: 0.6018\n",
            "Epoch [36/100], Step [ 51/ 78], Loss: 0.6274\n",
            "Epoch [36/100], Step [ 52/ 78], Loss: 0.5938\n",
            "Epoch [36/100], Step [ 53/ 78], Loss: 0.5677\n",
            "Epoch [36/100], Step [ 54/ 78], Loss: 0.5869\n",
            "Epoch [36/100], Step [ 55/ 78], Loss: 0.5939\n",
            "Epoch [36/100], Step [ 56/ 78], Loss: 0.5979\n",
            "Epoch [36/100], Step [ 57/ 78], Loss: 0.5856\n",
            "Epoch [36/100], Step [ 58/ 78], Loss: 0.5774\n",
            "Epoch [36/100], Step [ 59/ 78], Loss: 0.5938\n",
            "Epoch [36/100], Step [ 60/ 78], Loss: 0.6182\n",
            "Epoch [36/100], Step [ 61/ 78], Loss: 0.5808\n",
            "Epoch [36/100], Step [ 62/ 78], Loss: 0.5547\n",
            "Epoch [36/100], Step [ 63/ 78], Loss: 0.5604\n",
            "Epoch [37/100], Step [  1/ 78], Loss: 0.5780\n",
            "Epoch [37/100], Step [  2/ 78], Loss: 0.5959\n",
            "Epoch [37/100], Step [  3/ 78], Loss: 0.5839\n",
            "Epoch [37/100], Step [  4/ 78], Loss: 0.6486\n",
            "Epoch [37/100], Step [  5/ 78], Loss: 0.6009\n",
            "Epoch [37/100], Step [  6/ 78], Loss: 0.5952\n",
            "Epoch [37/100], Step [  7/ 78], Loss: 0.5567\n",
            "Epoch [37/100], Step [  8/ 78], Loss: 0.6116\n",
            "Epoch [37/100], Step [  9/ 78], Loss: 0.6003\n",
            "Epoch [37/100], Step [ 10/ 78], Loss: 0.5716\n",
            "Epoch [37/100], Step [ 11/ 78], Loss: 0.5700\n",
            "Epoch [37/100], Step [ 12/ 78], Loss: 0.5803\n",
            "Epoch [37/100], Step [ 13/ 78], Loss: 0.5411\n",
            "Epoch [37/100], Step [ 14/ 78], Loss: 0.5919\n",
            "Epoch [37/100], Step [ 15/ 78], Loss: 0.5864\n",
            "Epoch [37/100], Step [ 16/ 78], Loss: 0.6163\n",
            "Epoch [37/100], Step [ 17/ 78], Loss: 0.6162\n",
            "Epoch [37/100], Step [ 18/ 78], Loss: 0.5988\n",
            "Epoch [37/100], Step [ 19/ 78], Loss: 0.5692\n",
            "Epoch [37/100], Step [ 20/ 78], Loss: 0.5944\n",
            "Epoch [37/100], Step [ 21/ 78], Loss: 0.5953\n",
            "Epoch [37/100], Step [ 22/ 78], Loss: 0.5877\n",
            "Epoch [37/100], Step [ 23/ 78], Loss: 0.6160\n",
            "Epoch [37/100], Step [ 24/ 78], Loss: 0.5520\n",
            "Epoch [37/100], Step [ 25/ 78], Loss: 0.5772\n",
            "Epoch [37/100], Step [ 26/ 78], Loss: 0.6655\n",
            "Epoch [37/100], Step [ 27/ 78], Loss: 0.5399\n",
            "Epoch [37/100], Step [ 28/ 78], Loss: 0.5714\n",
            "Epoch [37/100], Step [ 29/ 78], Loss: 0.5985\n",
            "Epoch [37/100], Step [ 30/ 78], Loss: 0.6233\n",
            "Epoch [37/100], Step [ 31/ 78], Loss: 0.5969\n",
            "Epoch [37/100], Step [ 32/ 78], Loss: 0.5994\n",
            "Epoch [37/100], Step [ 33/ 78], Loss: 0.5620\n",
            "Epoch [37/100], Step [ 34/ 78], Loss: 0.6162\n",
            "Epoch [37/100], Step [ 35/ 78], Loss: 0.6050\n",
            "Epoch [37/100], Step [ 36/ 78], Loss: 0.5573\n",
            "Epoch [37/100], Step [ 37/ 78], Loss: 0.5814\n",
            "Epoch [37/100], Step [ 38/ 78], Loss: 0.5734\n",
            "Epoch [37/100], Step [ 39/ 78], Loss: 0.5716\n",
            "Epoch [37/100], Step [ 40/ 78], Loss: 0.6024\n",
            "Epoch [37/100], Step [ 41/ 78], Loss: 0.6291\n",
            "Epoch [37/100], Step [ 42/ 78], Loss: 0.5722\n",
            "Epoch [37/100], Step [ 43/ 78], Loss: 0.5947\n",
            "Epoch [37/100], Step [ 44/ 78], Loss: 0.5819\n",
            "Epoch [37/100], Step [ 45/ 78], Loss: 0.5784\n",
            "Epoch [37/100], Step [ 46/ 78], Loss: 0.5765\n",
            "Epoch [37/100], Step [ 47/ 78], Loss: 0.5939\n",
            "Epoch [37/100], Step [ 48/ 78], Loss: 0.6059\n",
            "Epoch [37/100], Step [ 49/ 78], Loss: 0.5841\n",
            "Epoch [37/100], Step [ 50/ 78], Loss: 0.6132\n",
            "Epoch [37/100], Step [ 51/ 78], Loss: 0.6477\n",
            "Epoch [37/100], Step [ 52/ 78], Loss: 0.6218\n",
            "Epoch [37/100], Step [ 53/ 78], Loss: 0.5960\n",
            "Epoch [37/100], Step [ 54/ 78], Loss: 0.5923\n",
            "Epoch [37/100], Step [ 55/ 78], Loss: 0.6002\n",
            "Epoch [37/100], Step [ 56/ 78], Loss: 0.5787\n",
            "Epoch [37/100], Step [ 57/ 78], Loss: 0.5929\n",
            "Epoch [37/100], Step [ 58/ 78], Loss: 0.5943\n",
            "Epoch [37/100], Step [ 59/ 78], Loss: 0.5411\n",
            "Epoch [37/100], Step [ 60/ 78], Loss: 0.5512\n",
            "Epoch [37/100], Step [ 61/ 78], Loss: 0.5750\n",
            "Epoch [37/100], Step [ 62/ 78], Loss: 0.5744\n",
            "Epoch [37/100], Step [ 63/ 78], Loss: 0.6074\n",
            "Epoch [38/100], Step [  1/ 78], Loss: 0.5479\n",
            "Epoch [38/100], Step [  2/ 78], Loss: 0.5768\n",
            "Epoch [38/100], Step [  3/ 78], Loss: 0.6145\n",
            "Epoch [38/100], Step [  4/ 78], Loss: 0.6034\n",
            "Epoch [38/100], Step [  5/ 78], Loss: 0.6111\n",
            "Epoch [38/100], Step [  6/ 78], Loss: 0.5851\n",
            "Epoch [38/100], Step [  7/ 78], Loss: 0.5689\n",
            "Epoch [38/100], Step [  8/ 78], Loss: 0.5662\n",
            "Epoch [38/100], Step [  9/ 78], Loss: 0.6053\n",
            "Epoch [38/100], Step [ 10/ 78], Loss: 0.5730\n",
            "Epoch [38/100], Step [ 11/ 78], Loss: 0.5413\n",
            "Epoch [38/100], Step [ 12/ 78], Loss: 0.5670\n",
            "Epoch [38/100], Step [ 13/ 78], Loss: 0.5903\n",
            "Epoch [38/100], Step [ 14/ 78], Loss: 0.5699\n",
            "Epoch [38/100], Step [ 15/ 78], Loss: 0.6122\n",
            "Epoch [38/100], Step [ 16/ 78], Loss: 0.5969\n",
            "Epoch [38/100], Step [ 17/ 78], Loss: 0.6274\n",
            "Epoch [38/100], Step [ 18/ 78], Loss: 0.6090\n",
            "Epoch [38/100], Step [ 19/ 78], Loss: 0.5757\n",
            "Epoch [38/100], Step [ 20/ 78], Loss: 0.5789\n",
            "Epoch [38/100], Step [ 21/ 78], Loss: 0.5688\n",
            "Epoch [38/100], Step [ 22/ 78], Loss: 0.6135\n",
            "Epoch [38/100], Step [ 23/ 78], Loss: 0.6164\n",
            "Epoch [38/100], Step [ 24/ 78], Loss: 0.5648\n",
            "Epoch [38/100], Step [ 25/ 78], Loss: 0.5530\n",
            "Epoch [38/100], Step [ 26/ 78], Loss: 0.6003\n",
            "Epoch [38/100], Step [ 27/ 78], Loss: 0.5800\n",
            "Epoch [38/100], Step [ 28/ 78], Loss: 0.5618\n",
            "Epoch [38/100], Step [ 29/ 78], Loss: 0.5856\n",
            "Epoch [38/100], Step [ 30/ 78], Loss: 0.5928\n",
            "Epoch [38/100], Step [ 31/ 78], Loss: 0.5551\n",
            "Epoch [38/100], Step [ 32/ 78], Loss: 0.5713\n",
            "Epoch [38/100], Step [ 33/ 78], Loss: 0.5841\n",
            "Epoch [38/100], Step [ 34/ 78], Loss: 0.5822\n",
            "Epoch [38/100], Step [ 35/ 78], Loss: 0.6175\n",
            "Epoch [38/100], Step [ 36/ 78], Loss: 0.5868\n",
            "Epoch [38/100], Step [ 37/ 78], Loss: 0.5655\n",
            "Epoch [38/100], Step [ 38/ 78], Loss: 0.5922\n",
            "Epoch [38/100], Step [ 39/ 78], Loss: 0.5582\n",
            "Epoch [38/100], Step [ 40/ 78], Loss: 0.5884\n",
            "Epoch [38/100], Step [ 41/ 78], Loss: 0.6019\n",
            "Epoch [38/100], Step [ 42/ 78], Loss: 0.5836\n",
            "Epoch [38/100], Step [ 43/ 78], Loss: 0.6056\n",
            "Epoch [38/100], Step [ 44/ 78], Loss: 0.5987\n",
            "Epoch [38/100], Step [ 45/ 78], Loss: 0.5998\n",
            "Epoch [38/100], Step [ 46/ 78], Loss: 0.6072\n",
            "Epoch [38/100], Step [ 47/ 78], Loss: 0.5294\n",
            "Epoch [38/100], Step [ 48/ 78], Loss: 0.6004\n",
            "Epoch [38/100], Step [ 49/ 78], Loss: 0.5996\n",
            "Epoch [38/100], Step [ 50/ 78], Loss: 0.5798\n",
            "Epoch [38/100], Step [ 51/ 78], Loss: 0.6027\n",
            "Epoch [38/100], Step [ 52/ 78], Loss: 0.5713\n",
            "Epoch [38/100], Step [ 53/ 78], Loss: 0.6780\n",
            "Epoch [38/100], Step [ 54/ 78], Loss: 0.5767\n",
            "Epoch [38/100], Step [ 55/ 78], Loss: 0.5672\n",
            "Epoch [38/100], Step [ 56/ 78], Loss: 0.5464\n",
            "Epoch [38/100], Step [ 57/ 78], Loss: 0.5675\n",
            "Epoch [38/100], Step [ 58/ 78], Loss: 0.5951\n",
            "Epoch [38/100], Step [ 59/ 78], Loss: 0.6412\n",
            "Epoch [38/100], Step [ 60/ 78], Loss: 0.6198\n",
            "Epoch [38/100], Step [ 61/ 78], Loss: 0.5741\n",
            "Epoch [38/100], Step [ 62/ 78], Loss: 0.6024\n",
            "Epoch [38/100], Step [ 63/ 78], Loss: 0.5564\n",
            "Epoch [39/100], Step [  1/ 78], Loss: 0.5496\n",
            "Epoch [39/100], Step [  2/ 78], Loss: 0.5590\n",
            "Epoch [39/100], Step [  3/ 78], Loss: 0.5600\n",
            "Epoch [39/100], Step [  4/ 78], Loss: 0.5734\n",
            "Epoch [39/100], Step [  5/ 78], Loss: 0.5593\n",
            "Epoch [39/100], Step [  6/ 78], Loss: 0.6066\n",
            "Epoch [39/100], Step [  7/ 78], Loss: 0.6299\n",
            "Epoch [39/100], Step [  8/ 78], Loss: 0.5918\n",
            "Epoch [39/100], Step [  9/ 78], Loss: 0.5948\n",
            "Epoch [39/100], Step [ 10/ 78], Loss: 0.5767\n",
            "Epoch [39/100], Step [ 11/ 78], Loss: 0.5801\n",
            "Epoch [39/100], Step [ 12/ 78], Loss: 0.5654\n",
            "Epoch [39/100], Step [ 13/ 78], Loss: 0.5815\n",
            "Epoch [39/100], Step [ 14/ 78], Loss: 0.5838\n",
            "Epoch [39/100], Step [ 15/ 78], Loss: 0.5820\n",
            "Epoch [39/100], Step [ 16/ 78], Loss: 0.6036\n",
            "Epoch [39/100], Step [ 17/ 78], Loss: 0.6032\n",
            "Epoch [39/100], Step [ 18/ 78], Loss: 0.5703\n",
            "Epoch [39/100], Step [ 19/ 78], Loss: 0.5743\n",
            "Epoch [39/100], Step [ 20/ 78], Loss: 0.5953\n",
            "Epoch [39/100], Step [ 21/ 78], Loss: 0.5849\n",
            "Epoch [39/100], Step [ 22/ 78], Loss: 0.5842\n",
            "Epoch [39/100], Step [ 23/ 78], Loss: 0.5923\n",
            "Epoch [39/100], Step [ 24/ 78], Loss: 0.5705\n",
            "Epoch [39/100], Step [ 25/ 78], Loss: 0.6013\n",
            "Epoch [39/100], Step [ 26/ 78], Loss: 0.5575\n",
            "Epoch [39/100], Step [ 27/ 78], Loss: 0.6059\n",
            "Epoch [39/100], Step [ 28/ 78], Loss: 0.6167\n",
            "Epoch [39/100], Step [ 29/ 78], Loss: 0.6133\n",
            "Epoch [39/100], Step [ 30/ 78], Loss: 0.5829\n",
            "Epoch [39/100], Step [ 31/ 78], Loss: 0.5807\n",
            "Epoch [39/100], Step [ 32/ 78], Loss: 0.5977\n",
            "Epoch [39/100], Step [ 33/ 78], Loss: 0.5834\n",
            "Epoch [39/100], Step [ 34/ 78], Loss: 0.5811\n",
            "Epoch [39/100], Step [ 35/ 78], Loss: 0.5941\n",
            "Epoch [39/100], Step [ 36/ 78], Loss: 0.5879\n",
            "Epoch [39/100], Step [ 37/ 78], Loss: 0.5796\n",
            "Epoch [39/100], Step [ 38/ 78], Loss: 0.5972\n",
            "Epoch [39/100], Step [ 39/ 78], Loss: 0.5912\n",
            "Epoch [39/100], Step [ 40/ 78], Loss: 0.5966\n",
            "Epoch [39/100], Step [ 41/ 78], Loss: 0.5600\n",
            "Epoch [39/100], Step [ 42/ 78], Loss: 0.5692\n",
            "Epoch [39/100], Step [ 43/ 78], Loss: 0.5544\n",
            "Epoch [39/100], Step [ 44/ 78], Loss: 0.5677\n",
            "Epoch [39/100], Step [ 45/ 78], Loss: 0.5773\n",
            "Epoch [39/100], Step [ 46/ 78], Loss: 0.5635\n",
            "Epoch [39/100], Step [ 47/ 78], Loss: 0.6117\n",
            "Epoch [39/100], Step [ 48/ 78], Loss: 0.5745\n",
            "Epoch [39/100], Step [ 49/ 78], Loss: 0.6010\n",
            "Epoch [39/100], Step [ 50/ 78], Loss: 0.5682\n",
            "Epoch [39/100], Step [ 51/ 78], Loss: 0.6146\n",
            "Epoch [39/100], Step [ 52/ 78], Loss: 0.5671\n",
            "Epoch [39/100], Step [ 53/ 78], Loss: 0.5745\n",
            "Epoch [39/100], Step [ 54/ 78], Loss: 0.5521\n",
            "Epoch [39/100], Step [ 55/ 78], Loss: 0.5819\n",
            "Epoch [39/100], Step [ 56/ 78], Loss: 0.6297\n",
            "Epoch [39/100], Step [ 57/ 78], Loss: 0.6123\n",
            "Epoch [39/100], Step [ 58/ 78], Loss: 0.6068\n",
            "Epoch [39/100], Step [ 59/ 78], Loss: 0.5821\n",
            "Epoch [39/100], Step [ 60/ 78], Loss: 0.5726\n",
            "Epoch [39/100], Step [ 61/ 78], Loss: 0.5781\n",
            "Epoch [39/100], Step [ 62/ 78], Loss: 0.6034\n",
            "Epoch [39/100], Step [ 63/ 78], Loss: 0.5719\n",
            "Epoch [40/100], Step [  1/ 78], Loss: 0.5514\n",
            "Epoch [40/100], Step [  2/ 78], Loss: 0.6034\n",
            "Epoch [40/100], Step [  3/ 78], Loss: 0.5652\n",
            "Epoch [40/100], Step [  4/ 78], Loss: 0.5822\n",
            "Epoch [40/100], Step [  5/ 78], Loss: 0.6167\n",
            "Epoch [40/100], Step [  6/ 78], Loss: 0.5638\n",
            "Epoch [40/100], Step [  7/ 78], Loss: 0.5894\n",
            "Epoch [40/100], Step [  8/ 78], Loss: 0.5748\n",
            "Epoch [40/100], Step [  9/ 78], Loss: 0.6232\n",
            "Epoch [40/100], Step [ 10/ 78], Loss: 0.5718\n",
            "Epoch [40/100], Step [ 11/ 78], Loss: 0.5746\n",
            "Epoch [40/100], Step [ 12/ 78], Loss: 0.5665\n",
            "Epoch [40/100], Step [ 13/ 78], Loss: 0.5693\n",
            "Epoch [40/100], Step [ 14/ 78], Loss: 0.6150\n",
            "Epoch [40/100], Step [ 15/ 78], Loss: 0.5991\n",
            "Epoch [40/100], Step [ 16/ 78], Loss: 0.6418\n",
            "Epoch [40/100], Step [ 17/ 78], Loss: 0.5883\n",
            "Epoch [40/100], Step [ 18/ 78], Loss: 0.5807\n",
            "Epoch [40/100], Step [ 19/ 78], Loss: 0.6398\n",
            "Epoch [40/100], Step [ 20/ 78], Loss: 0.6059\n",
            "Epoch [40/100], Step [ 21/ 78], Loss: 0.5521\n",
            "Epoch [40/100], Step [ 22/ 78], Loss: 0.5883\n",
            "Epoch [40/100], Step [ 23/ 78], Loss: 0.6007\n",
            "Epoch [40/100], Step [ 24/ 78], Loss: 0.5822\n",
            "Epoch [40/100], Step [ 25/ 78], Loss: 0.5967\n",
            "Epoch [40/100], Step [ 26/ 78], Loss: 0.5618\n",
            "Epoch [40/100], Step [ 27/ 78], Loss: 0.5689\n",
            "Epoch [40/100], Step [ 28/ 78], Loss: 0.5826\n",
            "Epoch [40/100], Step [ 29/ 78], Loss: 0.5691\n",
            "Epoch [40/100], Step [ 30/ 78], Loss: 0.5580\n",
            "Epoch [40/100], Step [ 31/ 78], Loss: 0.5660\n",
            "Epoch [40/100], Step [ 32/ 78], Loss: 0.5937\n",
            "Epoch [40/100], Step [ 33/ 78], Loss: 0.5575\n",
            "Epoch [40/100], Step [ 34/ 78], Loss: 0.5937\n",
            "Epoch [40/100], Step [ 35/ 78], Loss: 0.5595\n",
            "Epoch [40/100], Step [ 36/ 78], Loss: 0.6138\n",
            "Epoch [40/100], Step [ 37/ 78], Loss: 0.5760\n",
            "Epoch [40/100], Step [ 38/ 78], Loss: 0.5711\n",
            "Epoch [40/100], Step [ 39/ 78], Loss: 0.5701\n",
            "Epoch [40/100], Step [ 40/ 78], Loss: 0.6256\n",
            "Epoch [40/100], Step [ 41/ 78], Loss: 0.5559\n",
            "Epoch [40/100], Step [ 42/ 78], Loss: 0.5971\n",
            "Epoch [40/100], Step [ 43/ 78], Loss: 0.5489\n",
            "Epoch [40/100], Step [ 44/ 78], Loss: 0.5917\n",
            "Epoch [40/100], Step [ 45/ 78], Loss: 0.5795\n",
            "Epoch [40/100], Step [ 46/ 78], Loss: 0.5797\n",
            "Epoch [40/100], Step [ 47/ 78], Loss: 0.6155\n",
            "Epoch [40/100], Step [ 48/ 78], Loss: 0.5635\n",
            "Epoch [40/100], Step [ 49/ 78], Loss: 0.5752\n",
            "Epoch [40/100], Step [ 50/ 78], Loss: 0.5538\n",
            "Epoch [40/100], Step [ 51/ 78], Loss: 0.5662\n",
            "Epoch [40/100], Step [ 52/ 78], Loss: 0.5916\n",
            "Epoch [40/100], Step [ 53/ 78], Loss: 0.5745\n",
            "Epoch [40/100], Step [ 54/ 78], Loss: 0.5918\n",
            "Epoch [40/100], Step [ 55/ 78], Loss: 0.5962\n",
            "Epoch [40/100], Step [ 56/ 78], Loss: 0.5806\n",
            "Epoch [40/100], Step [ 57/ 78], Loss: 0.5903\n",
            "Epoch [40/100], Step [ 58/ 78], Loss: 0.5898\n",
            "Epoch [40/100], Step [ 59/ 78], Loss: 0.5552\n",
            "Epoch [40/100], Step [ 60/ 78], Loss: 0.5596\n",
            "Epoch [40/100], Step [ 61/ 78], Loss: 0.5884\n",
            "Epoch [40/100], Step [ 62/ 78], Loss: 0.5710\n",
            "Epoch [40/100], Step [ 63/ 78], Loss: 0.6054\n",
            "Epoch [41/100], Step [  1/ 78], Loss: 0.5834\n",
            "Epoch [41/100], Step [  2/ 78], Loss: 0.5931\n",
            "Epoch [41/100], Step [  3/ 78], Loss: 0.6209\n",
            "Epoch [41/100], Step [  4/ 78], Loss: 0.5971\n",
            "Epoch [41/100], Step [  5/ 78], Loss: 0.5673\n",
            "Epoch [41/100], Step [  6/ 78], Loss: 0.5606\n",
            "Epoch [41/100], Step [  7/ 78], Loss: 0.5897\n",
            "Epoch [41/100], Step [  8/ 78], Loss: 0.5729\n",
            "Epoch [41/100], Step [  9/ 78], Loss: 0.5774\n",
            "Epoch [41/100], Step [ 10/ 78], Loss: 0.5640\n",
            "Epoch [41/100], Step [ 11/ 78], Loss: 0.5737\n",
            "Epoch [41/100], Step [ 12/ 78], Loss: 0.5888\n",
            "Epoch [41/100], Step [ 13/ 78], Loss: 0.5969\n",
            "Epoch [41/100], Step [ 14/ 78], Loss: 0.5555\n",
            "Epoch [41/100], Step [ 15/ 78], Loss: 0.5669\n",
            "Epoch [41/100], Step [ 16/ 78], Loss: 0.6032\n",
            "Epoch [41/100], Step [ 17/ 78], Loss: 0.5611\n",
            "Epoch [41/100], Step [ 18/ 78], Loss: 0.6123\n",
            "Epoch [41/100], Step [ 19/ 78], Loss: 0.6069\n",
            "Epoch [41/100], Step [ 20/ 78], Loss: 0.5814\n",
            "Epoch [41/100], Step [ 21/ 78], Loss: 0.5841\n",
            "Epoch [41/100], Step [ 22/ 78], Loss: 0.5710\n",
            "Epoch [41/100], Step [ 23/ 78], Loss: 0.5794\n",
            "Epoch [41/100], Step [ 24/ 78], Loss: 0.5560\n",
            "Epoch [41/100], Step [ 25/ 78], Loss: 0.5331\n",
            "Epoch [41/100], Step [ 26/ 78], Loss: 0.6101\n",
            "Epoch [41/100], Step [ 27/ 78], Loss: 0.6283\n",
            "Epoch [41/100], Step [ 28/ 78], Loss: 0.5386\n",
            "Epoch [41/100], Step [ 29/ 78], Loss: 0.5997\n",
            "Epoch [41/100], Step [ 30/ 78], Loss: 0.5835\n",
            "Epoch [41/100], Step [ 31/ 78], Loss: 0.6092\n",
            "Epoch [41/100], Step [ 32/ 78], Loss: 0.5641\n",
            "Epoch [41/100], Step [ 33/ 78], Loss: 0.5581\n",
            "Epoch [41/100], Step [ 34/ 78], Loss: 0.5535\n",
            "Epoch [41/100], Step [ 35/ 78], Loss: 0.5823\n",
            "Epoch [41/100], Step [ 36/ 78], Loss: 0.5742\n",
            "Epoch [41/100], Step [ 37/ 78], Loss: 0.5823\n",
            "Epoch [41/100], Step [ 38/ 78], Loss: 0.5812\n",
            "Epoch [41/100], Step [ 39/ 78], Loss: 0.6100\n",
            "Epoch [41/100], Step [ 40/ 78], Loss: 0.5608\n",
            "Epoch [41/100], Step [ 41/ 78], Loss: 0.5741\n",
            "Epoch [41/100], Step [ 42/ 78], Loss: 0.5720\n",
            "Epoch [41/100], Step [ 43/ 78], Loss: 0.5920\n",
            "Epoch [41/100], Step [ 44/ 78], Loss: 0.5839\n",
            "Epoch [41/100], Step [ 45/ 78], Loss: 0.5958\n",
            "Epoch [41/100], Step [ 46/ 78], Loss: 0.5629\n",
            "Epoch [41/100], Step [ 47/ 78], Loss: 0.5753\n",
            "Epoch [41/100], Step [ 48/ 78], Loss: 0.5581\n",
            "Epoch [41/100], Step [ 49/ 78], Loss: 0.5716\n",
            "Epoch [41/100], Step [ 50/ 78], Loss: 0.5911\n",
            "Epoch [41/100], Step [ 51/ 78], Loss: 0.6145\n",
            "Epoch [41/100], Step [ 52/ 78], Loss: 0.5885\n",
            "Epoch [41/100], Step [ 53/ 78], Loss: 0.6044\n",
            "Epoch [41/100], Step [ 54/ 78], Loss: 0.6216\n",
            "Epoch [41/100], Step [ 55/ 78], Loss: 0.5952\n",
            "Epoch [41/100], Step [ 56/ 78], Loss: 0.5621\n",
            "Epoch [41/100], Step [ 57/ 78], Loss: 0.5680\n",
            "Epoch [41/100], Step [ 58/ 78], Loss: 0.5949\n",
            "Epoch [41/100], Step [ 59/ 78], Loss: 0.5882\n",
            "Epoch [41/100], Step [ 60/ 78], Loss: 0.5952\n",
            "Epoch [41/100], Step [ 61/ 78], Loss: 0.5707\n",
            "Epoch [41/100], Step [ 62/ 78], Loss: 0.5890\n",
            "Epoch [41/100], Step [ 63/ 78], Loss: 0.5325\n",
            "Epoch [42/100], Step [  1/ 78], Loss: 0.5750\n",
            "Epoch [42/100], Step [  2/ 78], Loss: 0.5545\n",
            "Epoch [42/100], Step [  3/ 78], Loss: 0.5634\n",
            "Epoch [42/100], Step [  4/ 78], Loss: 0.5603\n",
            "Epoch [42/100], Step [  5/ 78], Loss: 0.5754\n",
            "Epoch [42/100], Step [  6/ 78], Loss: 0.5468\n",
            "Epoch [42/100], Step [  7/ 78], Loss: 0.5652\n",
            "Epoch [42/100], Step [  8/ 78], Loss: 0.5604\n",
            "Epoch [42/100], Step [  9/ 78], Loss: 0.5712\n",
            "Epoch [42/100], Step [ 10/ 78], Loss: 0.5864\n",
            "Epoch [42/100], Step [ 11/ 78], Loss: 0.6058\n",
            "Epoch [42/100], Step [ 12/ 78], Loss: 0.5953\n",
            "Epoch [42/100], Step [ 13/ 78], Loss: 0.6191\n",
            "Epoch [42/100], Step [ 14/ 78], Loss: 0.6426\n",
            "Epoch [42/100], Step [ 15/ 78], Loss: 0.6170\n",
            "Epoch [42/100], Step [ 16/ 78], Loss: 0.5664\n",
            "Epoch [42/100], Step [ 17/ 78], Loss: 0.5976\n",
            "Epoch [42/100], Step [ 18/ 78], Loss: 0.5752\n",
            "Epoch [42/100], Step [ 19/ 78], Loss: 0.5517\n",
            "Epoch [42/100], Step [ 20/ 78], Loss: 0.5979\n",
            "Epoch [42/100], Step [ 21/ 78], Loss: 0.5731\n",
            "Epoch [42/100], Step [ 22/ 78], Loss: 0.5655\n",
            "Epoch [42/100], Step [ 23/ 78], Loss: 0.5422\n",
            "Epoch [42/100], Step [ 24/ 78], Loss: 0.5663\n",
            "Epoch [42/100], Step [ 25/ 78], Loss: 0.5942\n",
            "Epoch [42/100], Step [ 26/ 78], Loss: 0.5486\n",
            "Epoch [42/100], Step [ 27/ 78], Loss: 0.5989\n",
            "Epoch [42/100], Step [ 28/ 78], Loss: 0.5886\n",
            "Epoch [42/100], Step [ 29/ 78], Loss: 0.5529\n",
            "Epoch [42/100], Step [ 30/ 78], Loss: 0.5786\n",
            "Epoch [42/100], Step [ 31/ 78], Loss: 0.5895\n",
            "Epoch [42/100], Step [ 32/ 78], Loss: 0.5873\n",
            "Epoch [42/100], Step [ 33/ 78], Loss: 0.5656\n",
            "Epoch [42/100], Step [ 34/ 78], Loss: 0.5847\n",
            "Epoch [42/100], Step [ 35/ 78], Loss: 0.5536\n",
            "Epoch [42/100], Step [ 36/ 78], Loss: 0.5756\n",
            "Epoch [42/100], Step [ 37/ 78], Loss: 0.5999\n",
            "Epoch [42/100], Step [ 38/ 78], Loss: 0.6294\n",
            "Epoch [42/100], Step [ 39/ 78], Loss: 0.6331\n",
            "Epoch [42/100], Step [ 40/ 78], Loss: 0.6130\n",
            "Epoch [42/100], Step [ 41/ 78], Loss: 0.5936\n",
            "Epoch [42/100], Step [ 42/ 78], Loss: 0.5690\n",
            "Epoch [42/100], Step [ 43/ 78], Loss: 0.6065\n",
            "Epoch [42/100], Step [ 44/ 78], Loss: 0.5935\n",
            "Epoch [42/100], Step [ 45/ 78], Loss: 0.5322\n",
            "Epoch [42/100], Step [ 46/ 78], Loss: 0.5991\n",
            "Epoch [42/100], Step [ 47/ 78], Loss: 0.5546\n",
            "Epoch [42/100], Step [ 48/ 78], Loss: 0.5638\n",
            "Epoch [42/100], Step [ 49/ 78], Loss: 0.5545\n",
            "Epoch [42/100], Step [ 50/ 78], Loss: 0.6294\n",
            "Epoch [42/100], Step [ 51/ 78], Loss: 0.6049\n",
            "Epoch [42/100], Step [ 52/ 78], Loss: 0.5878\n",
            "Epoch [42/100], Step [ 53/ 78], Loss: 0.5765\n",
            "Epoch [42/100], Step [ 54/ 78], Loss: 0.6647\n",
            "Epoch [42/100], Step [ 55/ 78], Loss: 0.5674\n",
            "Epoch [42/100], Step [ 56/ 78], Loss: 0.5907\n",
            "Epoch [42/100], Step [ 57/ 78], Loss: 0.5804\n",
            "Epoch [42/100], Step [ 58/ 78], Loss: 0.5592\n",
            "Epoch [42/100], Step [ 59/ 78], Loss: 0.5751\n",
            "Epoch [42/100], Step [ 60/ 78], Loss: 0.5852\n",
            "Epoch [42/100], Step [ 61/ 78], Loss: 0.5811\n",
            "Epoch [42/100], Step [ 62/ 78], Loss: 0.5628\n",
            "Epoch [42/100], Step [ 63/ 78], Loss: 0.5942\n",
            "Epoch [43/100], Step [  1/ 78], Loss: 0.5889\n",
            "Epoch [43/100], Step [  2/ 78], Loss: 0.6237\n",
            "Epoch [43/100], Step [  3/ 78], Loss: 0.5585\n",
            "Epoch [43/100], Step [  4/ 78], Loss: 0.5587\n",
            "Epoch [43/100], Step [  5/ 78], Loss: 0.5879\n",
            "Epoch [43/100], Step [  6/ 78], Loss: 0.5517\n",
            "Epoch [43/100], Step [  7/ 78], Loss: 0.5630\n",
            "Epoch [43/100], Step [  8/ 78], Loss: 0.5825\n",
            "Epoch [43/100], Step [  9/ 78], Loss: 0.5732\n",
            "Epoch [43/100], Step [ 10/ 78], Loss: 0.5401\n",
            "Epoch [43/100], Step [ 11/ 78], Loss: 0.6255\n",
            "Epoch [43/100], Step [ 12/ 78], Loss: 0.5742\n",
            "Epoch [43/100], Step [ 13/ 78], Loss: 0.5980\n",
            "Epoch [43/100], Step [ 14/ 78], Loss: 0.5674\n",
            "Epoch [43/100], Step [ 15/ 78], Loss: 0.5649\n",
            "Epoch [43/100], Step [ 16/ 78], Loss: 0.5819\n",
            "Epoch [43/100], Step [ 17/ 78], Loss: 0.5502\n",
            "Epoch [43/100], Step [ 18/ 78], Loss: 0.5786\n",
            "Epoch [43/100], Step [ 19/ 78], Loss: 0.5709\n",
            "Epoch [43/100], Step [ 20/ 78], Loss: 0.5953\n",
            "Epoch [43/100], Step [ 21/ 78], Loss: 0.5740\n",
            "Epoch [43/100], Step [ 22/ 78], Loss: 0.6210\n",
            "Epoch [43/100], Step [ 23/ 78], Loss: 0.6095\n",
            "Epoch [43/100], Step [ 24/ 78], Loss: 0.5596\n",
            "Epoch [43/100], Step [ 25/ 78], Loss: 0.5706\n",
            "Epoch [43/100], Step [ 26/ 78], Loss: 0.5578\n",
            "Epoch [43/100], Step [ 27/ 78], Loss: 0.5861\n",
            "Epoch [43/100], Step [ 28/ 78], Loss: 0.5720\n",
            "Epoch [43/100], Step [ 29/ 78], Loss: 0.5767\n",
            "Epoch [43/100], Step [ 30/ 78], Loss: 0.6236\n",
            "Epoch [43/100], Step [ 31/ 78], Loss: 0.5800\n",
            "Epoch [43/100], Step [ 32/ 78], Loss: 0.5514\n",
            "Epoch [43/100], Step [ 33/ 78], Loss: 0.5916\n",
            "Epoch [43/100], Step [ 34/ 78], Loss: 0.6170\n",
            "Epoch [43/100], Step [ 35/ 78], Loss: 0.5993\n",
            "Epoch [43/100], Step [ 36/ 78], Loss: 0.5970\n",
            "Epoch [43/100], Step [ 37/ 78], Loss: 0.5758\n",
            "Epoch [43/100], Step [ 38/ 78], Loss: 0.5993\n",
            "Epoch [43/100], Step [ 39/ 78], Loss: 0.5900\n",
            "Epoch [43/100], Step [ 40/ 78], Loss: 0.5817\n",
            "Epoch [43/100], Step [ 41/ 78], Loss: 0.5755\n",
            "Epoch [43/100], Step [ 42/ 78], Loss: 0.5713\n",
            "Epoch [43/100], Step [ 43/ 78], Loss: 0.5890\n",
            "Epoch [43/100], Step [ 44/ 78], Loss: 0.5902\n",
            "Epoch [43/100], Step [ 45/ 78], Loss: 0.5715\n",
            "Epoch [43/100], Step [ 46/ 78], Loss: 0.5890\n",
            "Epoch [43/100], Step [ 47/ 78], Loss: 0.5565\n",
            "Epoch [43/100], Step [ 48/ 78], Loss: 0.5533\n",
            "Epoch [43/100], Step [ 49/ 78], Loss: 0.6171\n",
            "Epoch [43/100], Step [ 50/ 78], Loss: 0.5497\n",
            "Epoch [43/100], Step [ 51/ 78], Loss: 0.5865\n",
            "Epoch [43/100], Step [ 52/ 78], Loss: 0.5960\n",
            "Epoch [43/100], Step [ 53/ 78], Loss: 0.5809\n",
            "Epoch [43/100], Step [ 54/ 78], Loss: 0.5522\n",
            "Epoch [43/100], Step [ 55/ 78], Loss: 0.5988\n",
            "Epoch [43/100], Step [ 56/ 78], Loss: 0.5707\n",
            "Epoch [43/100], Step [ 57/ 78], Loss: 0.5802\n",
            "Epoch [43/100], Step [ 58/ 78], Loss: 0.5948\n",
            "Epoch [43/100], Step [ 59/ 78], Loss: 0.5311\n",
            "Epoch [43/100], Step [ 60/ 78], Loss: 0.5776\n",
            "Epoch [43/100], Step [ 61/ 78], Loss: 0.5758\n",
            "Epoch [43/100], Step [ 62/ 78], Loss: 0.6005\n",
            "Epoch [43/100], Step [ 63/ 78], Loss: 0.5950\n",
            "Epoch [44/100], Step [  1/ 78], Loss: 0.6029\n",
            "Epoch [44/100], Step [  2/ 78], Loss: 0.5601\n",
            "Epoch [44/100], Step [  3/ 78], Loss: 0.5525\n",
            "Epoch [44/100], Step [  4/ 78], Loss: 0.5411\n",
            "Epoch [44/100], Step [  5/ 78], Loss: 0.5785\n",
            "Epoch [44/100], Step [  6/ 78], Loss: 0.5782\n",
            "Epoch [44/100], Step [  7/ 78], Loss: 0.5846\n",
            "Epoch [44/100], Step [  8/ 78], Loss: 0.5974\n",
            "Epoch [44/100], Step [  9/ 78], Loss: 0.5936\n",
            "Epoch [44/100], Step [ 10/ 78], Loss: 0.6210\n",
            "Epoch [44/100], Step [ 11/ 78], Loss: 0.5822\n",
            "Epoch [44/100], Step [ 12/ 78], Loss: 0.5650\n",
            "Epoch [44/100], Step [ 13/ 78], Loss: 0.5614\n",
            "Epoch [44/100], Step [ 14/ 78], Loss: 0.5859\n",
            "Epoch [44/100], Step [ 15/ 78], Loss: 0.5552\n",
            "Epoch [44/100], Step [ 16/ 78], Loss: 0.6074\n",
            "Epoch [44/100], Step [ 17/ 78], Loss: 0.6079\n",
            "Epoch [44/100], Step [ 18/ 78], Loss: 0.5795\n",
            "Epoch [44/100], Step [ 19/ 78], Loss: 0.5528\n",
            "Epoch [44/100], Step [ 20/ 78], Loss: 0.5720\n",
            "Epoch [44/100], Step [ 21/ 78], Loss: 0.5692\n",
            "Epoch [44/100], Step [ 22/ 78], Loss: 0.5537\n",
            "Epoch [44/100], Step [ 23/ 78], Loss: 0.5616\n",
            "Epoch [44/100], Step [ 24/ 78], Loss: 0.5431\n",
            "Epoch [44/100], Step [ 25/ 78], Loss: 0.6083\n",
            "Epoch [44/100], Step [ 26/ 78], Loss: 0.5916\n",
            "Epoch [44/100], Step [ 27/ 78], Loss: 0.5520\n",
            "Epoch [44/100], Step [ 28/ 78], Loss: 0.5705\n",
            "Epoch [44/100], Step [ 29/ 78], Loss: 0.5708\n",
            "Epoch [44/100], Step [ 30/ 78], Loss: 0.5813\n",
            "Epoch [44/100], Step [ 31/ 78], Loss: 0.5968\n",
            "Epoch [44/100], Step [ 32/ 78], Loss: 0.5427\n",
            "Epoch [44/100], Step [ 33/ 78], Loss: 0.6269\n",
            "Epoch [44/100], Step [ 34/ 78], Loss: 0.5828\n",
            "Epoch [44/100], Step [ 35/ 78], Loss: 0.5934\n",
            "Epoch [44/100], Step [ 36/ 78], Loss: 0.5498\n",
            "Epoch [44/100], Step [ 37/ 78], Loss: 0.5547\n",
            "Epoch [44/100], Step [ 38/ 78], Loss: 0.5730\n",
            "Epoch [44/100], Step [ 39/ 78], Loss: 0.5815\n",
            "Epoch [44/100], Step [ 40/ 78], Loss: 0.6028\n",
            "Epoch [44/100], Step [ 41/ 78], Loss: 0.5714\n",
            "Epoch [44/100], Step [ 42/ 78], Loss: 0.5486\n",
            "Epoch [44/100], Step [ 43/ 78], Loss: 0.5980\n",
            "Epoch [44/100], Step [ 44/ 78], Loss: 0.5707\n",
            "Epoch [44/100], Step [ 45/ 78], Loss: 0.5940\n",
            "Epoch [44/100], Step [ 46/ 78], Loss: 0.5856\n",
            "Epoch [44/100], Step [ 47/ 78], Loss: 0.5491\n",
            "Epoch [44/100], Step [ 48/ 78], Loss: 0.5809\n",
            "Epoch [44/100], Step [ 49/ 78], Loss: 0.5530\n",
            "Epoch [44/100], Step [ 50/ 78], Loss: 0.5968\n",
            "Epoch [44/100], Step [ 51/ 78], Loss: 0.5490\n",
            "Epoch [44/100], Step [ 52/ 78], Loss: 0.5992\n",
            "Epoch [44/100], Step [ 53/ 78], Loss: 0.5942\n",
            "Epoch [44/100], Step [ 54/ 78], Loss: 0.5624\n",
            "Epoch [44/100], Step [ 55/ 78], Loss: 0.5449\n",
            "Epoch [44/100], Step [ 56/ 78], Loss: 0.5638\n",
            "Epoch [44/100], Step [ 57/ 78], Loss: 0.5844\n",
            "Epoch [44/100], Step [ 58/ 78], Loss: 0.5757\n",
            "Epoch [44/100], Step [ 59/ 78], Loss: 0.5680\n",
            "Epoch [44/100], Step [ 60/ 78], Loss: 0.5537\n",
            "Epoch [44/100], Step [ 61/ 78], Loss: 0.5760\n",
            "Epoch [44/100], Step [ 62/ 78], Loss: 0.5394\n",
            "Epoch [44/100], Step [ 63/ 78], Loss: 0.5686\n",
            "Epoch [45/100], Step [  1/ 78], Loss: 0.5695\n",
            "Epoch [45/100], Step [  2/ 78], Loss: 0.6049\n",
            "Epoch [45/100], Step [  3/ 78], Loss: 0.5764\n",
            "Epoch [45/100], Step [  4/ 78], Loss: 0.5683\n",
            "Epoch [45/100], Step [  5/ 78], Loss: 0.5853\n",
            "Epoch [45/100], Step [  6/ 78], Loss: 0.5595\n",
            "Epoch [45/100], Step [  7/ 78], Loss: 0.5972\n",
            "Epoch [45/100], Step [  8/ 78], Loss: 0.6065\n",
            "Epoch [45/100], Step [  9/ 78], Loss: 0.5677\n",
            "Epoch [45/100], Step [ 10/ 78], Loss: 0.6208\n",
            "Epoch [45/100], Step [ 11/ 78], Loss: 0.5975\n",
            "Epoch [45/100], Step [ 12/ 78], Loss: 0.5815\n",
            "Epoch [45/100], Step [ 13/ 78], Loss: 0.5547\n",
            "Epoch [45/100], Step [ 14/ 78], Loss: 0.5732\n",
            "Epoch [45/100], Step [ 15/ 78], Loss: 0.6041\n",
            "Epoch [45/100], Step [ 16/ 78], Loss: 0.5774\n",
            "Epoch [45/100], Step [ 17/ 78], Loss: 0.5857\n",
            "Epoch [45/100], Step [ 18/ 78], Loss: 0.5858\n",
            "Epoch [45/100], Step [ 19/ 78], Loss: 0.5792\n",
            "Epoch [45/100], Step [ 20/ 78], Loss: 0.5821\n",
            "Epoch [45/100], Step [ 21/ 78], Loss: 0.5892\n",
            "Epoch [45/100], Step [ 22/ 78], Loss: 0.5604\n",
            "Epoch [45/100], Step [ 23/ 78], Loss: 0.5934\n",
            "Epoch [45/100], Step [ 24/ 78], Loss: 0.5777\n",
            "Epoch [45/100], Step [ 25/ 78], Loss: 0.5916\n",
            "Epoch [45/100], Step [ 26/ 78], Loss: 0.5726\n",
            "Epoch [45/100], Step [ 27/ 78], Loss: 0.5547\n",
            "Epoch [45/100], Step [ 28/ 78], Loss: 0.5659\n",
            "Epoch [45/100], Step [ 29/ 78], Loss: 0.5692\n",
            "Epoch [45/100], Step [ 30/ 78], Loss: 0.5785\n",
            "Epoch [45/100], Step [ 31/ 78], Loss: 0.5734\n",
            "Epoch [45/100], Step [ 32/ 78], Loss: 0.6033\n",
            "Epoch [45/100], Step [ 33/ 78], Loss: 0.5641\n",
            "Epoch [45/100], Step [ 34/ 78], Loss: 0.5697\n",
            "Epoch [45/100], Step [ 35/ 78], Loss: 0.5609\n",
            "Epoch [45/100], Step [ 36/ 78], Loss: 0.5749\n",
            "Epoch [45/100], Step [ 37/ 78], Loss: 0.5596\n",
            "Epoch [45/100], Step [ 38/ 78], Loss: 0.5544\n",
            "Epoch [45/100], Step [ 39/ 78], Loss: 0.5578\n",
            "Epoch [45/100], Step [ 40/ 78], Loss: 0.6134\n",
            "Epoch [45/100], Step [ 41/ 78], Loss: 0.5555\n",
            "Epoch [45/100], Step [ 42/ 78], Loss: 0.5820\n",
            "Epoch [45/100], Step [ 43/ 78], Loss: 0.5693\n",
            "Epoch [45/100], Step [ 44/ 78], Loss: 0.5538\n",
            "Epoch [45/100], Step [ 45/ 78], Loss: 0.5543\n",
            "Epoch [45/100], Step [ 46/ 78], Loss: 0.5897\n",
            "Epoch [45/100], Step [ 47/ 78], Loss: 0.6250\n",
            "Epoch [45/100], Step [ 48/ 78], Loss: 0.5692\n",
            "Epoch [45/100], Step [ 49/ 78], Loss: 0.5636\n",
            "Epoch [45/100], Step [ 50/ 78], Loss: 0.6034\n",
            "Epoch [45/100], Step [ 51/ 78], Loss: 0.5616\n",
            "Epoch [45/100], Step [ 52/ 78], Loss: 0.5935\n",
            "Epoch [45/100], Step [ 53/ 78], Loss: 0.5606\n",
            "Epoch [45/100], Step [ 54/ 78], Loss: 0.5411\n",
            "Epoch [45/100], Step [ 55/ 78], Loss: 0.5370\n",
            "Epoch [45/100], Step [ 56/ 78], Loss: 0.5743\n",
            "Epoch [45/100], Step [ 57/ 78], Loss: 0.5804\n",
            "Epoch [45/100], Step [ 58/ 78], Loss: 0.5724\n",
            "Epoch [45/100], Step [ 59/ 78], Loss: 0.5553\n",
            "Epoch [45/100], Step [ 60/ 78], Loss: 0.5688\n",
            "Epoch [45/100], Step [ 61/ 78], Loss: 0.6034\n",
            "Epoch [45/100], Step [ 62/ 78], Loss: 0.5946\n",
            "Epoch [45/100], Step [ 63/ 78], Loss: 0.5604\n",
            "Epoch [46/100], Step [  1/ 78], Loss: 0.5742\n",
            "Epoch [46/100], Step [  2/ 78], Loss: 0.5578\n",
            "Epoch [46/100], Step [  3/ 78], Loss: 0.5879\n",
            "Epoch [46/100], Step [  4/ 78], Loss: 0.5530\n",
            "Epoch [46/100], Step [  5/ 78], Loss: 0.5671\n",
            "Epoch [46/100], Step [  6/ 78], Loss: 0.5582\n",
            "Epoch [46/100], Step [  7/ 78], Loss: 0.5760\n",
            "Epoch [46/100], Step [  8/ 78], Loss: 0.5437\n",
            "Epoch [46/100], Step [  9/ 78], Loss: 0.5896\n",
            "Epoch [46/100], Step [ 10/ 78], Loss: 0.5698\n",
            "Epoch [46/100], Step [ 11/ 78], Loss: 0.5486\n",
            "Epoch [46/100], Step [ 12/ 78], Loss: 0.5736\n",
            "Epoch [46/100], Step [ 13/ 78], Loss: 0.5796\n",
            "Epoch [46/100], Step [ 14/ 78], Loss: 0.5515\n",
            "Epoch [46/100], Step [ 15/ 78], Loss: 0.5861\n",
            "Epoch [46/100], Step [ 16/ 78], Loss: 0.5425\n",
            "Epoch [46/100], Step [ 17/ 78], Loss: 0.5821\n",
            "Epoch [46/100], Step [ 18/ 78], Loss: 0.5482\n",
            "Epoch [46/100], Step [ 19/ 78], Loss: 0.5712\n",
            "Epoch [46/100], Step [ 20/ 78], Loss: 0.5629\n",
            "Epoch [46/100], Step [ 21/ 78], Loss: 0.5549\n",
            "Epoch [46/100], Step [ 22/ 78], Loss: 0.5697\n",
            "Epoch [46/100], Step [ 23/ 78], Loss: 0.5761\n",
            "Epoch [46/100], Step [ 24/ 78], Loss: 0.5752\n",
            "Epoch [46/100], Step [ 25/ 78], Loss: 0.5809\n",
            "Epoch [46/100], Step [ 26/ 78], Loss: 0.5953\n",
            "Epoch [46/100], Step [ 27/ 78], Loss: 0.5935\n",
            "Epoch [46/100], Step [ 28/ 78], Loss: 0.6496\n",
            "Epoch [46/100], Step [ 29/ 78], Loss: 0.6067\n",
            "Epoch [46/100], Step [ 30/ 78], Loss: 0.5619\n",
            "Epoch [46/100], Step [ 31/ 78], Loss: 0.5599\n",
            "Epoch [46/100], Step [ 32/ 78], Loss: 0.5219\n",
            "Epoch [46/100], Step [ 33/ 78], Loss: 0.5540\n",
            "Epoch [46/100], Step [ 34/ 78], Loss: 0.5790\n",
            "Epoch [46/100], Step [ 35/ 78], Loss: 0.5503\n",
            "Epoch [46/100], Step [ 36/ 78], Loss: 0.5777\n",
            "Epoch [46/100], Step [ 37/ 78], Loss: 0.5712\n",
            "Epoch [46/100], Step [ 38/ 78], Loss: 0.5659\n",
            "Epoch [46/100], Step [ 39/ 78], Loss: 0.5515\n",
            "Epoch [46/100], Step [ 40/ 78], Loss: 0.5832\n",
            "Epoch [46/100], Step [ 41/ 78], Loss: 0.5221\n",
            "Epoch [46/100], Step [ 42/ 78], Loss: 0.5897\n",
            "Epoch [46/100], Step [ 43/ 78], Loss: 0.5583\n",
            "Epoch [46/100], Step [ 44/ 78], Loss: 0.5704\n",
            "Epoch [46/100], Step [ 45/ 78], Loss: 0.5918\n",
            "Epoch [46/100], Step [ 46/ 78], Loss: 0.6117\n",
            "Epoch [46/100], Step [ 47/ 78], Loss: 0.5477\n",
            "Epoch [46/100], Step [ 48/ 78], Loss: 0.5375\n",
            "Epoch [46/100], Step [ 49/ 78], Loss: 0.5412\n",
            "Epoch [46/100], Step [ 50/ 78], Loss: 0.5844\n",
            "Epoch [46/100], Step [ 51/ 78], Loss: 0.5658\n",
            "Epoch [46/100], Step [ 52/ 78], Loss: 0.5775\n",
            "Epoch [46/100], Step [ 53/ 78], Loss: 0.5767\n",
            "Epoch [46/100], Step [ 54/ 78], Loss: 0.5565\n",
            "Epoch [46/100], Step [ 55/ 78], Loss: 0.5632\n",
            "Epoch [46/100], Step [ 56/ 78], Loss: 0.5990\n",
            "Epoch [46/100], Step [ 57/ 78], Loss: 0.5808\n",
            "Epoch [46/100], Step [ 58/ 78], Loss: 0.5815\n",
            "Epoch [46/100], Step [ 59/ 78], Loss: 0.6045\n",
            "Epoch [46/100], Step [ 60/ 78], Loss: 0.6213\n",
            "Epoch [46/100], Step [ 61/ 78], Loss: 0.6111\n",
            "Epoch [46/100], Step [ 62/ 78], Loss: 0.6006\n",
            "Epoch [46/100], Step [ 63/ 78], Loss: 0.6073\n",
            "Epoch [47/100], Step [  1/ 78], Loss: 0.5684\n",
            "Epoch [47/100], Step [  2/ 78], Loss: 0.5842\n",
            "Epoch [47/100], Step [  3/ 78], Loss: 0.5647\n",
            "Epoch [47/100], Step [  4/ 78], Loss: 0.5351\n",
            "Epoch [47/100], Step [  5/ 78], Loss: 0.5682\n",
            "Epoch [47/100], Step [  6/ 78], Loss: 0.5769\n",
            "Epoch [47/100], Step [  7/ 78], Loss: 0.5744\n",
            "Epoch [47/100], Step [  8/ 78], Loss: 0.5501\n",
            "Epoch [47/100], Step [  9/ 78], Loss: 0.5341\n",
            "Epoch [47/100], Step [ 10/ 78], Loss: 0.5702\n",
            "Epoch [47/100], Step [ 11/ 78], Loss: 0.6108\n",
            "Epoch [47/100], Step [ 12/ 78], Loss: 0.5968\n",
            "Epoch [47/100], Step [ 13/ 78], Loss: 0.5747\n",
            "Epoch [47/100], Step [ 14/ 78], Loss: 0.5939\n",
            "Epoch [47/100], Step [ 15/ 78], Loss: 0.5626\n",
            "Epoch [47/100], Step [ 16/ 78], Loss: 0.5898\n",
            "Epoch [47/100], Step [ 17/ 78], Loss: 0.5838\n",
            "Epoch [47/100], Step [ 18/ 78], Loss: 0.5771\n",
            "Epoch [47/100], Step [ 19/ 78], Loss: 0.5781\n",
            "Epoch [47/100], Step [ 20/ 78], Loss: 0.5993\n",
            "Epoch [47/100], Step [ 21/ 78], Loss: 0.6118\n",
            "Epoch [47/100], Step [ 22/ 78], Loss: 0.5882\n",
            "Epoch [47/100], Step [ 23/ 78], Loss: 0.6192\n",
            "Epoch [47/100], Step [ 24/ 78], Loss: 0.5615\n",
            "Epoch [47/100], Step [ 25/ 78], Loss: 0.5675\n",
            "Epoch [47/100], Step [ 26/ 78], Loss: 0.5743\n",
            "Epoch [47/100], Step [ 27/ 78], Loss: 0.5839\n",
            "Epoch [47/100], Step [ 28/ 78], Loss: 0.5593\n",
            "Epoch [47/100], Step [ 29/ 78], Loss: 0.5478\n",
            "Epoch [47/100], Step [ 30/ 78], Loss: 0.5865\n",
            "Epoch [47/100], Step [ 31/ 78], Loss: 0.5751\n",
            "Epoch [47/100], Step [ 32/ 78], Loss: 0.6109\n",
            "Epoch [47/100], Step [ 33/ 78], Loss: 0.5703\n",
            "Epoch [47/100], Step [ 34/ 78], Loss: 0.5666\n",
            "Epoch [47/100], Step [ 35/ 78], Loss: 0.5683\n",
            "Epoch [47/100], Step [ 36/ 78], Loss: 0.5438\n",
            "Epoch [47/100], Step [ 37/ 78], Loss: 0.5781\n",
            "Epoch [47/100], Step [ 38/ 78], Loss: 0.5486\n",
            "Epoch [47/100], Step [ 39/ 78], Loss: 0.5770\n",
            "Epoch [47/100], Step [ 40/ 78], Loss: 0.5769\n",
            "Epoch [47/100], Step [ 41/ 78], Loss: 0.5820\n",
            "Epoch [47/100], Step [ 42/ 78], Loss: 0.5950\n",
            "Epoch [47/100], Step [ 43/ 78], Loss: 0.5504\n",
            "Epoch [47/100], Step [ 44/ 78], Loss: 0.5549\n",
            "Epoch [47/100], Step [ 45/ 78], Loss: 0.5552\n",
            "Epoch [47/100], Step [ 46/ 78], Loss: 0.5579\n",
            "Epoch [47/100], Step [ 47/ 78], Loss: 0.5732\n",
            "Epoch [47/100], Step [ 48/ 78], Loss: 0.5718\n",
            "Epoch [47/100], Step [ 49/ 78], Loss: 0.5530\n",
            "Epoch [47/100], Step [ 50/ 78], Loss: 0.5720\n",
            "Epoch [47/100], Step [ 51/ 78], Loss: 0.5587\n",
            "Epoch [47/100], Step [ 52/ 78], Loss: 0.5812\n",
            "Epoch [47/100], Step [ 53/ 78], Loss: 0.6001\n",
            "Epoch [47/100], Step [ 54/ 78], Loss: 0.5821\n",
            "Epoch [47/100], Step [ 55/ 78], Loss: 0.5754\n",
            "Epoch [47/100], Step [ 56/ 78], Loss: 0.5416\n",
            "Epoch [47/100], Step [ 57/ 78], Loss: 0.5589\n",
            "Epoch [47/100], Step [ 58/ 78], Loss: 0.5821\n",
            "Epoch [47/100], Step [ 59/ 78], Loss: 0.5245\n",
            "Epoch [47/100], Step [ 60/ 78], Loss: 0.6060\n",
            "Epoch [47/100], Step [ 61/ 78], Loss: 0.5665\n",
            "Epoch [47/100], Step [ 62/ 78], Loss: 0.6016\n",
            "Epoch [47/100], Step [ 63/ 78], Loss: 0.5522\n",
            "Epoch [48/100], Step [  1/ 78], Loss: 0.5485\n",
            "Epoch [48/100], Step [  2/ 78], Loss: 0.5750\n",
            "Epoch [48/100], Step [  3/ 78], Loss: 0.5450\n",
            "Epoch [48/100], Step [  4/ 78], Loss: 0.5714\n",
            "Epoch [48/100], Step [  5/ 78], Loss: 0.5521\n",
            "Epoch [48/100], Step [  6/ 78], Loss: 0.6167\n",
            "Epoch [48/100], Step [  7/ 78], Loss: 0.5714\n",
            "Epoch [48/100], Step [  8/ 78], Loss: 0.5388\n",
            "Epoch [48/100], Step [  9/ 78], Loss: 0.5423\n",
            "Epoch [48/100], Step [ 10/ 78], Loss: 0.5739\n",
            "Epoch [48/100], Step [ 11/ 78], Loss: 0.5587\n",
            "Epoch [48/100], Step [ 12/ 78], Loss: 0.5641\n",
            "Epoch [48/100], Step [ 13/ 78], Loss: 0.6130\n",
            "Epoch [48/100], Step [ 14/ 78], Loss: 0.6000\n",
            "Epoch [48/100], Step [ 15/ 78], Loss: 0.5943\n",
            "Epoch [48/100], Step [ 16/ 78], Loss: 0.6191\n",
            "Epoch [48/100], Step [ 17/ 78], Loss: 0.5949\n",
            "Epoch [48/100], Step [ 18/ 78], Loss: 0.6184\n",
            "Epoch [48/100], Step [ 19/ 78], Loss: 0.6023\n",
            "Epoch [48/100], Step [ 20/ 78], Loss: 0.5807\n",
            "Epoch [48/100], Step [ 21/ 78], Loss: 0.5533\n",
            "Epoch [48/100], Step [ 22/ 78], Loss: 0.5801\n",
            "Epoch [48/100], Step [ 23/ 78], Loss: 0.5337\n",
            "Epoch [48/100], Step [ 24/ 78], Loss: 0.5962\n",
            "Epoch [48/100], Step [ 25/ 78], Loss: 0.5620\n",
            "Epoch [48/100], Step [ 26/ 78], Loss: 0.5570\n",
            "Epoch [48/100], Step [ 27/ 78], Loss: 0.5957\n",
            "Epoch [48/100], Step [ 28/ 78], Loss: 0.5914\n",
            "Epoch [48/100], Step [ 29/ 78], Loss: 0.5829\n",
            "Epoch [48/100], Step [ 30/ 78], Loss: 0.5565\n",
            "Epoch [48/100], Step [ 31/ 78], Loss: 0.5550\n",
            "Epoch [48/100], Step [ 32/ 78], Loss: 0.5579\n",
            "Epoch [48/100], Step [ 33/ 78], Loss: 0.5707\n",
            "Epoch [48/100], Step [ 34/ 78], Loss: 0.5755\n",
            "Epoch [48/100], Step [ 35/ 78], Loss: 0.5520\n",
            "Epoch [48/100], Step [ 36/ 78], Loss: 0.5739\n",
            "Epoch [48/100], Step [ 37/ 78], Loss: 0.5970\n",
            "Epoch [48/100], Step [ 38/ 78], Loss: 0.5474\n",
            "Epoch [48/100], Step [ 39/ 78], Loss: 0.5271\n",
            "Epoch [48/100], Step [ 40/ 78], Loss: 0.5480\n",
            "Epoch [48/100], Step [ 41/ 78], Loss: 0.5883\n",
            "Epoch [48/100], Step [ 42/ 78], Loss: 0.5815\n",
            "Epoch [48/100], Step [ 43/ 78], Loss: 0.5662\n",
            "Epoch [48/100], Step [ 44/ 78], Loss: 0.5793\n",
            "Epoch [48/100], Step [ 45/ 78], Loss: 0.5552\n",
            "Epoch [48/100], Step [ 46/ 78], Loss: 0.5404\n",
            "Epoch [48/100], Step [ 47/ 78], Loss: 0.5721\n",
            "Epoch [48/100], Step [ 48/ 78], Loss: 0.5633\n",
            "Epoch [48/100], Step [ 49/ 78], Loss: 0.5551\n",
            "Epoch [48/100], Step [ 50/ 78], Loss: 0.5569\n",
            "Epoch [48/100], Step [ 51/ 78], Loss: 0.5678\n",
            "Epoch [48/100], Step [ 52/ 78], Loss: 0.6019\n",
            "Epoch [48/100], Step [ 53/ 78], Loss: 0.5862\n",
            "Epoch [48/100], Step [ 54/ 78], Loss: 0.5713\n",
            "Epoch [48/100], Step [ 55/ 78], Loss: 0.5557\n",
            "Epoch [48/100], Step [ 56/ 78], Loss: 0.5673\n",
            "Epoch [48/100], Step [ 57/ 78], Loss: 0.5900\n",
            "Epoch [48/100], Step [ 58/ 78], Loss: 0.5904\n",
            "Epoch [48/100], Step [ 59/ 78], Loss: 0.5790\n",
            "Epoch [48/100], Step [ 60/ 78], Loss: 0.5973\n",
            "Epoch [48/100], Step [ 61/ 78], Loss: 0.5881\n",
            "Epoch [48/100], Step [ 62/ 78], Loss: 0.5563\n",
            "Epoch [48/100], Step [ 63/ 78], Loss: 0.5647\n",
            "Epoch [49/100], Step [  1/ 78], Loss: 0.5586\n",
            "Epoch [49/100], Step [  2/ 78], Loss: 0.5740\n",
            "Epoch [49/100], Step [  3/ 78], Loss: 0.5637\n",
            "Epoch [49/100], Step [  4/ 78], Loss: 0.5933\n",
            "Epoch [49/100], Step [  5/ 78], Loss: 0.5657\n",
            "Epoch [49/100], Step [  6/ 78], Loss: 0.5645\n",
            "Epoch [49/100], Step [  7/ 78], Loss: 0.5797\n",
            "Epoch [49/100], Step [  8/ 78], Loss: 0.6134\n",
            "Epoch [49/100], Step [  9/ 78], Loss: 0.6225\n",
            "Epoch [49/100], Step [ 10/ 78], Loss: 0.5772\n",
            "Epoch [49/100], Step [ 11/ 78], Loss: 0.5551\n",
            "Epoch [49/100], Step [ 12/ 78], Loss: 0.5784\n",
            "Epoch [49/100], Step [ 13/ 78], Loss: 0.6037\n",
            "Epoch [49/100], Step [ 14/ 78], Loss: 0.5462\n",
            "Epoch [49/100], Step [ 15/ 78], Loss: 0.5423\n",
            "Epoch [49/100], Step [ 16/ 78], Loss: 0.5399\n",
            "Epoch [49/100], Step [ 17/ 78], Loss: 0.5618\n",
            "Epoch [49/100], Step [ 18/ 78], Loss: 0.5865\n",
            "Epoch [49/100], Step [ 19/ 78], Loss: 0.5545\n",
            "Epoch [49/100], Step [ 20/ 78], Loss: 0.5599\n",
            "Epoch [49/100], Step [ 21/ 78], Loss: 0.5587\n",
            "Epoch [49/100], Step [ 22/ 78], Loss: 0.5497\n",
            "Epoch [49/100], Step [ 23/ 78], Loss: 0.5598\n",
            "Epoch [49/100], Step [ 24/ 78], Loss: 0.5636\n",
            "Epoch [49/100], Step [ 25/ 78], Loss: 0.5496\n",
            "Epoch [49/100], Step [ 26/ 78], Loss: 0.5838\n",
            "Epoch [49/100], Step [ 27/ 78], Loss: 0.5793\n",
            "Epoch [49/100], Step [ 28/ 78], Loss: 0.5873\n",
            "Epoch [49/100], Step [ 29/ 78], Loss: 0.6002\n",
            "Epoch [49/100], Step [ 30/ 78], Loss: 0.5841\n",
            "Epoch [49/100], Step [ 31/ 78], Loss: 0.6229\n",
            "Epoch [49/100], Step [ 32/ 78], Loss: 0.5814\n",
            "Epoch [49/100], Step [ 33/ 78], Loss: 0.5423\n",
            "Epoch [49/100], Step [ 34/ 78], Loss: 0.5845\n",
            "Epoch [49/100], Step [ 35/ 78], Loss: 0.5612\n",
            "Epoch [49/100], Step [ 36/ 78], Loss: 0.5669\n",
            "Epoch [49/100], Step [ 37/ 78], Loss: 0.5585\n",
            "Epoch [49/100], Step [ 38/ 78], Loss: 0.5971\n",
            "Epoch [49/100], Step [ 39/ 78], Loss: 0.5433\n",
            "Epoch [49/100], Step [ 40/ 78], Loss: 0.6127\n",
            "Epoch [49/100], Step [ 41/ 78], Loss: 0.5682\n",
            "Epoch [49/100], Step [ 42/ 78], Loss: 0.5716\n",
            "Epoch [49/100], Step [ 43/ 78], Loss: 0.5450\n",
            "Epoch [49/100], Step [ 44/ 78], Loss: 0.6156\n",
            "Epoch [49/100], Step [ 45/ 78], Loss: 0.5255\n",
            "Epoch [49/100], Step [ 46/ 78], Loss: 0.5617\n",
            "Epoch [49/100], Step [ 47/ 78], Loss: 0.5686\n",
            "Epoch [49/100], Step [ 48/ 78], Loss: 0.5738\n",
            "Epoch [49/100], Step [ 49/ 78], Loss: 0.5871\n",
            "Epoch [49/100], Step [ 50/ 78], Loss: 0.5701\n",
            "Epoch [49/100], Step [ 51/ 78], Loss: 0.5463\n",
            "Epoch [49/100], Step [ 52/ 78], Loss: 0.5564\n",
            "Epoch [49/100], Step [ 53/ 78], Loss: 0.5559\n",
            "Epoch [49/100], Step [ 54/ 78], Loss: 0.5900\n",
            "Epoch [49/100], Step [ 55/ 78], Loss: 0.5659\n",
            "Epoch [49/100], Step [ 56/ 78], Loss: 0.5782\n",
            "Epoch [49/100], Step [ 57/ 78], Loss: 0.5462\n",
            "Epoch [49/100], Step [ 58/ 78], Loss: 0.5615\n",
            "Epoch [49/100], Step [ 59/ 78], Loss: 0.5429\n",
            "Epoch [49/100], Step [ 60/ 78], Loss: 0.5664\n",
            "Epoch [49/100], Step [ 61/ 78], Loss: 0.5508\n",
            "Epoch [49/100], Step [ 62/ 78], Loss: 0.5821\n",
            "Epoch [49/100], Step [ 63/ 78], Loss: 0.5855\n",
            "Epoch [50/100], Step [  1/ 78], Loss: 0.5830\n",
            "Epoch [50/100], Step [  2/ 78], Loss: 0.5782\n",
            "Epoch [50/100], Step [  3/ 78], Loss: 0.5691\n",
            "Epoch [50/100], Step [  4/ 78], Loss: 0.5753\n",
            "Epoch [50/100], Step [  5/ 78], Loss: 0.5361\n",
            "Epoch [50/100], Step [  6/ 78], Loss: 0.5380\n",
            "Epoch [50/100], Step [  7/ 78], Loss: 0.5735\n",
            "Epoch [50/100], Step [  8/ 78], Loss: 0.5495\n",
            "Epoch [50/100], Step [  9/ 78], Loss: 0.5362\n",
            "Epoch [50/100], Step [ 10/ 78], Loss: 0.5795\n",
            "Epoch [50/100], Step [ 11/ 78], Loss: 0.5713\n",
            "Epoch [50/100], Step [ 12/ 78], Loss: 0.5587\n",
            "Epoch [50/100], Step [ 13/ 78], Loss: 0.5509\n",
            "Epoch [50/100], Step [ 14/ 78], Loss: 0.5689\n",
            "Epoch [50/100], Step [ 15/ 78], Loss: 0.5621\n",
            "Epoch [50/100], Step [ 16/ 78], Loss: 0.5760\n",
            "Epoch [50/100], Step [ 17/ 78], Loss: 0.5819\n",
            "Epoch [50/100], Step [ 18/ 78], Loss: 0.5875\n",
            "Epoch [50/100], Step [ 19/ 78], Loss: 0.5728\n",
            "Epoch [50/100], Step [ 20/ 78], Loss: 0.5068\n",
            "Epoch [50/100], Step [ 21/ 78], Loss: 0.5819\n",
            "Epoch [50/100], Step [ 22/ 78], Loss: 0.5493\n",
            "Epoch [50/100], Step [ 23/ 78], Loss: 0.5437\n",
            "Epoch [50/100], Step [ 24/ 78], Loss: 0.5597\n",
            "Epoch [50/100], Step [ 25/ 78], Loss: 0.5304\n",
            "Epoch [50/100], Step [ 26/ 78], Loss: 0.5506\n",
            "Epoch [50/100], Step [ 27/ 78], Loss: 0.5886\n",
            "Epoch [50/100], Step [ 28/ 78], Loss: 0.5765\n",
            "Epoch [50/100], Step [ 29/ 78], Loss: 0.5459\n",
            "Epoch [50/100], Step [ 30/ 78], Loss: 0.5176\n",
            "Epoch [50/100], Step [ 31/ 78], Loss: 0.5711\n",
            "Epoch [50/100], Step [ 32/ 78], Loss: 0.5879\n",
            "Epoch [50/100], Step [ 33/ 78], Loss: 0.5760\n",
            "Epoch [50/100], Step [ 34/ 78], Loss: 0.5845\n",
            "Epoch [50/100], Step [ 35/ 78], Loss: 0.5412\n",
            "Epoch [50/100], Step [ 36/ 78], Loss: 0.5534\n",
            "Epoch [50/100], Step [ 37/ 78], Loss: 0.5488\n",
            "Epoch [50/100], Step [ 38/ 78], Loss: 0.5855\n",
            "Epoch [50/100], Step [ 39/ 78], Loss: 0.5669\n",
            "Epoch [50/100], Step [ 40/ 78], Loss: 0.5621\n",
            "Epoch [50/100], Step [ 41/ 78], Loss: 0.5918\n",
            "Epoch [50/100], Step [ 42/ 78], Loss: 0.5584\n",
            "Epoch [50/100], Step [ 43/ 78], Loss: 0.5698\n",
            "Epoch [50/100], Step [ 44/ 78], Loss: 0.5915\n",
            "Epoch [50/100], Step [ 45/ 78], Loss: 0.5545\n",
            "Epoch [50/100], Step [ 46/ 78], Loss: 0.5664\n",
            "Epoch [50/100], Step [ 47/ 78], Loss: 0.5490\n",
            "Epoch [50/100], Step [ 48/ 78], Loss: 0.5390\n",
            "Epoch [50/100], Step [ 49/ 78], Loss: 0.5546\n",
            "Epoch [50/100], Step [ 50/ 78], Loss: 0.5535\n",
            "Epoch [50/100], Step [ 51/ 78], Loss: 0.5830\n",
            "Epoch [50/100], Step [ 52/ 78], Loss: 0.5938\n",
            "Epoch [50/100], Step [ 53/ 78], Loss: 0.5701\n",
            "Epoch [50/100], Step [ 54/ 78], Loss: 0.5737\n",
            "Epoch [50/100], Step [ 55/ 78], Loss: 0.5722\n",
            "Epoch [50/100], Step [ 56/ 78], Loss: 0.5603\n",
            "Epoch [50/100], Step [ 57/ 78], Loss: 0.5694\n",
            "Epoch [50/100], Step [ 58/ 78], Loss: 0.5415\n",
            "Epoch [50/100], Step [ 59/ 78], Loss: 0.5231\n",
            "Epoch [50/100], Step [ 60/ 78], Loss: 0.5826\n",
            "Epoch [50/100], Step [ 61/ 78], Loss: 0.5806\n",
            "Epoch [50/100], Step [ 62/ 78], Loss: 0.5579\n",
            "Epoch [50/100], Step [ 63/ 78], Loss: 0.5921\n",
            "Epoch [51/100], Step [  1/ 78], Loss: 0.5588\n",
            "Epoch [51/100], Step [  2/ 78], Loss: 0.5776\n",
            "Epoch [51/100], Step [  3/ 78], Loss: 0.5535\n",
            "Epoch [51/100], Step [  4/ 78], Loss: 0.5628\n",
            "Epoch [51/100], Step [  5/ 78], Loss: 0.5939\n",
            "Epoch [51/100], Step [  6/ 78], Loss: 0.5705\n",
            "Epoch [51/100], Step [  7/ 78], Loss: 0.5676\n",
            "Epoch [51/100], Step [  8/ 78], Loss: 0.5993\n",
            "Epoch [51/100], Step [  9/ 78], Loss: 0.5508\n",
            "Epoch [51/100], Step [ 10/ 78], Loss: 0.5527\n",
            "Epoch [51/100], Step [ 11/ 78], Loss: 0.5555\n",
            "Epoch [51/100], Step [ 12/ 78], Loss: 0.5697\n",
            "Epoch [51/100], Step [ 13/ 78], Loss: 0.5611\n",
            "Epoch [51/100], Step [ 14/ 78], Loss: 0.5865\n",
            "Epoch [51/100], Step [ 15/ 78], Loss: 0.5740\n",
            "Epoch [51/100], Step [ 16/ 78], Loss: 0.5596\n",
            "Epoch [51/100], Step [ 17/ 78], Loss: 0.5473\n",
            "Epoch [51/100], Step [ 18/ 78], Loss: 0.5381\n",
            "Epoch [51/100], Step [ 19/ 78], Loss: 0.5781\n",
            "Epoch [51/100], Step [ 20/ 78], Loss: 0.5800\n",
            "Epoch [51/100], Step [ 21/ 78], Loss: 0.5584\n",
            "Epoch [51/100], Step [ 22/ 78], Loss: 0.5604\n",
            "Epoch [51/100], Step [ 23/ 78], Loss: 0.5824\n",
            "Epoch [51/100], Step [ 24/ 78], Loss: 0.5413\n",
            "Epoch [51/100], Step [ 25/ 78], Loss: 0.5743\n",
            "Epoch [51/100], Step [ 26/ 78], Loss: 0.5645\n",
            "Epoch [51/100], Step [ 27/ 78], Loss: 0.5379\n",
            "Epoch [51/100], Step [ 28/ 78], Loss: 0.6139\n",
            "Epoch [51/100], Step [ 29/ 78], Loss: 0.5634\n",
            "Epoch [51/100], Step [ 30/ 78], Loss: 0.5430\n",
            "Epoch [51/100], Step [ 31/ 78], Loss: 0.5901\n",
            "Epoch [51/100], Step [ 32/ 78], Loss: 0.5789\n",
            "Epoch [51/100], Step [ 33/ 78], Loss: 0.5304\n",
            "Epoch [51/100], Step [ 34/ 78], Loss: 0.5599\n",
            "Epoch [51/100], Step [ 35/ 78], Loss: 0.5623\n",
            "Epoch [51/100], Step [ 36/ 78], Loss: 0.5754\n",
            "Epoch [51/100], Step [ 37/ 78], Loss: 0.5350\n",
            "Epoch [51/100], Step [ 38/ 78], Loss: 0.5755\n",
            "Epoch [51/100], Step [ 39/ 78], Loss: 0.5678\n",
            "Epoch [51/100], Step [ 40/ 78], Loss: 0.5959\n",
            "Epoch [51/100], Step [ 41/ 78], Loss: 0.5944\n",
            "Epoch [51/100], Step [ 42/ 78], Loss: 0.5679\n",
            "Epoch [51/100], Step [ 43/ 78], Loss: 0.5277\n",
            "Epoch [51/100], Step [ 44/ 78], Loss: 0.5338\n",
            "Epoch [51/100], Step [ 45/ 78], Loss: 0.5738\n",
            "Epoch [51/100], Step [ 46/ 78], Loss: 0.5794\n",
            "Epoch [51/100], Step [ 47/ 78], Loss: 0.5682\n",
            "Epoch [51/100], Step [ 48/ 78], Loss: 0.5515\n",
            "Epoch [51/100], Step [ 49/ 78], Loss: 0.5772\n",
            "Epoch [51/100], Step [ 50/ 78], Loss: 0.5415\n",
            "Epoch [51/100], Step [ 51/ 78], Loss: 0.5924\n",
            "Epoch [51/100], Step [ 52/ 78], Loss: 0.5734\n",
            "Epoch [51/100], Step [ 53/ 78], Loss: 0.5779\n",
            "Epoch [51/100], Step [ 54/ 78], Loss: 0.5737\n",
            "Epoch [51/100], Step [ 55/ 78], Loss: 0.5360\n",
            "Epoch [51/100], Step [ 56/ 78], Loss: 0.5617\n",
            "Epoch [51/100], Step [ 57/ 78], Loss: 0.5085\n",
            "Epoch [51/100], Step [ 58/ 78], Loss: 0.5724\n",
            "Epoch [51/100], Step [ 59/ 78], Loss: 0.5941\n",
            "Epoch [51/100], Step [ 60/ 78], Loss: 0.5618\n",
            "Epoch [51/100], Step [ 61/ 78], Loss: 0.5623\n",
            "Epoch [51/100], Step [ 62/ 78], Loss: 0.5729\n",
            "Epoch [51/100], Step [ 63/ 78], Loss: 0.5753\n",
            "Epoch [52/100], Step [  1/ 78], Loss: 0.5871\n",
            "Epoch [52/100], Step [  2/ 78], Loss: 0.5953\n",
            "Epoch [52/100], Step [  3/ 78], Loss: 0.5769\n",
            "Epoch [52/100], Step [  4/ 78], Loss: 0.5803\n",
            "Epoch [52/100], Step [  5/ 78], Loss: 0.5753\n",
            "Epoch [52/100], Step [  6/ 78], Loss: 0.5780\n",
            "Epoch [52/100], Step [  7/ 78], Loss: 0.5448\n",
            "Epoch [52/100], Step [  8/ 78], Loss: 0.5934\n",
            "Epoch [52/100], Step [  9/ 78], Loss: 0.5636\n",
            "Epoch [52/100], Step [ 10/ 78], Loss: 0.5437\n",
            "Epoch [52/100], Step [ 11/ 78], Loss: 0.5679\n",
            "Epoch [52/100], Step [ 12/ 78], Loss: 0.5822\n",
            "Epoch [52/100], Step [ 13/ 78], Loss: 0.5337\n",
            "Epoch [52/100], Step [ 14/ 78], Loss: 0.5735\n",
            "Epoch [52/100], Step [ 15/ 78], Loss: 0.5558\n",
            "Epoch [52/100], Step [ 16/ 78], Loss: 0.5729\n",
            "Epoch [52/100], Step [ 17/ 78], Loss: 0.5749\n",
            "Epoch [52/100], Step [ 18/ 78], Loss: 0.5596\n",
            "Epoch [52/100], Step [ 19/ 78], Loss: 0.5376\n",
            "Epoch [52/100], Step [ 20/ 78], Loss: 0.5652\n",
            "Epoch [52/100], Step [ 21/ 78], Loss: 0.5862\n",
            "Epoch [52/100], Step [ 22/ 78], Loss: 0.5775\n",
            "Epoch [52/100], Step [ 23/ 78], Loss: 0.5439\n",
            "Epoch [52/100], Step [ 24/ 78], Loss: 0.5664\n",
            "Epoch [52/100], Step [ 25/ 78], Loss: 0.5743\n",
            "Epoch [52/100], Step [ 26/ 78], Loss: 0.5333\n",
            "Epoch [52/100], Step [ 27/ 78], Loss: 0.5858\n",
            "Epoch [52/100], Step [ 28/ 78], Loss: 0.5324\n",
            "Epoch [52/100], Step [ 29/ 78], Loss: 0.5975\n",
            "Epoch [52/100], Step [ 30/ 78], Loss: 0.5486\n",
            "Epoch [52/100], Step [ 31/ 78], Loss: 0.5379\n",
            "Epoch [52/100], Step [ 32/ 78], Loss: 0.5394\n",
            "Epoch [52/100], Step [ 33/ 78], Loss: 0.5755\n",
            "Epoch [52/100], Step [ 34/ 78], Loss: 0.5472\n",
            "Epoch [52/100], Step [ 35/ 78], Loss: 0.5769\n",
            "Epoch [52/100], Step [ 36/ 78], Loss: 0.5506\n",
            "Epoch [52/100], Step [ 37/ 78], Loss: 0.5648\n",
            "Epoch [52/100], Step [ 38/ 78], Loss: 0.5704\n",
            "Epoch [52/100], Step [ 39/ 78], Loss: 0.5689\n",
            "Epoch [52/100], Step [ 40/ 78], Loss: 0.5956\n",
            "Epoch [52/100], Step [ 41/ 78], Loss: 0.6015\n",
            "Epoch [52/100], Step [ 42/ 78], Loss: 0.5803\n",
            "Epoch [52/100], Step [ 43/ 78], Loss: 0.5720\n",
            "Epoch [52/100], Step [ 44/ 78], Loss: 0.5915\n",
            "Epoch [52/100], Step [ 45/ 78], Loss: 0.6029\n",
            "Epoch [52/100], Step [ 46/ 78], Loss: 0.5978\n",
            "Epoch [52/100], Step [ 47/ 78], Loss: 0.5548\n",
            "Epoch [52/100], Step [ 48/ 78], Loss: 0.5624\n",
            "Epoch [52/100], Step [ 49/ 78], Loss: 0.5774\n",
            "Epoch [52/100], Step [ 50/ 78], Loss: 0.5357\n",
            "Epoch [52/100], Step [ 51/ 78], Loss: 0.5935\n",
            "Epoch [52/100], Step [ 52/ 78], Loss: 0.5778\n",
            "Epoch [52/100], Step [ 53/ 78], Loss: 0.5536\n",
            "Epoch [52/100], Step [ 54/ 78], Loss: 0.5245\n",
            "Epoch [52/100], Step [ 55/ 78], Loss: 0.5465\n",
            "Epoch [52/100], Step [ 56/ 78], Loss: 0.5881\n",
            "Epoch [52/100], Step [ 57/ 78], Loss: 0.5815\n",
            "Epoch [52/100], Step [ 58/ 78], Loss: 0.5792\n",
            "Epoch [52/100], Step [ 59/ 78], Loss: 0.5915\n",
            "Epoch [52/100], Step [ 60/ 78], Loss: 0.5490\n",
            "Epoch [52/100], Step [ 61/ 78], Loss: 0.5381\n",
            "Epoch [52/100], Step [ 62/ 78], Loss: 0.5185\n",
            "Epoch [52/100], Step [ 63/ 78], Loss: 0.5952\n",
            "Epoch [53/100], Step [  1/ 78], Loss: 0.5639\n",
            "Epoch [53/100], Step [  2/ 78], Loss: 0.5599\n",
            "Epoch [53/100], Step [  3/ 78], Loss: 0.5470\n",
            "Epoch [53/100], Step [  4/ 78], Loss: 0.5648\n",
            "Epoch [53/100], Step [  5/ 78], Loss: 0.5251\n",
            "Epoch [53/100], Step [  6/ 78], Loss: 0.5806\n",
            "Epoch [53/100], Step [  7/ 78], Loss: 0.5801\n",
            "Epoch [53/100], Step [  8/ 78], Loss: 0.5587\n",
            "Epoch [53/100], Step [  9/ 78], Loss: 0.5615\n",
            "Epoch [53/100], Step [ 10/ 78], Loss: 0.5771\n",
            "Epoch [53/100], Step [ 11/ 78], Loss: 0.5620\n",
            "Epoch [53/100], Step [ 12/ 78], Loss: 0.5846\n",
            "Epoch [53/100], Step [ 13/ 78], Loss: 0.5996\n",
            "Epoch [53/100], Step [ 14/ 78], Loss: 0.5748\n",
            "Epoch [53/100], Step [ 15/ 78], Loss: 0.5399\n",
            "Epoch [53/100], Step [ 16/ 78], Loss: 0.5703\n",
            "Epoch [53/100], Step [ 17/ 78], Loss: 0.5576\n",
            "Epoch [53/100], Step [ 18/ 78], Loss: 0.5835\n",
            "Epoch [53/100], Step [ 19/ 78], Loss: 0.5394\n",
            "Epoch [53/100], Step [ 20/ 78], Loss: 0.5864\n",
            "Epoch [53/100], Step [ 21/ 78], Loss: 0.5384\n",
            "Epoch [53/100], Step [ 22/ 78], Loss: 0.5686\n",
            "Epoch [53/100], Step [ 23/ 78], Loss: 0.5787\n",
            "Epoch [53/100], Step [ 24/ 78], Loss: 0.5673\n",
            "Epoch [53/100], Step [ 25/ 78], Loss: 0.5531\n",
            "Epoch [53/100], Step [ 26/ 78], Loss: 0.5493\n",
            "Epoch [53/100], Step [ 27/ 78], Loss: 0.5747\n",
            "Epoch [53/100], Step [ 28/ 78], Loss: 0.5487\n",
            "Epoch [53/100], Step [ 29/ 78], Loss: 0.5485\n",
            "Epoch [53/100], Step [ 30/ 78], Loss: 0.5420\n",
            "Epoch [53/100], Step [ 31/ 78], Loss: 0.5630\n",
            "Epoch [53/100], Step [ 32/ 78], Loss: 0.5419\n",
            "Epoch [53/100], Step [ 33/ 78], Loss: 0.5671\n",
            "Epoch [53/100], Step [ 34/ 78], Loss: 0.5486\n",
            "Epoch [53/100], Step [ 35/ 78], Loss: 0.5533\n",
            "Epoch [53/100], Step [ 36/ 78], Loss: 0.5574\n",
            "Epoch [53/100], Step [ 37/ 78], Loss: 0.5522\n",
            "Epoch [53/100], Step [ 38/ 78], Loss: 0.5800\n",
            "Epoch [53/100], Step [ 39/ 78], Loss: 0.5202\n",
            "Epoch [53/100], Step [ 40/ 78], Loss: 0.5683\n",
            "Epoch [53/100], Step [ 41/ 78], Loss: 0.5818\n",
            "Epoch [53/100], Step [ 42/ 78], Loss: 0.5647\n",
            "Epoch [53/100], Step [ 43/ 78], Loss: 0.5488\n",
            "Epoch [53/100], Step [ 44/ 78], Loss: 0.5477\n",
            "Epoch [53/100], Step [ 45/ 78], Loss: 0.5635\n",
            "Epoch [53/100], Step [ 46/ 78], Loss: 0.5860\n",
            "Epoch [53/100], Step [ 47/ 78], Loss: 0.5922\n",
            "Epoch [53/100], Step [ 48/ 78], Loss: 0.5451\n",
            "Epoch [53/100], Step [ 49/ 78], Loss: 0.5358\n",
            "Epoch [53/100], Step [ 50/ 78], Loss: 0.5054\n",
            "Epoch [53/100], Step [ 51/ 78], Loss: 0.5405\n",
            "Epoch [53/100], Step [ 52/ 78], Loss: 0.5458\n",
            "Epoch [53/100], Step [ 53/ 78], Loss: 0.5716\n",
            "Epoch [53/100], Step [ 54/ 78], Loss: 0.5169\n",
            "Epoch [53/100], Step [ 55/ 78], Loss: 0.5859\n",
            "Epoch [53/100], Step [ 56/ 78], Loss: 0.5800\n",
            "Epoch [53/100], Step [ 57/ 78], Loss: 0.5700\n",
            "Epoch [53/100], Step [ 58/ 78], Loss: 0.5414\n",
            "Epoch [53/100], Step [ 59/ 78], Loss: 0.5778\n",
            "Epoch [53/100], Step [ 60/ 78], Loss: 0.5692\n",
            "Epoch [53/100], Step [ 61/ 78], Loss: 0.5912\n",
            "Epoch [53/100], Step [ 62/ 78], Loss: 0.5930\n",
            "Epoch [53/100], Step [ 63/ 78], Loss: 0.6351\n",
            "Epoch [54/100], Step [  1/ 78], Loss: 0.5890\n",
            "Epoch [54/100], Step [  2/ 78], Loss: 0.5675\n",
            "Epoch [54/100], Step [  3/ 78], Loss: 0.5938\n",
            "Epoch [54/100], Step [  4/ 78], Loss: 0.5827\n",
            "Epoch [54/100], Step [  5/ 78], Loss: 0.5610\n",
            "Epoch [54/100], Step [  6/ 78], Loss: 0.5816\n",
            "Epoch [54/100], Step [  7/ 78], Loss: 0.5819\n",
            "Epoch [54/100], Step [  8/ 78], Loss: 0.5658\n",
            "Epoch [54/100], Step [  9/ 78], Loss: 0.5599\n",
            "Epoch [54/100], Step [ 10/ 78], Loss: 0.5627\n",
            "Epoch [54/100], Step [ 11/ 78], Loss: 0.5483\n",
            "Epoch [54/100], Step [ 12/ 78], Loss: 0.5569\n",
            "Epoch [54/100], Step [ 13/ 78], Loss: 0.5804\n",
            "Epoch [54/100], Step [ 14/ 78], Loss: 0.5202\n",
            "Epoch [54/100], Step [ 15/ 78], Loss: 0.5428\n",
            "Epoch [54/100], Step [ 16/ 78], Loss: 0.5459\n",
            "Epoch [54/100], Step [ 17/ 78], Loss: 0.5759\n",
            "Epoch [54/100], Step [ 18/ 78], Loss: 0.5952\n",
            "Epoch [54/100], Step [ 19/ 78], Loss: 0.5539\n",
            "Epoch [54/100], Step [ 20/ 78], Loss: 0.5566\n",
            "Epoch [54/100], Step [ 21/ 78], Loss: 0.5861\n",
            "Epoch [54/100], Step [ 22/ 78], Loss: 0.6213\n",
            "Epoch [54/100], Step [ 23/ 78], Loss: 0.6100\n",
            "Epoch [54/100], Step [ 24/ 78], Loss: 0.5496\n",
            "Epoch [54/100], Step [ 25/ 78], Loss: 0.5889\n",
            "Epoch [54/100], Step [ 26/ 78], Loss: 0.5676\n",
            "Epoch [54/100], Step [ 27/ 78], Loss: 0.5450\n",
            "Epoch [54/100], Step [ 28/ 78], Loss: 0.5715\n",
            "Epoch [54/100], Step [ 29/ 78], Loss: 0.5639\n",
            "Epoch [54/100], Step [ 30/ 78], Loss: 0.5834\n",
            "Epoch [54/100], Step [ 31/ 78], Loss: 0.5702\n",
            "Epoch [54/100], Step [ 32/ 78], Loss: 0.5741\n",
            "Epoch [54/100], Step [ 33/ 78], Loss: 0.5584\n",
            "Epoch [54/100], Step [ 34/ 78], Loss: 0.5936\n",
            "Epoch [54/100], Step [ 35/ 78], Loss: 0.5677\n",
            "Epoch [54/100], Step [ 36/ 78], Loss: 0.6039\n",
            "Epoch [54/100], Step [ 37/ 78], Loss: 0.6177\n",
            "Epoch [54/100], Step [ 38/ 78], Loss: 0.5617\n",
            "Epoch [54/100], Step [ 39/ 78], Loss: 0.5279\n",
            "Epoch [54/100], Step [ 40/ 78], Loss: 0.5647\n",
            "Epoch [54/100], Step [ 41/ 78], Loss: 0.5710\n",
            "Epoch [54/100], Step [ 42/ 78], Loss: 0.5510\n",
            "Epoch [54/100], Step [ 43/ 78], Loss: 0.5475\n",
            "Epoch [54/100], Step [ 44/ 78], Loss: 0.5779\n",
            "Epoch [54/100], Step [ 45/ 78], Loss: 0.5665\n",
            "Epoch [54/100], Step [ 46/ 78], Loss: 0.6096\n",
            "Epoch [54/100], Step [ 47/ 78], Loss: 0.5807\n",
            "Epoch [54/100], Step [ 48/ 78], Loss: 0.5511\n",
            "Epoch [54/100], Step [ 49/ 78], Loss: 0.5532\n",
            "Epoch [54/100], Step [ 50/ 78], Loss: 0.5607\n",
            "Epoch [54/100], Step [ 51/ 78], Loss: 0.5426\n",
            "Epoch [54/100], Step [ 52/ 78], Loss: 0.5299\n",
            "Epoch [54/100], Step [ 53/ 78], Loss: 0.5349\n",
            "Epoch [54/100], Step [ 54/ 78], Loss: 0.5191\n",
            "Epoch [54/100], Step [ 55/ 78], Loss: 0.5270\n",
            "Epoch [54/100], Step [ 56/ 78], Loss: 0.5538\n",
            "Epoch [54/100], Step [ 57/ 78], Loss: 0.5689\n",
            "Epoch [54/100], Step [ 58/ 78], Loss: 0.5805\n",
            "Epoch [54/100], Step [ 59/ 78], Loss: 0.5885\n",
            "Epoch [54/100], Step [ 60/ 78], Loss: 0.5635\n",
            "Epoch [54/100], Step [ 61/ 78], Loss: 0.5517\n",
            "Epoch [54/100], Step [ 62/ 78], Loss: 0.5109\n",
            "Epoch [54/100], Step [ 63/ 78], Loss: 0.5763\n",
            "Epoch [55/100], Step [  1/ 78], Loss: 0.5587\n",
            "Epoch [55/100], Step [  2/ 78], Loss: 0.5845\n",
            "Epoch [55/100], Step [  3/ 78], Loss: 0.5475\n",
            "Epoch [55/100], Step [  4/ 78], Loss: 0.5483\n",
            "Epoch [55/100], Step [  5/ 78], Loss: 0.5444\n",
            "Epoch [55/100], Step [  6/ 78], Loss: 0.5237\n",
            "Epoch [55/100], Step [  7/ 78], Loss: 0.5694\n",
            "Epoch [55/100], Step [  8/ 78], Loss: 0.5797\n",
            "Epoch [55/100], Step [  9/ 78], Loss: 0.5264\n",
            "Epoch [55/100], Step [ 10/ 78], Loss: 0.5513\n",
            "Epoch [55/100], Step [ 11/ 78], Loss: 0.5547\n",
            "Epoch [55/100], Step [ 12/ 78], Loss: 0.5268\n",
            "Epoch [55/100], Step [ 13/ 78], Loss: 0.5351\n",
            "Epoch [55/100], Step [ 14/ 78], Loss: 0.5166\n",
            "Epoch [55/100], Step [ 15/ 78], Loss: 0.5646\n",
            "Epoch [55/100], Step [ 16/ 78], Loss: 0.5629\n",
            "Epoch [55/100], Step [ 17/ 78], Loss: 0.5451\n",
            "Epoch [55/100], Step [ 18/ 78], Loss: 0.5565\n",
            "Epoch [55/100], Step [ 19/ 78], Loss: 0.5299\n",
            "Epoch [55/100], Step [ 20/ 78], Loss: 0.5373\n",
            "Epoch [55/100], Step [ 21/ 78], Loss: 0.5389\n",
            "Epoch [55/100], Step [ 22/ 78], Loss: 0.5662\n",
            "Epoch [55/100], Step [ 23/ 78], Loss: 0.5639\n",
            "Epoch [55/100], Step [ 24/ 78], Loss: 0.5820\n",
            "Epoch [55/100], Step [ 25/ 78], Loss: 0.5332\n",
            "Epoch [55/100], Step [ 26/ 78], Loss: 0.5440\n",
            "Epoch [55/100], Step [ 27/ 78], Loss: 0.5659\n",
            "Epoch [55/100], Step [ 28/ 78], Loss: 0.5521\n",
            "Epoch [55/100], Step [ 29/ 78], Loss: 0.5882\n",
            "Epoch [55/100], Step [ 30/ 78], Loss: 0.5844\n",
            "Epoch [55/100], Step [ 31/ 78], Loss: 0.5736\n",
            "Epoch [55/100], Step [ 32/ 78], Loss: 0.5811\n",
            "Epoch [55/100], Step [ 33/ 78], Loss: 0.5860\n",
            "Epoch [55/100], Step [ 34/ 78], Loss: 0.5960\n",
            "Epoch [55/100], Step [ 35/ 78], Loss: 0.5810\n",
            "Epoch [55/100], Step [ 36/ 78], Loss: 0.5593\n",
            "Epoch [55/100], Step [ 37/ 78], Loss: 0.5505\n",
            "Epoch [55/100], Step [ 38/ 78], Loss: 0.5477\n",
            "Epoch [55/100], Step [ 39/ 78], Loss: 0.5698\n",
            "Epoch [55/100], Step [ 40/ 78], Loss: 0.5824\n",
            "Epoch [55/100], Step [ 41/ 78], Loss: 0.5410\n",
            "Epoch [55/100], Step [ 42/ 78], Loss: 0.5666\n",
            "Epoch [55/100], Step [ 43/ 78], Loss: 0.5221\n",
            "Epoch [55/100], Step [ 44/ 78], Loss: 0.5411\n",
            "Epoch [55/100], Step [ 45/ 78], Loss: 0.5626\n",
            "Epoch [55/100], Step [ 46/ 78], Loss: 0.5664\n",
            "Epoch [55/100], Step [ 47/ 78], Loss: 0.5475\n",
            "Epoch [55/100], Step [ 48/ 78], Loss: 0.5577\n",
            "Epoch [55/100], Step [ 49/ 78], Loss: 0.5762\n",
            "Epoch [55/100], Step [ 50/ 78], Loss: 0.5640\n",
            "Epoch [55/100], Step [ 51/ 78], Loss: 0.5923\n",
            "Epoch [55/100], Step [ 52/ 78], Loss: 0.5639\n",
            "Epoch [55/100], Step [ 53/ 78], Loss: 0.5841\n",
            "Epoch [55/100], Step [ 54/ 78], Loss: 0.5512\n",
            "Epoch [55/100], Step [ 55/ 78], Loss: 0.5469\n",
            "Epoch [55/100], Step [ 56/ 78], Loss: 0.5867\n",
            "Epoch [55/100], Step [ 57/ 78], Loss: 0.5768\n",
            "Epoch [55/100], Step [ 58/ 78], Loss: 0.5567\n",
            "Epoch [55/100], Step [ 59/ 78], Loss: 0.5307\n",
            "Epoch [55/100], Step [ 60/ 78], Loss: 0.5474\n",
            "Epoch [55/100], Step [ 61/ 78], Loss: 0.5414\n",
            "Epoch [55/100], Step [ 62/ 78], Loss: 0.5794\n",
            "Epoch [55/100], Step [ 63/ 78], Loss: 0.6195\n",
            "Epoch [56/100], Step [  1/ 78], Loss: 0.5700\n",
            "Epoch [56/100], Step [  2/ 78], Loss: 0.5749\n",
            "Epoch [56/100], Step [  3/ 78], Loss: 0.5863\n",
            "Epoch [56/100], Step [  4/ 78], Loss: 0.5772\n",
            "Epoch [56/100], Step [  5/ 78], Loss: 0.5655\n",
            "Epoch [56/100], Step [  6/ 78], Loss: 0.5752\n",
            "Epoch [56/100], Step [  7/ 78], Loss: 0.5463\n",
            "Epoch [56/100], Step [  8/ 78], Loss: 0.5365\n",
            "Epoch [56/100], Step [  9/ 78], Loss: 0.5514\n",
            "Epoch [56/100], Step [ 10/ 78], Loss: 0.5565\n",
            "Epoch [56/100], Step [ 11/ 78], Loss: 0.5634\n",
            "Epoch [56/100], Step [ 12/ 78], Loss: 0.5905\n",
            "Epoch [56/100], Step [ 13/ 78], Loss: 0.5667\n",
            "Epoch [56/100], Step [ 14/ 78], Loss: 0.5514\n",
            "Epoch [56/100], Step [ 15/ 78], Loss: 0.5739\n",
            "Epoch [56/100], Step [ 16/ 78], Loss: 0.5491\n",
            "Epoch [56/100], Step [ 17/ 78], Loss: 0.5776\n",
            "Epoch [56/100], Step [ 18/ 78], Loss: 0.5742\n",
            "Epoch [56/100], Step [ 19/ 78], Loss: 0.5206\n",
            "Epoch [56/100], Step [ 20/ 78], Loss: 0.5379\n",
            "Epoch [56/100], Step [ 21/ 78], Loss: 0.5356\n",
            "Epoch [56/100], Step [ 22/ 78], Loss: 0.5225\n",
            "Epoch [56/100], Step [ 23/ 78], Loss: 0.5376\n",
            "Epoch [56/100], Step [ 24/ 78], Loss: 0.5438\n",
            "Epoch [56/100], Step [ 25/ 78], Loss: 0.5510\n",
            "Epoch [56/100], Step [ 26/ 78], Loss: 0.5727\n",
            "Epoch [56/100], Step [ 27/ 78], Loss: 0.5545\n",
            "Epoch [56/100], Step [ 28/ 78], Loss: 0.5617\n",
            "Epoch [56/100], Step [ 29/ 78], Loss: 0.5592\n",
            "Epoch [56/100], Step [ 30/ 78], Loss: 0.5557\n",
            "Epoch [56/100], Step [ 31/ 78], Loss: 0.5173\n",
            "Epoch [56/100], Step [ 32/ 78], Loss: 0.5594\n",
            "Epoch [56/100], Step [ 33/ 78], Loss: 0.5316\n",
            "Epoch [56/100], Step [ 34/ 78], Loss: 0.5755\n",
            "Epoch [56/100], Step [ 35/ 78], Loss: 0.5469\n",
            "Epoch [56/100], Step [ 36/ 78], Loss: 0.5634\n",
            "Epoch [56/100], Step [ 37/ 78], Loss: 0.6152\n",
            "Epoch [56/100], Step [ 38/ 78], Loss: 0.5901\n",
            "Epoch [56/100], Step [ 39/ 78], Loss: 0.5783\n",
            "Epoch [56/100], Step [ 40/ 78], Loss: 0.5426\n",
            "Epoch [56/100], Step [ 41/ 78], Loss: 0.5613\n",
            "Epoch [56/100], Step [ 42/ 78], Loss: 0.5599\n",
            "Epoch [56/100], Step [ 43/ 78], Loss: 0.5463\n",
            "Epoch [56/100], Step [ 44/ 78], Loss: 0.5615\n",
            "Epoch [56/100], Step [ 45/ 78], Loss: 0.5925\n",
            "Epoch [56/100], Step [ 46/ 78], Loss: 0.5508\n",
            "Epoch [56/100], Step [ 47/ 78], Loss: 0.5557\n",
            "Epoch [56/100], Step [ 48/ 78], Loss: 0.6233\n",
            "Epoch [56/100], Step [ 49/ 78], Loss: 0.6083\n",
            "Epoch [56/100], Step [ 50/ 78], Loss: 0.5316\n",
            "Epoch [56/100], Step [ 51/ 78], Loss: 0.5548\n",
            "Epoch [56/100], Step [ 52/ 78], Loss: 0.5417\n",
            "Epoch [56/100], Step [ 53/ 78], Loss: 0.5635\n",
            "Epoch [56/100], Step [ 54/ 78], Loss: 0.5820\n",
            "Epoch [56/100], Step [ 55/ 78], Loss: 0.5644\n",
            "Epoch [56/100], Step [ 56/ 78], Loss: 0.5905\n",
            "Epoch [56/100], Step [ 57/ 78], Loss: 0.5276\n",
            "Epoch [56/100], Step [ 58/ 78], Loss: 0.5556\n",
            "Epoch [56/100], Step [ 59/ 78], Loss: 0.5417\n",
            "Epoch [56/100], Step [ 60/ 78], Loss: 0.5442\n",
            "Epoch [56/100], Step [ 61/ 78], Loss: 0.5824\n",
            "Epoch [56/100], Step [ 62/ 78], Loss: 0.5482\n",
            "Epoch [56/100], Step [ 63/ 78], Loss: 0.5963\n",
            "Epoch [57/100], Step [  1/ 78], Loss: 0.5678\n",
            "Epoch [57/100], Step [  2/ 78], Loss: 0.5284\n",
            "Epoch [57/100], Step [  3/ 78], Loss: 0.5606\n",
            "Epoch [57/100], Step [  4/ 78], Loss: 0.5670\n",
            "Epoch [57/100], Step [  5/ 78], Loss: 0.5400\n",
            "Epoch [57/100], Step [  6/ 78], Loss: 0.5753\n",
            "Epoch [57/100], Step [  7/ 78], Loss: 0.5583\n",
            "Epoch [57/100], Step [  8/ 78], Loss: 0.5634\n",
            "Epoch [57/100], Step [  9/ 78], Loss: 0.5491\n",
            "Epoch [57/100], Step [ 10/ 78], Loss: 0.5895\n",
            "Epoch [57/100], Step [ 11/ 78], Loss: 0.5663\n",
            "Epoch [57/100], Step [ 12/ 78], Loss: 0.5641\n",
            "Epoch [57/100], Step [ 13/ 78], Loss: 0.5590\n",
            "Epoch [57/100], Step [ 14/ 78], Loss: 0.5633\n",
            "Epoch [57/100], Step [ 15/ 78], Loss: 0.5542\n",
            "Epoch [57/100], Step [ 16/ 78], Loss: 0.5202\n",
            "Epoch [57/100], Step [ 17/ 78], Loss: 0.5807\n",
            "Epoch [57/100], Step [ 18/ 78], Loss: 0.5684\n",
            "Epoch [57/100], Step [ 19/ 78], Loss: 0.5584\n",
            "Epoch [57/100], Step [ 20/ 78], Loss: 0.5502\n",
            "Epoch [57/100], Step [ 21/ 78], Loss: 0.5307\n",
            "Epoch [57/100], Step [ 22/ 78], Loss: 0.5648\n",
            "Epoch [57/100], Step [ 23/ 78], Loss: 0.5585\n",
            "Epoch [57/100], Step [ 24/ 78], Loss: 0.5950\n",
            "Epoch [57/100], Step [ 25/ 78], Loss: 0.5644\n",
            "Epoch [57/100], Step [ 26/ 78], Loss: 0.5593\n",
            "Epoch [57/100], Step [ 27/ 78], Loss: 0.5419\n",
            "Epoch [57/100], Step [ 28/ 78], Loss: 0.5828\n",
            "Epoch [57/100], Step [ 29/ 78], Loss: 0.6250\n",
            "Epoch [57/100], Step [ 30/ 78], Loss: 0.5643\n",
            "Epoch [57/100], Step [ 31/ 78], Loss: 0.5913\n",
            "Epoch [57/100], Step [ 32/ 78], Loss: 0.5843\n",
            "Epoch [57/100], Step [ 33/ 78], Loss: 0.5205\n",
            "Epoch [57/100], Step [ 34/ 78], Loss: 0.5767\n",
            "Epoch [57/100], Step [ 35/ 78], Loss: 0.5674\n",
            "Epoch [57/100], Step [ 36/ 78], Loss: 0.5843\n",
            "Epoch [57/100], Step [ 37/ 78], Loss: 0.5379\n",
            "Epoch [57/100], Step [ 38/ 78], Loss: 0.5226\n",
            "Epoch [57/100], Step [ 39/ 78], Loss: 0.5374\n",
            "Epoch [57/100], Step [ 40/ 78], Loss: 0.5554\n",
            "Epoch [57/100], Step [ 41/ 78], Loss: 0.5572\n",
            "Epoch [57/100], Step [ 42/ 78], Loss: 0.6152\n",
            "Epoch [57/100], Step [ 43/ 78], Loss: 0.5475\n",
            "Epoch [57/100], Step [ 44/ 78], Loss: 0.5518\n",
            "Epoch [57/100], Step [ 45/ 78], Loss: 0.5739\n",
            "Epoch [57/100], Step [ 46/ 78], Loss: 0.5494\n",
            "Epoch [57/100], Step [ 47/ 78], Loss: 0.5529\n",
            "Epoch [57/100], Step [ 48/ 78], Loss: 0.5602\n",
            "Epoch [57/100], Step [ 49/ 78], Loss: 0.5447\n",
            "Epoch [57/100], Step [ 50/ 78], Loss: 0.5586\n",
            "Epoch [57/100], Step [ 51/ 78], Loss: 0.5387\n",
            "Epoch [57/100], Step [ 52/ 78], Loss: 0.5413\n",
            "Epoch [57/100], Step [ 53/ 78], Loss: 0.5609\n",
            "Epoch [57/100], Step [ 54/ 78], Loss: 0.5952\n",
            "Epoch [57/100], Step [ 55/ 78], Loss: 0.5163\n",
            "Epoch [57/100], Step [ 56/ 78], Loss: 0.5270\n",
            "Epoch [57/100], Step [ 57/ 78], Loss: 0.5780\n",
            "Epoch [57/100], Step [ 58/ 78], Loss: 0.5663\n",
            "Epoch [57/100], Step [ 59/ 78], Loss: 0.5538\n",
            "Epoch [57/100], Step [ 60/ 78], Loss: 0.5661\n",
            "Epoch [57/100], Step [ 61/ 78], Loss: 0.5868\n",
            "Epoch [57/100], Step [ 62/ 78], Loss: 0.5640\n",
            "Epoch [57/100], Step [ 63/ 78], Loss: 0.5083\n",
            "Epoch [58/100], Step [  1/ 78], Loss: 0.5690\n",
            "Epoch [58/100], Step [  2/ 78], Loss: 0.5549\n",
            "Epoch [58/100], Step [  3/ 78], Loss: 0.5369\n",
            "Epoch [58/100], Step [  4/ 78], Loss: 0.5590\n",
            "Epoch [58/100], Step [  5/ 78], Loss: 0.5723\n",
            "Epoch [58/100], Step [  6/ 78], Loss: 0.5498\n",
            "Epoch [58/100], Step [  7/ 78], Loss: 0.5554\n",
            "Epoch [58/100], Step [  8/ 78], Loss: 0.5663\n",
            "Epoch [58/100], Step [  9/ 78], Loss: 0.5500\n",
            "Epoch [58/100], Step [ 10/ 78], Loss: 0.5621\n",
            "Epoch [58/100], Step [ 11/ 78], Loss: 0.5624\n",
            "Epoch [58/100], Step [ 12/ 78], Loss: 0.5714\n",
            "Epoch [58/100], Step [ 13/ 78], Loss: 0.5195\n",
            "Epoch [58/100], Step [ 14/ 78], Loss: 0.5507\n",
            "Epoch [58/100], Step [ 15/ 78], Loss: 0.5638\n",
            "Epoch [58/100], Step [ 16/ 78], Loss: 0.5576\n",
            "Epoch [58/100], Step [ 17/ 78], Loss: 0.5484\n",
            "Epoch [58/100], Step [ 18/ 78], Loss: 0.5720\n",
            "Epoch [58/100], Step [ 19/ 78], Loss: 0.5626\n",
            "Epoch [58/100], Step [ 20/ 78], Loss: 0.5909\n",
            "Epoch [58/100], Step [ 21/ 78], Loss: 0.5740\n",
            "Epoch [58/100], Step [ 22/ 78], Loss: 0.5751\n",
            "Epoch [58/100], Step [ 23/ 78], Loss: 0.5491\n",
            "Epoch [58/100], Step [ 24/ 78], Loss: 0.5591\n",
            "Epoch [58/100], Step [ 25/ 78], Loss: 0.5191\n",
            "Epoch [58/100], Step [ 26/ 78], Loss: 0.5487\n",
            "Epoch [58/100], Step [ 27/ 78], Loss: 0.5514\n",
            "Epoch [58/100], Step [ 28/ 78], Loss: 0.5261\n",
            "Epoch [58/100], Step [ 29/ 78], Loss: 0.5310\n",
            "Epoch [58/100], Step [ 30/ 78], Loss: 0.5695\n",
            "Epoch [58/100], Step [ 31/ 78], Loss: 0.5818\n",
            "Epoch [58/100], Step [ 32/ 78], Loss: 0.5526\n",
            "Epoch [58/100], Step [ 33/ 78], Loss: 0.5013\n",
            "Epoch [58/100], Step [ 34/ 78], Loss: 0.5920\n",
            "Epoch [58/100], Step [ 35/ 78], Loss: 0.5668\n",
            "Epoch [58/100], Step [ 36/ 78], Loss: 0.5521\n",
            "Epoch [58/100], Step [ 37/ 78], Loss: 0.5216\n",
            "Epoch [58/100], Step [ 38/ 78], Loss: 0.5598\n",
            "Epoch [58/100], Step [ 39/ 78], Loss: 0.5663\n",
            "Epoch [58/100], Step [ 40/ 78], Loss: 0.5513\n",
            "Epoch [58/100], Step [ 41/ 78], Loss: 0.5400\n",
            "Epoch [58/100], Step [ 42/ 78], Loss: 0.5596\n",
            "Epoch [58/100], Step [ 43/ 78], Loss: 0.5554\n",
            "Epoch [58/100], Step [ 44/ 78], Loss: 0.5421\n",
            "Epoch [58/100], Step [ 45/ 78], Loss: 0.5241\n",
            "Epoch [58/100], Step [ 46/ 78], Loss: 0.5280\n",
            "Epoch [58/100], Step [ 47/ 78], Loss: 0.5763\n",
            "Epoch [58/100], Step [ 48/ 78], Loss: 0.5504\n",
            "Epoch [58/100], Step [ 49/ 78], Loss: 0.5623\n",
            "Epoch [58/100], Step [ 50/ 78], Loss: 0.5528\n",
            "Epoch [58/100], Step [ 51/ 78], Loss: 0.5540\n",
            "Epoch [58/100], Step [ 52/ 78], Loss: 0.5489\n",
            "Epoch [58/100], Step [ 53/ 78], Loss: 0.5501\n",
            "Epoch [58/100], Step [ 54/ 78], Loss: 0.5599\n",
            "Epoch [58/100], Step [ 55/ 78], Loss: 0.5406\n",
            "Epoch [58/100], Step [ 56/ 78], Loss: 0.5756\n",
            "Epoch [58/100], Step [ 57/ 78], Loss: 0.5435\n",
            "Epoch [58/100], Step [ 58/ 78], Loss: 0.5236\n",
            "Epoch [58/100], Step [ 59/ 78], Loss: 0.5829\n",
            "Epoch [58/100], Step [ 60/ 78], Loss: 0.5846\n",
            "Epoch [58/100], Step [ 61/ 78], Loss: 0.5564\n",
            "Epoch [58/100], Step [ 62/ 78], Loss: 0.5637\n",
            "Epoch [58/100], Step [ 63/ 78], Loss: 0.5056\n",
            "Epoch [59/100], Step [  1/ 78], Loss: 0.5175\n",
            "Epoch [59/100], Step [  2/ 78], Loss: 0.5396\n",
            "Epoch [59/100], Step [  3/ 78], Loss: 0.5540\n",
            "Epoch [59/100], Step [  4/ 78], Loss: 0.5661\n",
            "Epoch [59/100], Step [  5/ 78], Loss: 0.4983\n",
            "Epoch [59/100], Step [  6/ 78], Loss: 0.5439\n",
            "Epoch [59/100], Step [  7/ 78], Loss: 0.5592\n",
            "Epoch [59/100], Step [  8/ 78], Loss: 0.5781\n",
            "Epoch [59/100], Step [  9/ 78], Loss: 0.5550\n",
            "Epoch [59/100], Step [ 10/ 78], Loss: 0.5349\n",
            "Epoch [59/100], Step [ 11/ 78], Loss: 0.5280\n",
            "Epoch [59/100], Step [ 12/ 78], Loss: 0.5379\n",
            "Epoch [59/100], Step [ 13/ 78], Loss: 0.5454\n",
            "Epoch [59/100], Step [ 14/ 78], Loss: 0.5514\n",
            "Epoch [59/100], Step [ 15/ 78], Loss: 0.5338\n",
            "Epoch [59/100], Step [ 16/ 78], Loss: 0.5308\n",
            "Epoch [59/100], Step [ 17/ 78], Loss: 0.5161\n",
            "Epoch [59/100], Step [ 18/ 78], Loss: 0.5413\n",
            "Epoch [59/100], Step [ 19/ 78], Loss: 0.5466\n",
            "Epoch [59/100], Step [ 20/ 78], Loss: 0.5666\n",
            "Epoch [59/100], Step [ 21/ 78], Loss: 0.5634\n",
            "Epoch [59/100], Step [ 22/ 78], Loss: 0.5483\n",
            "Epoch [59/100], Step [ 23/ 78], Loss: 0.5342\n",
            "Epoch [59/100], Step [ 24/ 78], Loss: 0.5629\n",
            "Epoch [59/100], Step [ 25/ 78], Loss: 0.5087\n",
            "Epoch [59/100], Step [ 26/ 78], Loss: 0.5682\n",
            "Epoch [59/100], Step [ 27/ 78], Loss: 0.5446\n",
            "Epoch [59/100], Step [ 28/ 78], Loss: 0.5740\n",
            "Epoch [59/100], Step [ 29/ 78], Loss: 0.5463\n",
            "Epoch [59/100], Step [ 30/ 78], Loss: 0.5453\n",
            "Epoch [59/100], Step [ 31/ 78], Loss: 0.5726\n",
            "Epoch [59/100], Step [ 32/ 78], Loss: 0.5695\n",
            "Epoch [59/100], Step [ 33/ 78], Loss: 0.5456\n",
            "Epoch [59/100], Step [ 34/ 78], Loss: 0.5614\n",
            "Epoch [59/100], Step [ 35/ 78], Loss: 0.5477\n",
            "Epoch [59/100], Step [ 36/ 78], Loss: 0.5651\n",
            "Epoch [59/100], Step [ 37/ 78], Loss: 0.5635\n",
            "Epoch [59/100], Step [ 38/ 78], Loss: 0.5572\n",
            "Epoch [59/100], Step [ 39/ 78], Loss: 0.5543\n",
            "Epoch [59/100], Step [ 40/ 78], Loss: 0.5576\n",
            "Epoch [59/100], Step [ 41/ 78], Loss: 0.5489\n",
            "Epoch [59/100], Step [ 42/ 78], Loss: 0.5660\n",
            "Epoch [59/100], Step [ 43/ 78], Loss: 0.5754\n",
            "Epoch [59/100], Step [ 44/ 78], Loss: 0.5425\n",
            "Epoch [59/100], Step [ 45/ 78], Loss: 0.5594\n",
            "Epoch [59/100], Step [ 46/ 78], Loss: 0.5659\n",
            "Epoch [59/100], Step [ 47/ 78], Loss: 0.5586\n",
            "Epoch [59/100], Step [ 48/ 78], Loss: 0.5712\n",
            "Epoch [59/100], Step [ 49/ 78], Loss: 0.5779\n",
            "Epoch [59/100], Step [ 50/ 78], Loss: 0.5720\n",
            "Epoch [59/100], Step [ 51/ 78], Loss: 0.5574\n",
            "Epoch [59/100], Step [ 52/ 78], Loss: 0.5526\n",
            "Epoch [59/100], Step [ 53/ 78], Loss: 0.5591\n",
            "Epoch [59/100], Step [ 54/ 78], Loss: 0.5832\n",
            "Epoch [59/100], Step [ 55/ 78], Loss: 0.5471\n",
            "Epoch [59/100], Step [ 56/ 78], Loss: 0.5256\n",
            "Epoch [59/100], Step [ 57/ 78], Loss: 0.5424\n",
            "Epoch [59/100], Step [ 58/ 78], Loss: 0.5278\n",
            "Epoch [59/100], Step [ 59/ 78], Loss: 0.5187\n",
            "Epoch [59/100], Step [ 60/ 78], Loss: 0.6081\n",
            "Epoch [59/100], Step [ 61/ 78], Loss: 0.5876\n",
            "Epoch [59/100], Step [ 62/ 78], Loss: 0.5350\n",
            "Epoch [59/100], Step [ 63/ 78], Loss: 0.5876\n",
            "Epoch [60/100], Step [  1/ 78], Loss: 0.5471\n",
            "Epoch [60/100], Step [  2/ 78], Loss: 0.5308\n",
            "Epoch [60/100], Step [  3/ 78], Loss: 0.5239\n",
            "Epoch [60/100], Step [  4/ 78], Loss: 0.5822\n",
            "Epoch [60/100], Step [  5/ 78], Loss: 0.5679\n",
            "Epoch [60/100], Step [  6/ 78], Loss: 0.5740\n",
            "Epoch [60/100], Step [  7/ 78], Loss: 0.5223\n",
            "Epoch [60/100], Step [  8/ 78], Loss: 0.6247\n",
            "Epoch [60/100], Step [  9/ 78], Loss: 0.5153\n",
            "Epoch [60/100], Step [ 10/ 78], Loss: 0.5759\n",
            "Epoch [60/100], Step [ 11/ 78], Loss: 0.5615\n",
            "Epoch [60/100], Step [ 12/ 78], Loss: 0.5695\n",
            "Epoch [60/100], Step [ 13/ 78], Loss: 0.5816\n",
            "Epoch [60/100], Step [ 14/ 78], Loss: 0.6153\n",
            "Epoch [60/100], Step [ 15/ 78], Loss: 0.5837\n",
            "Epoch [60/100], Step [ 16/ 78], Loss: 0.5748\n",
            "Epoch [60/100], Step [ 17/ 78], Loss: 0.5462\n",
            "Epoch [60/100], Step [ 18/ 78], Loss: 0.5278\n",
            "Epoch [60/100], Step [ 19/ 78], Loss: 0.5408\n",
            "Epoch [60/100], Step [ 20/ 78], Loss: 0.5293\n",
            "Epoch [60/100], Step [ 21/ 78], Loss: 0.5689\n",
            "Epoch [60/100], Step [ 22/ 78], Loss: 0.5178\n",
            "Epoch [60/100], Step [ 23/ 78], Loss: 0.5368\n",
            "Epoch [60/100], Step [ 24/ 78], Loss: 0.5520\n",
            "Epoch [60/100], Step [ 25/ 78], Loss: 0.5591\n",
            "Epoch [60/100], Step [ 26/ 78], Loss: 0.5323\n",
            "Epoch [60/100], Step [ 27/ 78], Loss: 0.5662\n",
            "Epoch [60/100], Step [ 28/ 78], Loss: 0.5453\n",
            "Epoch [60/100], Step [ 29/ 78], Loss: 0.5576\n",
            "Epoch [60/100], Step [ 30/ 78], Loss: 0.5189\n",
            "Epoch [60/100], Step [ 31/ 78], Loss: 0.5479\n",
            "Epoch [60/100], Step [ 32/ 78], Loss: 0.5351\n",
            "Epoch [60/100], Step [ 33/ 78], Loss: 0.5814\n",
            "Epoch [60/100], Step [ 34/ 78], Loss: 0.5445\n",
            "Epoch [60/100], Step [ 35/ 78], Loss: 0.5187\n",
            "Epoch [60/100], Step [ 36/ 78], Loss: 0.5270\n",
            "Epoch [60/100], Step [ 37/ 78], Loss: 0.5495\n",
            "Epoch [60/100], Step [ 38/ 78], Loss: 0.5189\n",
            "Epoch [60/100], Step [ 39/ 78], Loss: 0.5460\n",
            "Epoch [60/100], Step [ 40/ 78], Loss: 0.5500\n",
            "Epoch [60/100], Step [ 41/ 78], Loss: 0.5595\n",
            "Epoch [60/100], Step [ 42/ 78], Loss: 0.5474\n",
            "Epoch [60/100], Step [ 43/ 78], Loss: 0.5471\n",
            "Epoch [60/100], Step [ 44/ 78], Loss: 0.5718\n",
            "Epoch [60/100], Step [ 45/ 78], Loss: 0.5597\n",
            "Epoch [60/100], Step [ 46/ 78], Loss: 0.5638\n",
            "Epoch [60/100], Step [ 47/ 78], Loss: 0.5509\n",
            "Epoch [60/100], Step [ 48/ 78], Loss: 0.5660\n",
            "Epoch [60/100], Step [ 49/ 78], Loss: 0.5661\n",
            "Epoch [60/100], Step [ 50/ 78], Loss: 0.5660\n",
            "Epoch [60/100], Step [ 51/ 78], Loss: 0.6017\n",
            "Epoch [60/100], Step [ 52/ 78], Loss: 0.5736\n",
            "Epoch [60/100], Step [ 53/ 78], Loss: 0.5827\n",
            "Epoch [60/100], Step [ 54/ 78], Loss: 0.5899\n",
            "Epoch [60/100], Step [ 55/ 78], Loss: 0.5311\n",
            "Epoch [60/100], Step [ 56/ 78], Loss: 0.5970\n",
            "Epoch [60/100], Step [ 57/ 78], Loss: 0.5696\n",
            "Epoch [60/100], Step [ 58/ 78], Loss: 0.5909\n",
            "Epoch [60/100], Step [ 59/ 78], Loss: 0.5682\n",
            "Epoch [60/100], Step [ 60/ 78], Loss: 0.5338\n",
            "Epoch [60/100], Step [ 61/ 78], Loss: 0.5475\n",
            "Epoch [60/100], Step [ 62/ 78], Loss: 0.5565\n",
            "Epoch [60/100], Step [ 63/ 78], Loss: 0.5694\n",
            "Epoch [61/100], Step [  1/ 78], Loss: 0.5705\n",
            "Epoch [61/100], Step [  2/ 78], Loss: 0.5574\n",
            "Epoch [61/100], Step [  3/ 78], Loss: 0.5336\n",
            "Epoch [61/100], Step [  4/ 78], Loss: 0.5585\n",
            "Epoch [61/100], Step [  5/ 78], Loss: 0.5195\n",
            "Epoch [61/100], Step [  6/ 78], Loss: 0.5435\n",
            "Epoch [61/100], Step [  7/ 78], Loss: 0.5620\n",
            "Epoch [61/100], Step [  8/ 78], Loss: 0.5576\n",
            "Epoch [61/100], Step [  9/ 78], Loss: 0.5818\n",
            "Epoch [61/100], Step [ 10/ 78], Loss: 0.5578\n",
            "Epoch [61/100], Step [ 11/ 78], Loss: 0.5583\n",
            "Epoch [61/100], Step [ 12/ 78], Loss: 0.5491\n",
            "Epoch [61/100], Step [ 13/ 78], Loss: 0.5336\n",
            "Epoch [61/100], Step [ 14/ 78], Loss: 0.5560\n",
            "Epoch [61/100], Step [ 15/ 78], Loss: 0.5560\n",
            "Epoch [61/100], Step [ 16/ 78], Loss: 0.5538\n",
            "Epoch [61/100], Step [ 17/ 78], Loss: 0.5477\n",
            "Epoch [61/100], Step [ 18/ 78], Loss: 0.5639\n",
            "Epoch [61/100], Step [ 19/ 78], Loss: 0.5783\n",
            "Epoch [61/100], Step [ 20/ 78], Loss: 0.5639\n",
            "Epoch [61/100], Step [ 21/ 78], Loss: 0.5639\n",
            "Epoch [61/100], Step [ 22/ 78], Loss: 0.5554\n",
            "Epoch [61/100], Step [ 23/ 78], Loss: 0.5479\n",
            "Epoch [61/100], Step [ 24/ 78], Loss: 0.5378\n",
            "Epoch [61/100], Step [ 25/ 78], Loss: 0.5415\n",
            "Epoch [61/100], Step [ 26/ 78], Loss: 0.5836\n",
            "Epoch [61/100], Step [ 27/ 78], Loss: 0.5563\n",
            "Epoch [61/100], Step [ 28/ 78], Loss: 0.5165\n",
            "Epoch [61/100], Step [ 29/ 78], Loss: 0.5337\n",
            "Epoch [61/100], Step [ 30/ 78], Loss: 0.5294\n",
            "Epoch [61/100], Step [ 31/ 78], Loss: 0.5701\n",
            "Epoch [61/100], Step [ 32/ 78], Loss: 0.5314\n",
            "Epoch [61/100], Step [ 33/ 78], Loss: 0.5937\n",
            "Epoch [61/100], Step [ 34/ 78], Loss: 0.5199\n",
            "Epoch [61/100], Step [ 35/ 78], Loss: 0.5183\n",
            "Epoch [61/100], Step [ 36/ 78], Loss: 0.5552\n",
            "Epoch [61/100], Step [ 37/ 78], Loss: 0.5018\n",
            "Epoch [61/100], Step [ 38/ 78], Loss: 0.5607\n",
            "Epoch [61/100], Step [ 39/ 78], Loss: 0.5410\n",
            "Epoch [61/100], Step [ 40/ 78], Loss: 0.5362\n",
            "Epoch [61/100], Step [ 41/ 78], Loss: 0.5631\n",
            "Epoch [61/100], Step [ 42/ 78], Loss: 0.5866\n",
            "Epoch [61/100], Step [ 43/ 78], Loss: 0.5614\n",
            "Epoch [61/100], Step [ 44/ 78], Loss: 0.5928\n",
            "Epoch [61/100], Step [ 45/ 78], Loss: 0.5311\n",
            "Epoch [61/100], Step [ 46/ 78], Loss: 0.4656\n",
            "Epoch [61/100], Step [ 47/ 78], Loss: 0.5516\n",
            "Epoch [61/100], Step [ 48/ 78], Loss: 0.5715\n",
            "Epoch [61/100], Step [ 49/ 78], Loss: 0.5932\n",
            "Epoch [61/100], Step [ 50/ 78], Loss: 0.5620\n",
            "Epoch [61/100], Step [ 51/ 78], Loss: 0.5770\n",
            "Epoch [61/100], Step [ 52/ 78], Loss: 0.5483\n",
            "Epoch [61/100], Step [ 53/ 78], Loss: 0.5944\n",
            "Epoch [61/100], Step [ 54/ 78], Loss: 0.5267\n",
            "Epoch [61/100], Step [ 55/ 78], Loss: 0.5381\n",
            "Epoch [61/100], Step [ 56/ 78], Loss: 0.5303\n",
            "Epoch [61/100], Step [ 57/ 78], Loss: 0.5236\n",
            "Epoch [61/100], Step [ 58/ 78], Loss: 0.5459\n",
            "Epoch [61/100], Step [ 59/ 78], Loss: 0.5414\n",
            "Epoch [61/100], Step [ 60/ 78], Loss: 0.5666\n",
            "Epoch [61/100], Step [ 61/ 78], Loss: 0.5655\n",
            "Epoch [61/100], Step [ 62/ 78], Loss: 0.5198\n",
            "Epoch [61/100], Step [ 63/ 78], Loss: 0.5871\n",
            "Epoch [62/100], Step [  1/ 78], Loss: 0.5774\n",
            "Epoch [62/100], Step [  2/ 78], Loss: 0.5786\n",
            "Epoch [62/100], Step [  3/ 78], Loss: 0.5741\n",
            "Epoch [62/100], Step [  4/ 78], Loss: 0.5262\n",
            "Epoch [62/100], Step [  5/ 78], Loss: 0.5541\n",
            "Epoch [62/100], Step [  6/ 78], Loss: 0.5756\n",
            "Epoch [62/100], Step [  7/ 78], Loss: 0.6046\n",
            "Epoch [62/100], Step [  8/ 78], Loss: 0.5707\n",
            "Epoch [62/100], Step [  9/ 78], Loss: 0.5501\n",
            "Epoch [62/100], Step [ 10/ 78], Loss: 0.5721\n",
            "Epoch [62/100], Step [ 11/ 78], Loss: 0.5491\n",
            "Epoch [62/100], Step [ 12/ 78], Loss: 0.5641\n",
            "Epoch [62/100], Step [ 13/ 78], Loss: 0.5703\n",
            "Epoch [62/100], Step [ 14/ 78], Loss: 0.5342\n",
            "Epoch [62/100], Step [ 15/ 78], Loss: 0.5499\n",
            "Epoch [62/100], Step [ 16/ 78], Loss: 0.5075\n",
            "Epoch [62/100], Step [ 17/ 78], Loss: 0.5476\n",
            "Epoch [62/100], Step [ 18/ 78], Loss: 0.5386\n",
            "Epoch [62/100], Step [ 19/ 78], Loss: 0.5165\n",
            "Epoch [62/100], Step [ 20/ 78], Loss: 0.5140\n",
            "Epoch [62/100], Step [ 21/ 78], Loss: 0.5506\n",
            "Epoch [62/100], Step [ 22/ 78], Loss: 0.5562\n",
            "Epoch [62/100], Step [ 23/ 78], Loss: 0.5352\n",
            "Epoch [62/100], Step [ 24/ 78], Loss: 0.5993\n",
            "Epoch [62/100], Step [ 25/ 78], Loss: 0.5728\n",
            "Epoch [62/100], Step [ 26/ 78], Loss: 0.5556\n",
            "Epoch [62/100], Step [ 27/ 78], Loss: 0.5493\n",
            "Epoch [62/100], Step [ 28/ 78], Loss: 0.5353\n",
            "Epoch [62/100], Step [ 29/ 78], Loss: 0.5456\n",
            "Epoch [62/100], Step [ 30/ 78], Loss: 0.5629\n",
            "Epoch [62/100], Step [ 31/ 78], Loss: 0.5694\n",
            "Epoch [62/100], Step [ 32/ 78], Loss: 0.5835\n",
            "Epoch [62/100], Step [ 33/ 78], Loss: 0.5609\n",
            "Epoch [62/100], Step [ 34/ 78], Loss: 0.5462\n",
            "Epoch [62/100], Step [ 35/ 78], Loss: 0.5515\n",
            "Epoch [62/100], Step [ 36/ 78], Loss: 0.5419\n",
            "Epoch [62/100], Step [ 37/ 78], Loss: 0.5644\n",
            "Epoch [62/100], Step [ 38/ 78], Loss: 0.5820\n",
            "Epoch [62/100], Step [ 39/ 78], Loss: 0.5513\n",
            "Epoch [62/100], Step [ 40/ 78], Loss: 0.5201\n",
            "Epoch [62/100], Step [ 41/ 78], Loss: 0.5477\n",
            "Epoch [62/100], Step [ 42/ 78], Loss: 0.5581\n",
            "Epoch [62/100], Step [ 43/ 78], Loss: 0.5471\n",
            "Epoch [62/100], Step [ 44/ 78], Loss: 0.5906\n",
            "Epoch [62/100], Step [ 45/ 78], Loss: 0.5525\n",
            "Epoch [62/100], Step [ 46/ 78], Loss: 0.5271\n",
            "Epoch [62/100], Step [ 47/ 78], Loss: 0.5563\n",
            "Epoch [62/100], Step [ 48/ 78], Loss: 0.5666\n",
            "Epoch [62/100], Step [ 49/ 78], Loss: 0.5714\n",
            "Epoch [62/100], Step [ 50/ 78], Loss: 0.5531\n",
            "Epoch [62/100], Step [ 51/ 78], Loss: 0.5246\n",
            "Epoch [62/100], Step [ 52/ 78], Loss: 0.5332\n",
            "Epoch [62/100], Step [ 53/ 78], Loss: 0.5579\n",
            "Epoch [62/100], Step [ 54/ 78], Loss: 0.5667\n",
            "Epoch [62/100], Step [ 55/ 78], Loss: 0.5305\n",
            "Epoch [62/100], Step [ 56/ 78], Loss: 0.5429\n",
            "Epoch [62/100], Step [ 57/ 78], Loss: 0.5530\n",
            "Epoch [62/100], Step [ 58/ 78], Loss: 0.5546\n",
            "Epoch [62/100], Step [ 59/ 78], Loss: 0.5635\n",
            "Epoch [62/100], Step [ 60/ 78], Loss: 0.5719\n",
            "Epoch [62/100], Step [ 61/ 78], Loss: 0.5286\n",
            "Epoch [62/100], Step [ 62/ 78], Loss: 0.5136\n",
            "Epoch [62/100], Step [ 63/ 78], Loss: 0.5603\n",
            "Epoch [63/100], Step [  1/ 78], Loss: 0.5632\n",
            "Epoch [63/100], Step [  2/ 78], Loss: 0.5667\n",
            "Epoch [63/100], Step [  3/ 78], Loss: 0.5506\n",
            "Epoch [63/100], Step [  4/ 78], Loss: 0.5366\n",
            "Epoch [63/100], Step [  5/ 78], Loss: 0.5245\n",
            "Epoch [63/100], Step [  6/ 78], Loss: 0.5770\n",
            "Epoch [63/100], Step [  7/ 78], Loss: 0.5626\n",
            "Epoch [63/100], Step [  8/ 78], Loss: 0.5626\n",
            "Epoch [63/100], Step [  9/ 78], Loss: 0.5355\n",
            "Epoch [63/100], Step [ 10/ 78], Loss: 0.5565\n",
            "Epoch [63/100], Step [ 11/ 78], Loss: 0.5075\n",
            "Epoch [63/100], Step [ 12/ 78], Loss: 0.5668\n",
            "Epoch [63/100], Step [ 13/ 78], Loss: 0.5634\n",
            "Epoch [63/100], Step [ 14/ 78], Loss: 0.5963\n",
            "Epoch [63/100], Step [ 15/ 78], Loss: 0.5421\n",
            "Epoch [63/100], Step [ 16/ 78], Loss: 0.5784\n",
            "Epoch [63/100], Step [ 17/ 78], Loss: 0.5840\n",
            "Epoch [63/100], Step [ 18/ 78], Loss: 0.5320\n",
            "Epoch [63/100], Step [ 19/ 78], Loss: 0.5440\n",
            "Epoch [63/100], Step [ 20/ 78], Loss: 0.5425\n",
            "Epoch [63/100], Step [ 21/ 78], Loss: 0.5384\n",
            "Epoch [63/100], Step [ 22/ 78], Loss: 0.5540\n",
            "Epoch [63/100], Step [ 23/ 78], Loss: 0.5691\n",
            "Epoch [63/100], Step [ 24/ 78], Loss: 0.5531\n",
            "Epoch [63/100], Step [ 25/ 78], Loss: 0.5715\n",
            "Epoch [63/100], Step [ 26/ 78], Loss: 0.5407\n",
            "Epoch [63/100], Step [ 27/ 78], Loss: 0.5182\n",
            "Epoch [63/100], Step [ 28/ 78], Loss: 0.5734\n",
            "Epoch [63/100], Step [ 29/ 78], Loss: 0.5297\n",
            "Epoch [63/100], Step [ 30/ 78], Loss: 0.5336\n",
            "Epoch [63/100], Step [ 31/ 78], Loss: 0.5180\n",
            "Epoch [63/100], Step [ 32/ 78], Loss: 0.5622\n",
            "Epoch [63/100], Step [ 33/ 78], Loss: 0.5368\n",
            "Epoch [63/100], Step [ 34/ 78], Loss: 0.5863\n",
            "Epoch [63/100], Step [ 35/ 78], Loss: 0.5738\n",
            "Epoch [63/100], Step [ 36/ 78], Loss: 0.5462\n",
            "Epoch [63/100], Step [ 37/ 78], Loss: 0.5322\n",
            "Epoch [63/100], Step [ 38/ 78], Loss: 0.5675\n",
            "Epoch [63/100], Step [ 39/ 78], Loss: 0.5596\n",
            "Epoch [63/100], Step [ 40/ 78], Loss: 0.5440\n",
            "Epoch [63/100], Step [ 41/ 78], Loss: 0.5705\n",
            "Epoch [63/100], Step [ 42/ 78], Loss: 0.5533\n",
            "Epoch [63/100], Step [ 43/ 78], Loss: 0.5616\n",
            "Epoch [63/100], Step [ 44/ 78], Loss: 0.5593\n",
            "Epoch [63/100], Step [ 45/ 78], Loss: 0.5198\n",
            "Epoch [63/100], Step [ 46/ 78], Loss: 0.5441\n",
            "Epoch [63/100], Step [ 47/ 78], Loss: 0.5530\n",
            "Epoch [63/100], Step [ 48/ 78], Loss: 0.5344\n",
            "Epoch [63/100], Step [ 49/ 78], Loss: 0.5806\n",
            "Epoch [63/100], Step [ 50/ 78], Loss: 0.5514\n",
            "Epoch [63/100], Step [ 51/ 78], Loss: 0.5579\n",
            "Epoch [63/100], Step [ 52/ 78], Loss: 0.5177\n",
            "Epoch [63/100], Step [ 53/ 78], Loss: 0.5179\n",
            "Epoch [63/100], Step [ 54/ 78], Loss: 0.5713\n",
            "Epoch [63/100], Step [ 55/ 78], Loss: 0.5838\n",
            "Epoch [63/100], Step [ 56/ 78], Loss: 0.5799\n",
            "Epoch [63/100], Step [ 57/ 78], Loss: 0.5685\n",
            "Epoch [63/100], Step [ 58/ 78], Loss: 0.5391\n",
            "Epoch [63/100], Step [ 59/ 78], Loss: 0.5322\n",
            "Epoch [63/100], Step [ 60/ 78], Loss: 0.5567\n",
            "Epoch [63/100], Step [ 61/ 78], Loss: 0.5744\n",
            "Epoch [63/100], Step [ 62/ 78], Loss: 0.6050\n",
            "Epoch [63/100], Step [ 63/ 78], Loss: 0.5186\n",
            "Epoch [64/100], Step [  1/ 78], Loss: 0.5577\n",
            "Epoch [64/100], Step [  2/ 78], Loss: 0.5513\n",
            "Epoch [64/100], Step [  3/ 78], Loss: 0.5481\n",
            "Epoch [64/100], Step [  4/ 78], Loss: 0.5412\n",
            "Epoch [64/100], Step [  5/ 78], Loss: 0.5406\n",
            "Epoch [64/100], Step [  6/ 78], Loss: 0.5476\n",
            "Epoch [64/100], Step [  7/ 78], Loss: 0.5526\n",
            "Epoch [64/100], Step [  8/ 78], Loss: 0.5288\n",
            "Epoch [64/100], Step [  9/ 78], Loss: 0.5164\n",
            "Epoch [64/100], Step [ 10/ 78], Loss: 0.5408\n",
            "Epoch [64/100], Step [ 11/ 78], Loss: 0.5816\n",
            "Epoch [64/100], Step [ 12/ 78], Loss: 0.5230\n",
            "Epoch [64/100], Step [ 13/ 78], Loss: 0.5291\n",
            "Epoch [64/100], Step [ 14/ 78], Loss: 0.5425\n",
            "Epoch [64/100], Step [ 15/ 78], Loss: 0.5575\n",
            "Epoch [64/100], Step [ 16/ 78], Loss: 0.5630\n",
            "Epoch [64/100], Step [ 17/ 78], Loss: 0.5292\n",
            "Epoch [64/100], Step [ 18/ 78], Loss: 0.5705\n",
            "Epoch [64/100], Step [ 19/ 78], Loss: 0.5458\n",
            "Epoch [64/100], Step [ 20/ 78], Loss: 0.5350\n",
            "Epoch [64/100], Step [ 21/ 78], Loss: 0.5840\n",
            "Epoch [64/100], Step [ 22/ 78], Loss: 0.5771\n",
            "Epoch [64/100], Step [ 23/ 78], Loss: 0.6038\n",
            "Epoch [64/100], Step [ 24/ 78], Loss: 0.5494\n",
            "Epoch [64/100], Step [ 25/ 78], Loss: 0.5331\n",
            "Epoch [64/100], Step [ 26/ 78], Loss: 0.5463\n",
            "Epoch [64/100], Step [ 27/ 78], Loss: 0.5871\n",
            "Epoch [64/100], Step [ 28/ 78], Loss: 0.5397\n",
            "Epoch [64/100], Step [ 29/ 78], Loss: 0.5708\n",
            "Epoch [64/100], Step [ 30/ 78], Loss: 0.5993\n",
            "Epoch [64/100], Step [ 31/ 78], Loss: 0.5334\n",
            "Epoch [64/100], Step [ 32/ 78], Loss: 0.5541\n",
            "Epoch [64/100], Step [ 33/ 78], Loss: 0.5349\n",
            "Epoch [64/100], Step [ 34/ 78], Loss: 0.5533\n",
            "Epoch [64/100], Step [ 35/ 78], Loss: 0.5515\n",
            "Epoch [64/100], Step [ 36/ 78], Loss: 0.5594\n",
            "Epoch [64/100], Step [ 37/ 78], Loss: 0.5459\n",
            "Epoch [64/100], Step [ 38/ 78], Loss: 0.5511\n",
            "Epoch [64/100], Step [ 39/ 78], Loss: 0.5633\n",
            "Epoch [64/100], Step [ 40/ 78], Loss: 0.5625\n",
            "Epoch [64/100], Step [ 41/ 78], Loss: 0.5738\n",
            "Epoch [64/100], Step [ 42/ 78], Loss: 0.5623\n",
            "Epoch [64/100], Step [ 43/ 78], Loss: 0.5472\n",
            "Epoch [64/100], Step [ 44/ 78], Loss: 0.4734\n",
            "Epoch [64/100], Step [ 45/ 78], Loss: 0.5207\n",
            "Epoch [64/100], Step [ 46/ 78], Loss: 0.5967\n",
            "Epoch [64/100], Step [ 47/ 78], Loss: 0.5522\n",
            "Epoch [64/100], Step [ 48/ 78], Loss: 0.5905\n",
            "Epoch [64/100], Step [ 49/ 78], Loss: 0.6337\n",
            "Epoch [64/100], Step [ 50/ 78], Loss: 0.5424\n",
            "Epoch [64/100], Step [ 51/ 78], Loss: 0.5177\n",
            "Epoch [64/100], Step [ 52/ 78], Loss: 0.5354\n",
            "Epoch [64/100], Step [ 53/ 78], Loss: 0.5833\n",
            "Epoch [64/100], Step [ 54/ 78], Loss: 0.5167\n",
            "Epoch [64/100], Step [ 55/ 78], Loss: 0.5156\n",
            "Epoch [64/100], Step [ 56/ 78], Loss: 0.5276\n",
            "Epoch [64/100], Step [ 57/ 78], Loss: 0.5188\n",
            "Epoch [64/100], Step [ 58/ 78], Loss: 0.5652\n",
            "Epoch [64/100], Step [ 59/ 78], Loss: 0.5281\n",
            "Epoch [64/100], Step [ 60/ 78], Loss: 0.5581\n",
            "Epoch [64/100], Step [ 61/ 78], Loss: 0.5674\n",
            "Epoch [64/100], Step [ 62/ 78], Loss: 0.5416\n",
            "Epoch [64/100], Step [ 63/ 78], Loss: 0.5388\n",
            "Epoch [65/100], Step [  1/ 78], Loss: 0.5904\n",
            "Epoch [65/100], Step [  2/ 78], Loss: 0.6138\n",
            "Epoch [65/100], Step [  3/ 78], Loss: 0.5630\n",
            "Epoch [65/100], Step [  4/ 78], Loss: 0.5436\n",
            "Epoch [65/100], Step [  5/ 78], Loss: 0.5662\n",
            "Epoch [65/100], Step [  6/ 78], Loss: 0.5490\n",
            "Epoch [65/100], Step [  7/ 78], Loss: 0.5332\n",
            "Epoch [65/100], Step [  8/ 78], Loss: 0.5384\n",
            "Epoch [65/100], Step [  9/ 78], Loss: 0.5424\n",
            "Epoch [65/100], Step [ 10/ 78], Loss: 0.5623\n",
            "Epoch [65/100], Step [ 11/ 78], Loss: 0.5397\n",
            "Epoch [65/100], Step [ 12/ 78], Loss: 0.5257\n",
            "Epoch [65/100], Step [ 13/ 78], Loss: 0.5370\n",
            "Epoch [65/100], Step [ 14/ 78], Loss: 0.5571\n",
            "Epoch [65/100], Step [ 15/ 78], Loss: 0.5382\n",
            "Epoch [65/100], Step [ 16/ 78], Loss: 0.5667\n",
            "Epoch [65/100], Step [ 17/ 78], Loss: 0.5462\n",
            "Epoch [65/100], Step [ 18/ 78], Loss: 0.5100\n",
            "Epoch [65/100], Step [ 19/ 78], Loss: 0.5763\n",
            "Epoch [65/100], Step [ 20/ 78], Loss: 0.5342\n",
            "Epoch [65/100], Step [ 21/ 78], Loss: 0.5183\n",
            "Epoch [65/100], Step [ 22/ 78], Loss: 0.5114\n",
            "Epoch [65/100], Step [ 23/ 78], Loss: 0.5354\n",
            "Epoch [65/100], Step [ 24/ 78], Loss: 0.5512\n",
            "Epoch [65/100], Step [ 25/ 78], Loss: 0.5590\n",
            "Epoch [65/100], Step [ 26/ 78], Loss: 0.5362\n",
            "Epoch [65/100], Step [ 27/ 78], Loss: 0.5231\n",
            "Epoch [65/100], Step [ 28/ 78], Loss: 0.5381\n",
            "Epoch [65/100], Step [ 29/ 78], Loss: 0.5705\n",
            "Epoch [65/100], Step [ 30/ 78], Loss: 0.5513\n",
            "Epoch [65/100], Step [ 31/ 78], Loss: 0.5699\n",
            "Epoch [65/100], Step [ 32/ 78], Loss: 0.5425\n",
            "Epoch [65/100], Step [ 33/ 78], Loss: 0.5319\n",
            "Epoch [65/100], Step [ 34/ 78], Loss: 0.5596\n",
            "Epoch [65/100], Step [ 35/ 78], Loss: 0.5865\n",
            "Epoch [65/100], Step [ 36/ 78], Loss: 0.5711\n",
            "Epoch [65/100], Step [ 37/ 78], Loss: 0.5286\n",
            "Epoch [65/100], Step [ 38/ 78], Loss: 0.5255\n",
            "Epoch [65/100], Step [ 39/ 78], Loss: 0.5546\n",
            "Epoch [65/100], Step [ 40/ 78], Loss: 0.5618\n",
            "Epoch [65/100], Step [ 41/ 78], Loss: 0.5397\n",
            "Epoch [65/100], Step [ 42/ 78], Loss: 0.5315\n",
            "Epoch [65/100], Step [ 43/ 78], Loss: 0.5340\n",
            "Epoch [65/100], Step [ 44/ 78], Loss: 0.5451\n",
            "Epoch [65/100], Step [ 45/ 78], Loss: 0.5355\n",
            "Epoch [65/100], Step [ 46/ 78], Loss: 0.5436\n",
            "Epoch [65/100], Step [ 47/ 78], Loss: 0.5347\n",
            "Epoch [65/100], Step [ 48/ 78], Loss: 0.5878\n",
            "Epoch [65/100], Step [ 49/ 78], Loss: 0.5547\n",
            "Epoch [65/100], Step [ 50/ 78], Loss: 0.5454\n",
            "Epoch [65/100], Step [ 51/ 78], Loss: 0.5634\n",
            "Epoch [65/100], Step [ 52/ 78], Loss: 0.5555\n",
            "Epoch [65/100], Step [ 53/ 78], Loss: 0.5303\n",
            "Epoch [65/100], Step [ 54/ 78], Loss: 0.5321\n",
            "Epoch [65/100], Step [ 55/ 78], Loss: 0.5190\n",
            "Epoch [65/100], Step [ 56/ 78], Loss: 0.5497\n",
            "Epoch [65/100], Step [ 57/ 78], Loss: 0.5464\n",
            "Epoch [65/100], Step [ 58/ 78], Loss: 0.5479\n",
            "Epoch [65/100], Step [ 59/ 78], Loss: 0.5168\n",
            "Epoch [65/100], Step [ 60/ 78], Loss: 0.5563\n",
            "Epoch [65/100], Step [ 61/ 78], Loss: 0.5479\n",
            "Epoch [65/100], Step [ 62/ 78], Loss: 0.5752\n",
            "Epoch [65/100], Step [ 63/ 78], Loss: 0.5274\n",
            "Epoch [66/100], Step [  1/ 78], Loss: 0.5785\n",
            "Epoch [66/100], Step [  2/ 78], Loss: 0.5783\n",
            "Epoch [66/100], Step [  3/ 78], Loss: 0.5596\n",
            "Epoch [66/100], Step [  4/ 78], Loss: 0.5386\n",
            "Epoch [66/100], Step [  5/ 78], Loss: 0.5502\n",
            "Epoch [66/100], Step [  6/ 78], Loss: 0.5273\n",
            "Epoch [66/100], Step [  7/ 78], Loss: 0.5275\n",
            "Epoch [66/100], Step [  8/ 78], Loss: 0.5402\n",
            "Epoch [66/100], Step [  9/ 78], Loss: 0.5829\n",
            "Epoch [66/100], Step [ 10/ 78], Loss: 0.5513\n",
            "Epoch [66/100], Step [ 11/ 78], Loss: 0.5449\n",
            "Epoch [66/100], Step [ 12/ 78], Loss: 0.5800\n",
            "Epoch [66/100], Step [ 13/ 78], Loss: 0.5917\n",
            "Epoch [66/100], Step [ 14/ 78], Loss: 0.5513\n",
            "Epoch [66/100], Step [ 15/ 78], Loss: 0.5308\n",
            "Epoch [66/100], Step [ 16/ 78], Loss: 0.5656\n",
            "Epoch [66/100], Step [ 17/ 78], Loss: 0.5553\n",
            "Epoch [66/100], Step [ 18/ 78], Loss: 0.5398\n",
            "Epoch [66/100], Step [ 19/ 78], Loss: 0.5229\n",
            "Epoch [66/100], Step [ 20/ 78], Loss: 0.5273\n",
            "Epoch [66/100], Step [ 21/ 78], Loss: 0.5676\n",
            "Epoch [66/100], Step [ 22/ 78], Loss: 0.5566\n",
            "Epoch [66/100], Step [ 23/ 78], Loss: 0.5250\n",
            "Epoch [66/100], Step [ 24/ 78], Loss: 0.5572\n",
            "Epoch [66/100], Step [ 25/ 78], Loss: 0.5591\n",
            "Epoch [66/100], Step [ 26/ 78], Loss: 0.5190\n",
            "Epoch [66/100], Step [ 27/ 78], Loss: 0.5584\n",
            "Epoch [66/100], Step [ 28/ 78], Loss: 0.5145\n",
            "Epoch [66/100], Step [ 29/ 78], Loss: 0.5251\n",
            "Epoch [66/100], Step [ 30/ 78], Loss: 0.5247\n",
            "Epoch [66/100], Step [ 31/ 78], Loss: 0.5348\n",
            "Epoch [66/100], Step [ 32/ 78], Loss: 0.5915\n",
            "Epoch [66/100], Step [ 33/ 78], Loss: 0.6445\n",
            "Epoch [66/100], Step [ 34/ 78], Loss: 0.5590\n",
            "Epoch [66/100], Step [ 35/ 78], Loss: 0.5432\n",
            "Epoch [66/100], Step [ 36/ 78], Loss: 0.5223\n",
            "Epoch [66/100], Step [ 37/ 78], Loss: 0.5345\n",
            "Epoch [66/100], Step [ 38/ 78], Loss: 0.5467\n",
            "Epoch [66/100], Step [ 39/ 78], Loss: 0.5221\n",
            "Epoch [66/100], Step [ 40/ 78], Loss: 0.5260\n",
            "Epoch [66/100], Step [ 41/ 78], Loss: 0.5490\n",
            "Epoch [66/100], Step [ 42/ 78], Loss: 0.5225\n",
            "Epoch [66/100], Step [ 43/ 78], Loss: 0.5637\n",
            "Epoch [66/100], Step [ 44/ 78], Loss: 0.5330\n",
            "Epoch [66/100], Step [ 45/ 78], Loss: 0.5467\n",
            "Epoch [66/100], Step [ 46/ 78], Loss: 0.5459\n",
            "Epoch [66/100], Step [ 47/ 78], Loss: 0.5591\n",
            "Epoch [66/100], Step [ 48/ 78], Loss: 0.5390\n",
            "Epoch [66/100], Step [ 49/ 78], Loss: 0.5412\n",
            "Epoch [66/100], Step [ 50/ 78], Loss: 0.5440\n",
            "Epoch [66/100], Step [ 51/ 78], Loss: 0.5661\n",
            "Epoch [66/100], Step [ 52/ 78], Loss: 0.5952\n",
            "Epoch [66/100], Step [ 53/ 78], Loss: 0.5627\n",
            "Epoch [66/100], Step [ 54/ 78], Loss: 0.5546\n",
            "Epoch [66/100], Step [ 55/ 78], Loss: 0.5338\n",
            "Epoch [66/100], Step [ 56/ 78], Loss: 0.5374\n",
            "Epoch [66/100], Step [ 57/ 78], Loss: 0.5664\n",
            "Epoch [66/100], Step [ 58/ 78], Loss: 0.5264\n",
            "Epoch [66/100], Step [ 59/ 78], Loss: 0.5279\n",
            "Epoch [66/100], Step [ 60/ 78], Loss: 0.5448\n",
            "Epoch [66/100], Step [ 61/ 78], Loss: 0.5288\n",
            "Epoch [66/100], Step [ 62/ 78], Loss: 0.5051\n",
            "Epoch [66/100], Step [ 63/ 78], Loss: 0.5458\n",
            "Epoch [67/100], Step [  1/ 78], Loss: 0.5583\n",
            "Epoch [67/100], Step [  2/ 78], Loss: 0.5023\n",
            "Epoch [67/100], Step [  3/ 78], Loss: 0.5316\n",
            "Epoch [67/100], Step [  4/ 78], Loss: 0.5457\n",
            "Epoch [67/100], Step [  5/ 78], Loss: 0.5688\n",
            "Epoch [67/100], Step [  6/ 78], Loss: 0.5415\n",
            "Epoch [67/100], Step [  7/ 78], Loss: 0.5510\n",
            "Epoch [67/100], Step [  8/ 78], Loss: 0.5506\n",
            "Epoch [67/100], Step [  9/ 78], Loss: 0.5510\n",
            "Epoch [67/100], Step [ 10/ 78], Loss: 0.5662\n",
            "Epoch [67/100], Step [ 11/ 78], Loss: 0.5655\n",
            "Epoch [67/100], Step [ 12/ 78], Loss: 0.5529\n",
            "Epoch [67/100], Step [ 13/ 78], Loss: 0.5631\n",
            "Epoch [67/100], Step [ 14/ 78], Loss: 0.5542\n",
            "Epoch [67/100], Step [ 15/ 78], Loss: 0.5361\n",
            "Epoch [67/100], Step [ 16/ 78], Loss: 0.5614\n",
            "Epoch [67/100], Step [ 17/ 78], Loss: 0.5570\n",
            "Epoch [67/100], Step [ 18/ 78], Loss: 0.5761\n",
            "Epoch [67/100], Step [ 19/ 78], Loss: 0.5187\n",
            "Epoch [67/100], Step [ 20/ 78], Loss: 0.5546\n",
            "Epoch [67/100], Step [ 21/ 78], Loss: 0.5430\n",
            "Epoch [67/100], Step [ 22/ 78], Loss: 0.5508\n",
            "Epoch [67/100], Step [ 23/ 78], Loss: 0.5775\n",
            "Epoch [67/100], Step [ 24/ 78], Loss: 0.5004\n",
            "Epoch [67/100], Step [ 25/ 78], Loss: 0.5713\n",
            "Epoch [67/100], Step [ 26/ 78], Loss: 0.5304\n",
            "Epoch [67/100], Step [ 27/ 78], Loss: 0.5623\n",
            "Epoch [67/100], Step [ 28/ 78], Loss: 0.5480\n",
            "Epoch [67/100], Step [ 29/ 78], Loss: 0.5823\n",
            "Epoch [67/100], Step [ 30/ 78], Loss: 0.5354\n",
            "Epoch [67/100], Step [ 31/ 78], Loss: 0.5312\n",
            "Epoch [67/100], Step [ 32/ 78], Loss: 0.5677\n",
            "Epoch [67/100], Step [ 33/ 78], Loss: 0.5172\n",
            "Epoch [67/100], Step [ 34/ 78], Loss: 0.5391\n",
            "Epoch [67/100], Step [ 35/ 78], Loss: 0.5433\n",
            "Epoch [67/100], Step [ 36/ 78], Loss: 0.5392\n",
            "Epoch [67/100], Step [ 37/ 78], Loss: 0.5630\n",
            "Epoch [67/100], Step [ 38/ 78], Loss: 0.5335\n",
            "Epoch [67/100], Step [ 39/ 78], Loss: 0.5279\n",
            "Epoch [67/100], Step [ 40/ 78], Loss: 0.5811\n",
            "Epoch [67/100], Step [ 41/ 78], Loss: 0.5431\n",
            "Epoch [67/100], Step [ 42/ 78], Loss: 0.5415\n",
            "Epoch [67/100], Step [ 43/ 78], Loss: 0.5149\n",
            "Epoch [67/100], Step [ 44/ 78], Loss: 0.5661\n",
            "Epoch [67/100], Step [ 45/ 78], Loss: 0.5735\n",
            "Epoch [67/100], Step [ 46/ 78], Loss: 0.5480\n",
            "Epoch [67/100], Step [ 47/ 78], Loss: 0.5378\n",
            "Epoch [67/100], Step [ 48/ 78], Loss: 0.5465\n",
            "Epoch [67/100], Step [ 49/ 78], Loss: 0.5892\n",
            "Epoch [67/100], Step [ 50/ 78], Loss: 0.5310\n",
            "Epoch [67/100], Step [ 51/ 78], Loss: 0.5249\n",
            "Epoch [67/100], Step [ 52/ 78], Loss: 0.5939\n",
            "Epoch [67/100], Step [ 53/ 78], Loss: 0.5554\n",
            "Epoch [67/100], Step [ 54/ 78], Loss: 0.5618\n",
            "Epoch [67/100], Step [ 55/ 78], Loss: 0.5609\n",
            "Epoch [67/100], Step [ 56/ 78], Loss: 0.5305\n",
            "Epoch [67/100], Step [ 57/ 78], Loss: 0.5067\n",
            "Epoch [67/100], Step [ 58/ 78], Loss: 0.5643\n",
            "Epoch [67/100], Step [ 59/ 78], Loss: 0.5592\n",
            "Epoch [67/100], Step [ 60/ 78], Loss: 0.5395\n",
            "Epoch [67/100], Step [ 61/ 78], Loss: 0.5307\n",
            "Epoch [67/100], Step [ 62/ 78], Loss: 0.5378\n",
            "Epoch [67/100], Step [ 63/ 78], Loss: 0.5050\n",
            "Epoch [68/100], Step [  1/ 78], Loss: 0.5402\n",
            "Epoch [68/100], Step [  2/ 78], Loss: 0.5607\n",
            "Epoch [68/100], Step [  3/ 78], Loss: 0.5351\n",
            "Epoch [68/100], Step [  4/ 78], Loss: 0.5302\n",
            "Epoch [68/100], Step [  5/ 78], Loss: 0.5525\n",
            "Epoch [68/100], Step [  6/ 78], Loss: 0.5313\n",
            "Epoch [68/100], Step [  7/ 78], Loss: 0.5525\n",
            "Epoch [68/100], Step [  8/ 78], Loss: 0.5143\n",
            "Epoch [68/100], Step [  9/ 78], Loss: 0.5445\n",
            "Epoch [68/100], Step [ 10/ 78], Loss: 0.5611\n",
            "Epoch [68/100], Step [ 11/ 78], Loss: 0.5025\n",
            "Epoch [68/100], Step [ 12/ 78], Loss: 0.5134\n",
            "Epoch [68/100], Step [ 13/ 78], Loss: 0.5641\n",
            "Epoch [68/100], Step [ 14/ 78], Loss: 0.5674\n",
            "Epoch [68/100], Step [ 15/ 78], Loss: 0.5174\n",
            "Epoch [68/100], Step [ 16/ 78], Loss: 0.5489\n",
            "Epoch [68/100], Step [ 17/ 78], Loss: 0.5536\n",
            "Epoch [68/100], Step [ 18/ 78], Loss: 0.5655\n",
            "Epoch [68/100], Step [ 19/ 78], Loss: 0.5566\n",
            "Epoch [68/100], Step [ 20/ 78], Loss: 0.5394\n",
            "Epoch [68/100], Step [ 21/ 78], Loss: 0.5298\n",
            "Epoch [68/100], Step [ 22/ 78], Loss: 0.5487\n",
            "Epoch [68/100], Step [ 23/ 78], Loss: 0.5035\n",
            "Epoch [68/100], Step [ 24/ 78], Loss: 0.5409\n",
            "Epoch [68/100], Step [ 25/ 78], Loss: 0.5537\n",
            "Epoch [68/100], Step [ 26/ 78], Loss: 0.5487\n",
            "Epoch [68/100], Step [ 27/ 78], Loss: 0.5545\n",
            "Epoch [68/100], Step [ 28/ 78], Loss: 0.5384\n",
            "Epoch [68/100], Step [ 29/ 78], Loss: 0.5277\n",
            "Epoch [68/100], Step [ 30/ 78], Loss: 0.6212\n",
            "Epoch [68/100], Step [ 31/ 78], Loss: 0.5966\n",
            "Epoch [68/100], Step [ 32/ 78], Loss: 0.5541\n",
            "Epoch [68/100], Step [ 33/ 78], Loss: 0.5193\n",
            "Epoch [68/100], Step [ 34/ 78], Loss: 0.5444\n",
            "Epoch [68/100], Step [ 35/ 78], Loss: 0.5354\n",
            "Epoch [68/100], Step [ 36/ 78], Loss: 0.5455\n",
            "Epoch [68/100], Step [ 37/ 78], Loss: 0.5028\n",
            "Epoch [68/100], Step [ 38/ 78], Loss: 0.5515\n",
            "Epoch [68/100], Step [ 39/ 78], Loss: 0.5326\n",
            "Epoch [68/100], Step [ 40/ 78], Loss: 0.5585\n",
            "Epoch [68/100], Step [ 41/ 78], Loss: 0.5282\n",
            "Epoch [68/100], Step [ 42/ 78], Loss: 0.5332\n",
            "Epoch [68/100], Step [ 43/ 78], Loss: 0.5494\n",
            "Epoch [68/100], Step [ 44/ 78], Loss: 0.5538\n",
            "Epoch [68/100], Step [ 45/ 78], Loss: 0.5212\n",
            "Epoch [68/100], Step [ 46/ 78], Loss: 0.5529\n",
            "Epoch [68/100], Step [ 47/ 78], Loss: 0.5366\n",
            "Epoch [68/100], Step [ 48/ 78], Loss: 0.4955\n",
            "Epoch [68/100], Step [ 49/ 78], Loss: 0.5150\n",
            "Epoch [68/100], Step [ 50/ 78], Loss: 0.5259\n",
            "Epoch [68/100], Step [ 51/ 78], Loss: 0.5577\n",
            "Epoch [68/100], Step [ 52/ 78], Loss: 0.5216\n",
            "Epoch [68/100], Step [ 53/ 78], Loss: 0.4864\n",
            "Epoch [68/100], Step [ 54/ 78], Loss: 0.5422\n",
            "Epoch [68/100], Step [ 55/ 78], Loss: 0.5754\n",
            "Epoch [68/100], Step [ 56/ 78], Loss: 0.5715\n",
            "Epoch [68/100], Step [ 57/ 78], Loss: 0.5457\n",
            "Epoch [68/100], Step [ 58/ 78], Loss: 0.5580\n",
            "Epoch [68/100], Step [ 59/ 78], Loss: 0.5738\n",
            "Epoch [68/100], Step [ 60/ 78], Loss: 0.5894\n",
            "Epoch [68/100], Step [ 61/ 78], Loss: 0.5384\n",
            "Epoch [68/100], Step [ 62/ 78], Loss: 0.5136\n",
            "Epoch [68/100], Step [ 63/ 78], Loss: 0.5738\n",
            "Epoch [69/100], Step [  1/ 78], Loss: 0.5280\n",
            "Epoch [69/100], Step [  2/ 78], Loss: 0.5398\n",
            "Epoch [69/100], Step [  3/ 78], Loss: 0.5263\n",
            "Epoch [69/100], Step [  4/ 78], Loss: 0.5697\n",
            "Epoch [69/100], Step [  5/ 78], Loss: 0.5218\n",
            "Epoch [69/100], Step [  6/ 78], Loss: 0.5760\n",
            "Epoch [69/100], Step [  7/ 78], Loss: 0.5432\n",
            "Epoch [69/100], Step [  8/ 78], Loss: 0.5768\n",
            "Epoch [69/100], Step [  9/ 78], Loss: 0.5307\n",
            "Epoch [69/100], Step [ 10/ 78], Loss: 0.5574\n",
            "Epoch [69/100], Step [ 11/ 78], Loss: 0.5377\n",
            "Epoch [69/100], Step [ 12/ 78], Loss: 0.5188\n",
            "Epoch [69/100], Step [ 13/ 78], Loss: 0.5582\n",
            "Epoch [69/100], Step [ 14/ 78], Loss: 0.5031\n",
            "Epoch [69/100], Step [ 15/ 78], Loss: 0.5712\n",
            "Epoch [69/100], Step [ 16/ 78], Loss: 0.5227\n",
            "Epoch [69/100], Step [ 17/ 78], Loss: 0.5727\n",
            "Epoch [69/100], Step [ 18/ 78], Loss: 0.5673\n",
            "Epoch [69/100], Step [ 19/ 78], Loss: 0.5469\n",
            "Epoch [69/100], Step [ 20/ 78], Loss: 0.5458\n",
            "Epoch [69/100], Step [ 21/ 78], Loss: 0.5562\n",
            "Epoch [69/100], Step [ 22/ 78], Loss: 0.5314\n",
            "Epoch [69/100], Step [ 23/ 78], Loss: 0.5570\n",
            "Epoch [69/100], Step [ 24/ 78], Loss: 0.5201\n",
            "Epoch [69/100], Step [ 25/ 78], Loss: 0.5494\n",
            "Epoch [69/100], Step [ 26/ 78], Loss: 0.5198\n",
            "Epoch [69/100], Step [ 27/ 78], Loss: 0.5744\n",
            "Epoch [69/100], Step [ 28/ 78], Loss: 0.5142\n",
            "Epoch [69/100], Step [ 29/ 78], Loss: 0.5322\n",
            "Epoch [69/100], Step [ 30/ 78], Loss: 0.5760\n",
            "Epoch [69/100], Step [ 31/ 78], Loss: 0.5422\n",
            "Epoch [69/100], Step [ 32/ 78], Loss: 0.5482\n",
            "Epoch [69/100], Step [ 33/ 78], Loss: 0.5358\n",
            "Epoch [69/100], Step [ 34/ 78], Loss: 0.5169\n",
            "Epoch [69/100], Step [ 35/ 78], Loss: 0.5366\n",
            "Epoch [69/100], Step [ 36/ 78], Loss: 0.5568\n",
            "Epoch [69/100], Step [ 37/ 78], Loss: 0.5734\n",
            "Epoch [69/100], Step [ 38/ 78], Loss: 0.5300\n",
            "Epoch [69/100], Step [ 39/ 78], Loss: 0.5431\n",
            "Epoch [69/100], Step [ 40/ 78], Loss: 0.5425\n",
            "Epoch [69/100], Step [ 41/ 78], Loss: 0.5482\n",
            "Epoch [69/100], Step [ 42/ 78], Loss: 0.5784\n",
            "Epoch [69/100], Step [ 43/ 78], Loss: 0.5088\n",
            "Epoch [69/100], Step [ 44/ 78], Loss: 0.5322\n",
            "Epoch [69/100], Step [ 45/ 78], Loss: 0.5179\n",
            "Epoch [69/100], Step [ 46/ 78], Loss: 0.5601\n",
            "Epoch [69/100], Step [ 47/ 78], Loss: 0.5380\n",
            "Epoch [69/100], Step [ 48/ 78], Loss: 0.6052\n",
            "Epoch [69/100], Step [ 49/ 78], Loss: 0.5948\n",
            "Epoch [69/100], Step [ 50/ 78], Loss: 0.5161\n",
            "Epoch [69/100], Step [ 51/ 78], Loss: 0.4924\n",
            "Epoch [69/100], Step [ 52/ 78], Loss: 0.5351\n",
            "Epoch [69/100], Step [ 53/ 78], Loss: 0.5779\n",
            "Epoch [69/100], Step [ 54/ 78], Loss: 0.5458\n",
            "Epoch [69/100], Step [ 55/ 78], Loss: 0.5546\n",
            "Epoch [69/100], Step [ 56/ 78], Loss: 0.5229\n",
            "Epoch [69/100], Step [ 57/ 78], Loss: 0.5335\n",
            "Epoch [69/100], Step [ 58/ 78], Loss: 0.5477\n",
            "Epoch [69/100], Step [ 59/ 78], Loss: 0.5197\n",
            "Epoch [69/100], Step [ 60/ 78], Loss: 0.5555\n",
            "Epoch [69/100], Step [ 61/ 78], Loss: 0.5611\n",
            "Epoch [69/100], Step [ 62/ 78], Loss: 0.5252\n",
            "Epoch [69/100], Step [ 63/ 78], Loss: 0.5285\n",
            "Epoch [70/100], Step [  1/ 78], Loss: 0.5147\n",
            "Epoch [70/100], Step [  2/ 78], Loss: 0.5342\n",
            "Epoch [70/100], Step [  3/ 78], Loss: 0.5149\n",
            "Epoch [70/100], Step [  4/ 78], Loss: 0.5840\n",
            "Epoch [70/100], Step [  5/ 78], Loss: 0.5382\n",
            "Epoch [70/100], Step [  6/ 78], Loss: 0.5565\n",
            "Epoch [70/100], Step [  7/ 78], Loss: 0.5475\n",
            "Epoch [70/100], Step [  8/ 78], Loss: 0.5183\n",
            "Epoch [70/100], Step [  9/ 78], Loss: 0.4936\n",
            "Epoch [70/100], Step [ 10/ 78], Loss: 0.5227\n",
            "Epoch [70/100], Step [ 11/ 78], Loss: 0.5134\n",
            "Epoch [70/100], Step [ 12/ 78], Loss: 0.5496\n",
            "Epoch [70/100], Step [ 13/ 78], Loss: 0.5347\n",
            "Epoch [70/100], Step [ 14/ 78], Loss: 0.5330\n",
            "Epoch [70/100], Step [ 15/ 78], Loss: 0.5386\n",
            "Epoch [70/100], Step [ 16/ 78], Loss: 0.5264\n",
            "Epoch [70/100], Step [ 17/ 78], Loss: 0.5636\n",
            "Epoch [70/100], Step [ 18/ 78], Loss: 0.5601\n",
            "Epoch [70/100], Step [ 19/ 78], Loss: 0.5243\n",
            "Epoch [70/100], Step [ 20/ 78], Loss: 0.5214\n",
            "Epoch [70/100], Step [ 21/ 78], Loss: 0.5700\n",
            "Epoch [70/100], Step [ 22/ 78], Loss: 0.5664\n",
            "Epoch [70/100], Step [ 23/ 78], Loss: 0.5256\n",
            "Epoch [70/100], Step [ 24/ 78], Loss: 0.5304\n",
            "Epoch [70/100], Step [ 25/ 78], Loss: 0.5494\n",
            "Epoch [70/100], Step [ 26/ 78], Loss: 0.5378\n",
            "Epoch [70/100], Step [ 27/ 78], Loss: 0.4847\n",
            "Epoch [70/100], Step [ 28/ 78], Loss: 0.5227\n",
            "Epoch [70/100], Step [ 29/ 78], Loss: 0.5638\n",
            "Epoch [70/100], Step [ 30/ 78], Loss: 0.5325\n",
            "Epoch [70/100], Step [ 31/ 78], Loss: 0.5422\n",
            "Epoch [70/100], Step [ 32/ 78], Loss: 0.5522\n",
            "Epoch [70/100], Step [ 33/ 78], Loss: 0.5483\n",
            "Epoch [70/100], Step [ 34/ 78], Loss: 0.5790\n",
            "Epoch [70/100], Step [ 35/ 78], Loss: 0.6095\n",
            "Epoch [70/100], Step [ 36/ 78], Loss: 0.5403\n",
            "Epoch [70/100], Step [ 37/ 78], Loss: 0.5654\n",
            "Epoch [70/100], Step [ 38/ 78], Loss: 0.5535\n",
            "Epoch [70/100], Step [ 39/ 78], Loss: 0.5797\n",
            "Epoch [70/100], Step [ 40/ 78], Loss: 0.5680\n",
            "Epoch [70/100], Step [ 41/ 78], Loss: 0.5403\n",
            "Epoch [70/100], Step [ 42/ 78], Loss: 0.5653\n",
            "Epoch [70/100], Step [ 43/ 78], Loss: 0.5723\n",
            "Epoch [70/100], Step [ 44/ 78], Loss: 0.5234\n",
            "Epoch [70/100], Step [ 45/ 78], Loss: 0.5216\n",
            "Epoch [70/100], Step [ 46/ 78], Loss: 0.5120\n",
            "Epoch [70/100], Step [ 47/ 78], Loss: 0.5519\n",
            "Epoch [70/100], Step [ 48/ 78], Loss: 0.5217\n",
            "Epoch [70/100], Step [ 49/ 78], Loss: 0.5359\n",
            "Epoch [70/100], Step [ 50/ 78], Loss: 0.5135\n",
            "Epoch [70/100], Step [ 51/ 78], Loss: 0.5515\n",
            "Epoch [70/100], Step [ 52/ 78], Loss: 0.5359\n",
            "Epoch [70/100], Step [ 53/ 78], Loss: 0.5587\n",
            "Epoch [70/100], Step [ 54/ 78], Loss: 0.5068\n",
            "Epoch [70/100], Step [ 55/ 78], Loss: 0.5280\n",
            "Epoch [70/100], Step [ 56/ 78], Loss: 0.5384\n",
            "Epoch [70/100], Step [ 57/ 78], Loss: 0.5360\n",
            "Epoch [70/100], Step [ 58/ 78], Loss: 0.5808\n",
            "Epoch [70/100], Step [ 59/ 78], Loss: 0.5494\n",
            "Epoch [70/100], Step [ 60/ 78], Loss: 0.5550\n",
            "Epoch [70/100], Step [ 61/ 78], Loss: 0.5292\n",
            "Epoch [70/100], Step [ 62/ 78], Loss: 0.5276\n",
            "Epoch [70/100], Step [ 63/ 78], Loss: 0.5352\n",
            "Epoch [71/100], Step [  1/ 78], Loss: 0.5835\n",
            "Epoch [71/100], Step [  2/ 78], Loss: 0.5649\n",
            "Epoch [71/100], Step [  3/ 78], Loss: 0.5454\n",
            "Epoch [71/100], Step [  4/ 78], Loss: 0.5248\n",
            "Epoch [71/100], Step [  5/ 78], Loss: 0.5182\n",
            "Epoch [71/100], Step [  6/ 78], Loss: 0.5651\n",
            "Epoch [71/100], Step [  7/ 78], Loss: 0.5144\n",
            "Epoch [71/100], Step [  8/ 78], Loss: 0.5159\n",
            "Epoch [71/100], Step [  9/ 78], Loss: 0.5469\n",
            "Epoch [71/100], Step [ 10/ 78], Loss: 0.5417\n",
            "Epoch [71/100], Step [ 11/ 78], Loss: 0.5680\n",
            "Epoch [71/100], Step [ 12/ 78], Loss: 0.5200\n",
            "Epoch [71/100], Step [ 13/ 78], Loss: 0.5419\n",
            "Epoch [71/100], Step [ 14/ 78], Loss: 0.5267\n",
            "Epoch [71/100], Step [ 15/ 78], Loss: 0.5380\n",
            "Epoch [71/100], Step [ 16/ 78], Loss: 0.5757\n",
            "Epoch [71/100], Step [ 17/ 78], Loss: 0.5166\n",
            "Epoch [71/100], Step [ 18/ 78], Loss: 0.5778\n",
            "Epoch [71/100], Step [ 19/ 78], Loss: 0.5529\n",
            "Epoch [71/100], Step [ 20/ 78], Loss: 0.5613\n",
            "Epoch [71/100], Step [ 21/ 78], Loss: 0.5172\n",
            "Epoch [71/100], Step [ 22/ 78], Loss: 0.5266\n",
            "Epoch [71/100], Step [ 23/ 78], Loss: 0.5102\n",
            "Epoch [71/100], Step [ 24/ 78], Loss: 0.5370\n",
            "Epoch [71/100], Step [ 25/ 78], Loss: 0.5441\n",
            "Epoch [71/100], Step [ 26/ 78], Loss: 0.5441\n",
            "Epoch [71/100], Step [ 27/ 78], Loss: 0.5690\n",
            "Epoch [71/100], Step [ 28/ 78], Loss: 0.5290\n",
            "Epoch [71/100], Step [ 29/ 78], Loss: 0.5712\n",
            "Epoch [71/100], Step [ 30/ 78], Loss: 0.5995\n",
            "Epoch [71/100], Step [ 31/ 78], Loss: 0.6045\n",
            "Epoch [71/100], Step [ 32/ 78], Loss: 0.5487\n",
            "Epoch [71/100], Step [ 33/ 78], Loss: 0.5655\n",
            "Epoch [71/100], Step [ 34/ 78], Loss: 0.5648\n",
            "Epoch [71/100], Step [ 35/ 78], Loss: 0.5352\n",
            "Epoch [71/100], Step [ 36/ 78], Loss: 0.5211\n",
            "Epoch [71/100], Step [ 37/ 78], Loss: 0.5347\n",
            "Epoch [71/100], Step [ 38/ 78], Loss: 0.5377\n",
            "Epoch [71/100], Step [ 39/ 78], Loss: 0.5625\n",
            "Epoch [71/100], Step [ 40/ 78], Loss: 0.5494\n",
            "Epoch [71/100], Step [ 41/ 78], Loss: 0.5274\n",
            "Epoch [71/100], Step [ 42/ 78], Loss: 0.5060\n",
            "Epoch [71/100], Step [ 43/ 78], Loss: 0.5943\n",
            "Epoch [71/100], Step [ 44/ 78], Loss: 0.5692\n",
            "Epoch [71/100], Step [ 45/ 78], Loss: 0.5411\n",
            "Epoch [71/100], Step [ 46/ 78], Loss: 0.5363\n",
            "Epoch [71/100], Step [ 47/ 78], Loss: 0.5677\n",
            "Epoch [71/100], Step [ 48/ 78], Loss: 0.5518\n",
            "Epoch [71/100], Step [ 49/ 78], Loss: 0.5295\n",
            "Epoch [71/100], Step [ 50/ 78], Loss: 0.5652\n",
            "Epoch [71/100], Step [ 51/ 78], Loss: 0.5225\n",
            "Epoch [71/100], Step [ 52/ 78], Loss: 0.5461\n",
            "Epoch [71/100], Step [ 53/ 78], Loss: 0.5136\n",
            "Epoch [71/100], Step [ 54/ 78], Loss: 0.5821\n",
            "Epoch [71/100], Step [ 55/ 78], Loss: 0.5691\n",
            "Epoch [71/100], Step [ 56/ 78], Loss: 0.5219\n",
            "Epoch [71/100], Step [ 57/ 78], Loss: 0.5464\n",
            "Epoch [71/100], Step [ 58/ 78], Loss: 0.5185\n",
            "Epoch [71/100], Step [ 59/ 78], Loss: 0.5488\n",
            "Epoch [71/100], Step [ 60/ 78], Loss: 0.5123\n",
            "Epoch [71/100], Step [ 61/ 78], Loss: 0.5236\n",
            "Epoch [71/100], Step [ 62/ 78], Loss: 0.5425\n",
            "Epoch [71/100], Step [ 63/ 78], Loss: 0.5556\n",
            "Epoch [72/100], Step [  1/ 78], Loss: 0.5013\n",
            "Epoch [72/100], Step [  2/ 78], Loss: 0.5493\n",
            "Epoch [72/100], Step [  3/ 78], Loss: 0.5202\n",
            "Epoch [72/100], Step [  4/ 78], Loss: 0.5128\n",
            "Epoch [72/100], Step [  5/ 78], Loss: 0.5459\n",
            "Epoch [72/100], Step [  6/ 78], Loss: 0.6002\n",
            "Epoch [72/100], Step [  7/ 78], Loss: 0.5270\n",
            "Epoch [72/100], Step [  8/ 78], Loss: 0.5863\n",
            "Epoch [72/100], Step [  9/ 78], Loss: 0.5567\n",
            "Epoch [72/100], Step [ 10/ 78], Loss: 0.5397\n",
            "Epoch [72/100], Step [ 11/ 78], Loss: 0.6213\n",
            "Epoch [72/100], Step [ 12/ 78], Loss: 0.5605\n",
            "Epoch [72/100], Step [ 13/ 78], Loss: 0.5343\n",
            "Epoch [72/100], Step [ 14/ 78], Loss: 0.5360\n",
            "Epoch [72/100], Step [ 15/ 78], Loss: 0.5325\n",
            "Epoch [72/100], Step [ 16/ 78], Loss: 0.5206\n",
            "Epoch [72/100], Step [ 17/ 78], Loss: 0.5529\n",
            "Epoch [72/100], Step [ 18/ 78], Loss: 0.5496\n",
            "Epoch [72/100], Step [ 19/ 78], Loss: 0.5239\n",
            "Epoch [72/100], Step [ 20/ 78], Loss: 0.5168\n",
            "Epoch [72/100], Step [ 21/ 78], Loss: 0.5268\n",
            "Epoch [72/100], Step [ 22/ 78], Loss: 0.5251\n",
            "Epoch [72/100], Step [ 23/ 78], Loss: 0.5250\n",
            "Epoch [72/100], Step [ 24/ 78], Loss: 0.5336\n",
            "Epoch [72/100], Step [ 25/ 78], Loss: 0.5224\n",
            "Epoch [72/100], Step [ 26/ 78], Loss: 0.5259\n",
            "Epoch [72/100], Step [ 27/ 78], Loss: 0.5135\n",
            "Epoch [72/100], Step [ 28/ 78], Loss: 0.5548\n",
            "Epoch [72/100], Step [ 29/ 78], Loss: 0.4932\n",
            "Epoch [72/100], Step [ 30/ 78], Loss: 0.5371\n",
            "Epoch [72/100], Step [ 31/ 78], Loss: 0.5333\n",
            "Epoch [72/100], Step [ 32/ 78], Loss: 0.5311\n",
            "Epoch [72/100], Step [ 33/ 78], Loss: 0.5317\n",
            "Epoch [72/100], Step [ 34/ 78], Loss: 0.5253\n",
            "Epoch [72/100], Step [ 35/ 78], Loss: 0.5731\n",
            "Epoch [72/100], Step [ 36/ 78], Loss: 0.5357\n",
            "Epoch [72/100], Step [ 37/ 78], Loss: 0.5370\n",
            "Epoch [72/100], Step [ 38/ 78], Loss: 0.5216\n",
            "Epoch [72/100], Step [ 39/ 78], Loss: 0.5431\n",
            "Epoch [72/100], Step [ 40/ 78], Loss: 0.5937\n",
            "Epoch [72/100], Step [ 41/ 78], Loss: 0.5332\n",
            "Epoch [72/100], Step [ 42/ 78], Loss: 0.5440\n",
            "Epoch [72/100], Step [ 43/ 78], Loss: 0.5846\n",
            "Epoch [72/100], Step [ 44/ 78], Loss: 0.5671\n",
            "Epoch [72/100], Step [ 45/ 78], Loss: 0.5284\n",
            "Epoch [72/100], Step [ 46/ 78], Loss: 0.5371\n",
            "Epoch [72/100], Step [ 47/ 78], Loss: 0.5492\n",
            "Epoch [72/100], Step [ 48/ 78], Loss: 0.5211\n",
            "Epoch [72/100], Step [ 49/ 78], Loss: 0.5128\n",
            "Epoch [72/100], Step [ 50/ 78], Loss: 0.5051\n",
            "Epoch [72/100], Step [ 51/ 78], Loss: 0.5024\n",
            "Epoch [72/100], Step [ 52/ 78], Loss: 0.5225\n",
            "Epoch [72/100], Step [ 53/ 78], Loss: 0.5772\n",
            "Epoch [72/100], Step [ 54/ 78], Loss: 0.5437\n",
            "Epoch [72/100], Step [ 55/ 78], Loss: 0.5303\n",
            "Epoch [72/100], Step [ 56/ 78], Loss: 0.5406\n",
            "Epoch [72/100], Step [ 57/ 78], Loss: 0.5611\n",
            "Epoch [72/100], Step [ 58/ 78], Loss: 0.5509\n",
            "Epoch [72/100], Step [ 59/ 78], Loss: 0.5458\n",
            "Epoch [72/100], Step [ 60/ 78], Loss: 0.5178\n",
            "Epoch [72/100], Step [ 61/ 78], Loss: 0.5867\n",
            "Epoch [72/100], Step [ 62/ 78], Loss: 0.5509\n",
            "Epoch [72/100], Step [ 63/ 78], Loss: 0.4929\n",
            "Epoch [73/100], Step [  1/ 78], Loss: 0.5357\n",
            "Epoch [73/100], Step [  2/ 78], Loss: 0.5366\n",
            "Epoch [73/100], Step [  3/ 78], Loss: 0.5427\n",
            "Epoch [73/100], Step [  4/ 78], Loss: 0.5279\n",
            "Epoch [73/100], Step [  5/ 78], Loss: 0.5350\n",
            "Epoch [73/100], Step [  6/ 78], Loss: 0.6057\n",
            "Epoch [73/100], Step [  7/ 78], Loss: 0.5221\n",
            "Epoch [73/100], Step [  8/ 78], Loss: 0.5474\n",
            "Epoch [73/100], Step [  9/ 78], Loss: 0.5194\n",
            "Epoch [73/100], Step [ 10/ 78], Loss: 0.5068\n",
            "Epoch [73/100], Step [ 11/ 78], Loss: 0.5444\n",
            "Epoch [73/100], Step [ 12/ 78], Loss: 0.5343\n",
            "Epoch [73/100], Step [ 13/ 78], Loss: 0.5492\n",
            "Epoch [73/100], Step [ 14/ 78], Loss: 0.5709\n",
            "Epoch [73/100], Step [ 15/ 78], Loss: 0.5338\n",
            "Epoch [73/100], Step [ 16/ 78], Loss: 0.5232\n",
            "Epoch [73/100], Step [ 17/ 78], Loss: 0.5033\n",
            "Epoch [73/100], Step [ 18/ 78], Loss: 0.5859\n",
            "Epoch [73/100], Step [ 19/ 78], Loss: 0.5332\n",
            "Epoch [73/100], Step [ 20/ 78], Loss: 0.5180\n",
            "Epoch [73/100], Step [ 21/ 78], Loss: 0.5442\n",
            "Epoch [73/100], Step [ 22/ 78], Loss: 0.5099\n",
            "Epoch [73/100], Step [ 23/ 78], Loss: 0.5623\n",
            "Epoch [73/100], Step [ 24/ 78], Loss: 0.5386\n",
            "Epoch [73/100], Step [ 25/ 78], Loss: 0.5419\n",
            "Epoch [73/100], Step [ 26/ 78], Loss: 0.5314\n",
            "Epoch [73/100], Step [ 27/ 78], Loss: 0.5002\n",
            "Epoch [73/100], Step [ 28/ 78], Loss: 0.4929\n",
            "Epoch [73/100], Step [ 29/ 78], Loss: 0.5406\n",
            "Epoch [73/100], Step [ 30/ 78], Loss: 0.5394\n",
            "Epoch [73/100], Step [ 31/ 78], Loss: 0.5576\n",
            "Epoch [73/100], Step [ 32/ 78], Loss: 0.5731\n",
            "Epoch [73/100], Step [ 33/ 78], Loss: 0.5873\n",
            "Epoch [73/100], Step [ 34/ 78], Loss: 0.5366\n",
            "Epoch [73/100], Step [ 35/ 78], Loss: 0.5336\n",
            "Epoch [73/100], Step [ 36/ 78], Loss: 0.5367\n",
            "Epoch [73/100], Step [ 37/ 78], Loss: 0.5243\n",
            "Epoch [73/100], Step [ 38/ 78], Loss: 0.5870\n",
            "Epoch [73/100], Step [ 39/ 78], Loss: 0.5770\n",
            "Epoch [73/100], Step [ 40/ 78], Loss: 0.5911\n",
            "Epoch [73/100], Step [ 41/ 78], Loss: 0.5528\n",
            "Epoch [73/100], Step [ 42/ 78], Loss: 0.5419\n",
            "Epoch [73/100], Step [ 43/ 78], Loss: 0.5731\n",
            "Epoch [73/100], Step [ 44/ 78], Loss: 0.5332\n",
            "Epoch [73/100], Step [ 45/ 78], Loss: 0.5494\n",
            "Epoch [73/100], Step [ 46/ 78], Loss: 0.5562\n",
            "Epoch [73/100], Step [ 47/ 78], Loss: 0.5563\n",
            "Epoch [73/100], Step [ 48/ 78], Loss: 0.5934\n",
            "Epoch [73/100], Step [ 49/ 78], Loss: 0.4836\n",
            "Epoch [73/100], Step [ 50/ 78], Loss: 0.5019\n",
            "Epoch [73/100], Step [ 51/ 78], Loss: 0.5522\n",
            "Epoch [73/100], Step [ 52/ 78], Loss: 0.5271\n",
            "Epoch [73/100], Step [ 53/ 78], Loss: 0.5551\n",
            "Epoch [73/100], Step [ 54/ 78], Loss: 0.5331\n",
            "Epoch [73/100], Step [ 55/ 78], Loss: 0.5633\n",
            "Epoch [73/100], Step [ 56/ 78], Loss: 0.5512\n",
            "Epoch [73/100], Step [ 57/ 78], Loss: 0.5405\n",
            "Epoch [73/100], Step [ 58/ 78], Loss: 0.5138\n",
            "Epoch [73/100], Step [ 59/ 78], Loss: 0.5071\n",
            "Epoch [73/100], Step [ 60/ 78], Loss: 0.5346\n",
            "Epoch [73/100], Step [ 61/ 78], Loss: 0.5534\n",
            "Epoch [73/100], Step [ 62/ 78], Loss: 0.5349\n",
            "Epoch [73/100], Step [ 63/ 78], Loss: 0.5661\n",
            "Epoch [74/100], Step [  1/ 78], Loss: 0.5399\n",
            "Epoch [74/100], Step [  2/ 78], Loss: 0.5198\n",
            "Epoch [74/100], Step [  3/ 78], Loss: 0.5328\n",
            "Epoch [74/100], Step [  4/ 78], Loss: 0.5103\n",
            "Epoch [74/100], Step [  5/ 78], Loss: 0.5356\n",
            "Epoch [74/100], Step [  6/ 78], Loss: 0.4841\n",
            "Epoch [74/100], Step [  7/ 78], Loss: 0.5563\n",
            "Epoch [74/100], Step [  8/ 78], Loss: 0.5400\n",
            "Epoch [74/100], Step [  9/ 78], Loss: 0.5510\n",
            "Epoch [74/100], Step [ 10/ 78], Loss: 0.5588\n",
            "Epoch [74/100], Step [ 11/ 78], Loss: 0.5425\n",
            "Epoch [74/100], Step [ 12/ 78], Loss: 0.5874\n",
            "Epoch [74/100], Step [ 13/ 78], Loss: 0.5561\n",
            "Epoch [74/100], Step [ 14/ 78], Loss: 0.5485\n",
            "Epoch [74/100], Step [ 15/ 78], Loss: 0.5504\n",
            "Epoch [74/100], Step [ 16/ 78], Loss: 0.5426\n",
            "Epoch [74/100], Step [ 17/ 78], Loss: 0.5577\n",
            "Epoch [74/100], Step [ 18/ 78], Loss: 0.5263\n",
            "Epoch [74/100], Step [ 19/ 78], Loss: 0.5468\n",
            "Epoch [74/100], Step [ 20/ 78], Loss: 0.5328\n",
            "Epoch [74/100], Step [ 21/ 78], Loss: 0.5649\n",
            "Epoch [74/100], Step [ 22/ 78], Loss: 0.5383\n",
            "Epoch [74/100], Step [ 23/ 78], Loss: 0.5143\n",
            "Epoch [74/100], Step [ 24/ 78], Loss: 0.5515\n",
            "Epoch [74/100], Step [ 25/ 78], Loss: 0.5671\n",
            "Epoch [74/100], Step [ 26/ 78], Loss: 0.5105\n",
            "Epoch [74/100], Step [ 27/ 78], Loss: 0.5503\n",
            "Epoch [74/100], Step [ 28/ 78], Loss: 0.5354\n",
            "Epoch [74/100], Step [ 29/ 78], Loss: 0.5510\n",
            "Epoch [74/100], Step [ 30/ 78], Loss: 0.6346\n",
            "Epoch [74/100], Step [ 31/ 78], Loss: 0.5822\n",
            "Epoch [74/100], Step [ 32/ 78], Loss: 0.5349\n",
            "Epoch [74/100], Step [ 33/ 78], Loss: 0.5430\n",
            "Epoch [74/100], Step [ 34/ 78], Loss: 0.5379\n",
            "Epoch [74/100], Step [ 35/ 78], Loss: 0.5657\n",
            "Epoch [74/100], Step [ 36/ 78], Loss: 0.5206\n",
            "Epoch [74/100], Step [ 37/ 78], Loss: 0.5549\n",
            "Epoch [74/100], Step [ 38/ 78], Loss: 0.5369\n",
            "Epoch [74/100], Step [ 39/ 78], Loss: 0.5203\n",
            "Epoch [74/100], Step [ 40/ 78], Loss: 0.5523\n",
            "Epoch [74/100], Step [ 41/ 78], Loss: 0.5184\n",
            "Epoch [74/100], Step [ 42/ 78], Loss: 0.5638\n",
            "Epoch [74/100], Step [ 43/ 78], Loss: 0.5181\n",
            "Epoch [74/100], Step [ 44/ 78], Loss: 0.5197\n",
            "Epoch [74/100], Step [ 45/ 78], Loss: 0.5052\n",
            "Epoch [74/100], Step [ 46/ 78], Loss: 0.5267\n",
            "Epoch [74/100], Step [ 47/ 78], Loss: 0.4949\n",
            "Epoch [74/100], Step [ 48/ 78], Loss: 0.5695\n",
            "Epoch [74/100], Step [ 49/ 78], Loss: 0.5549\n",
            "Epoch [74/100], Step [ 50/ 78], Loss: 0.5470\n",
            "Epoch [74/100], Step [ 51/ 78], Loss: 0.5188\n",
            "Epoch [74/100], Step [ 52/ 78], Loss: 0.5298\n",
            "Epoch [74/100], Step [ 53/ 78], Loss: 0.5151\n",
            "Epoch [74/100], Step [ 54/ 78], Loss: 0.5326\n",
            "Epoch [74/100], Step [ 55/ 78], Loss: 0.5884\n",
            "Epoch [74/100], Step [ 56/ 78], Loss: 0.5483\n",
            "Epoch [74/100], Step [ 57/ 78], Loss: 0.5278\n",
            "Epoch [74/100], Step [ 58/ 78], Loss: 0.5381\n",
            "Epoch [74/100], Step [ 59/ 78], Loss: 0.5438\n",
            "Epoch [74/100], Step [ 60/ 78], Loss: 0.5433\n",
            "Epoch [74/100], Step [ 61/ 78], Loss: 0.5265\n",
            "Epoch [74/100], Step [ 62/ 78], Loss: 0.5391\n",
            "Epoch [74/100], Step [ 63/ 78], Loss: 0.5326\n",
            "Epoch [75/100], Step [  1/ 78], Loss: 0.5645\n",
            "Epoch [75/100], Step [  2/ 78], Loss: 0.5521\n",
            "Epoch [75/100], Step [  3/ 78], Loss: 0.5542\n",
            "Epoch [75/100], Step [  4/ 78], Loss: 0.5390\n",
            "Epoch [75/100], Step [  5/ 78], Loss: 0.5410\n",
            "Epoch [75/100], Step [  6/ 78], Loss: 0.5178\n",
            "Epoch [75/100], Step [  7/ 78], Loss: 0.5202\n",
            "Epoch [75/100], Step [  8/ 78], Loss: 0.5450\n",
            "Epoch [75/100], Step [  9/ 78], Loss: 0.4967\n",
            "Epoch [75/100], Step [ 10/ 78], Loss: 0.5449\n",
            "Epoch [75/100], Step [ 11/ 78], Loss: 0.5279\n",
            "Epoch [75/100], Step [ 12/ 78], Loss: 0.5112\n",
            "Epoch [75/100], Step [ 13/ 78], Loss: 0.5393\n",
            "Epoch [75/100], Step [ 14/ 78], Loss: 0.5854\n",
            "Epoch [75/100], Step [ 15/ 78], Loss: 0.5364\n",
            "Epoch [75/100], Step [ 16/ 78], Loss: 0.5298\n",
            "Epoch [75/100], Step [ 17/ 78], Loss: 0.5657\n",
            "Epoch [75/100], Step [ 18/ 78], Loss: 0.5575\n",
            "Epoch [75/100], Step [ 19/ 78], Loss: 0.5640\n",
            "Epoch [75/100], Step [ 20/ 78], Loss: 0.5241\n",
            "Epoch [75/100], Step [ 21/ 78], Loss: 0.5280\n",
            "Epoch [75/100], Step [ 22/ 78], Loss: 0.5380\n",
            "Epoch [75/100], Step [ 23/ 78], Loss: 0.5475\n",
            "Epoch [75/100], Step [ 24/ 78], Loss: 0.5398\n",
            "Epoch [75/100], Step [ 25/ 78], Loss: 0.5420\n",
            "Epoch [75/100], Step [ 26/ 78], Loss: 0.5099\n",
            "Epoch [75/100], Step [ 27/ 78], Loss: 0.5142\n",
            "Epoch [75/100], Step [ 28/ 78], Loss: 0.5084\n",
            "Epoch [75/100], Step [ 29/ 78], Loss: 0.5191\n",
            "Epoch [75/100], Step [ 30/ 78], Loss: 0.5240\n",
            "Epoch [75/100], Step [ 31/ 78], Loss: 0.5231\n",
            "Epoch [75/100], Step [ 32/ 78], Loss: 0.5341\n",
            "Epoch [75/100], Step [ 33/ 78], Loss: 0.5268\n",
            "Epoch [75/100], Step [ 34/ 78], Loss: 0.5674\n",
            "Epoch [75/100], Step [ 35/ 78], Loss: 0.5204\n",
            "Epoch [75/100], Step [ 36/ 78], Loss: 0.5296\n",
            "Epoch [75/100], Step [ 37/ 78], Loss: 0.5252\n",
            "Epoch [75/100], Step [ 38/ 78], Loss: 0.5804\n",
            "Epoch [75/100], Step [ 39/ 78], Loss: 0.5172\n",
            "Epoch [75/100], Step [ 40/ 78], Loss: 0.5199\n",
            "Epoch [75/100], Step [ 41/ 78], Loss: 0.5345\n",
            "Epoch [75/100], Step [ 42/ 78], Loss: 0.4919\n",
            "Epoch [75/100], Step [ 43/ 78], Loss: 0.5287\n",
            "Epoch [75/100], Step [ 44/ 78], Loss: 0.5349\n",
            "Epoch [75/100], Step [ 45/ 78], Loss: 0.5261\n",
            "Epoch [75/100], Step [ 46/ 78], Loss: 0.5527\n",
            "Epoch [75/100], Step [ 47/ 78], Loss: 0.5106\n",
            "Epoch [75/100], Step [ 48/ 78], Loss: 0.5140\n",
            "Epoch [75/100], Step [ 49/ 78], Loss: 0.5576\n",
            "Epoch [75/100], Step [ 50/ 78], Loss: 0.5408\n",
            "Epoch [75/100], Step [ 51/ 78], Loss: 0.5779\n",
            "Epoch [75/100], Step [ 52/ 78], Loss: 0.5089\n",
            "Epoch [75/100], Step [ 53/ 78], Loss: 0.5773\n",
            "Epoch [75/100], Step [ 54/ 78], Loss: 0.5847\n",
            "Epoch [75/100], Step [ 55/ 78], Loss: 0.5460\n",
            "Epoch [75/100], Step [ 56/ 78], Loss: 0.5104\n",
            "Epoch [75/100], Step [ 57/ 78], Loss: 0.5436\n",
            "Epoch [75/100], Step [ 58/ 78], Loss: 0.5234\n",
            "Epoch [75/100], Step [ 59/ 78], Loss: 0.5353\n",
            "Epoch [75/100], Step [ 60/ 78], Loss: 0.4981\n",
            "Epoch [75/100], Step [ 61/ 78], Loss: 0.5303\n",
            "Epoch [75/100], Step [ 62/ 78], Loss: 0.5319\n",
            "Epoch [75/100], Step [ 63/ 78], Loss: 0.5356\n",
            "Epoch [76/100], Step [  1/ 78], Loss: 0.5673\n",
            "Epoch [76/100], Step [  2/ 78], Loss: 0.5498\n",
            "Epoch [76/100], Step [  3/ 78], Loss: 0.5198\n",
            "Epoch [76/100], Step [  4/ 78], Loss: 0.5664\n",
            "Epoch [76/100], Step [  5/ 78], Loss: 0.5469\n",
            "Epoch [76/100], Step [  6/ 78], Loss: 0.5522\n",
            "Epoch [76/100], Step [  7/ 78], Loss: 0.5367\n",
            "Epoch [76/100], Step [  8/ 78], Loss: 0.5076\n",
            "Epoch [76/100], Step [  9/ 78], Loss: 0.5332\n",
            "Epoch [76/100], Step [ 10/ 78], Loss: 0.5460\n",
            "Epoch [76/100], Step [ 11/ 78], Loss: 0.5490\n",
            "Epoch [76/100], Step [ 12/ 78], Loss: 0.5378\n",
            "Epoch [76/100], Step [ 13/ 78], Loss: 0.5420\n",
            "Epoch [76/100], Step [ 14/ 78], Loss: 0.5324\n",
            "Epoch [76/100], Step [ 15/ 78], Loss: 0.5268\n",
            "Epoch [76/100], Step [ 16/ 78], Loss: 0.5444\n",
            "Epoch [76/100], Step [ 17/ 78], Loss: 0.5207\n",
            "Epoch [76/100], Step [ 18/ 78], Loss: 0.5183\n",
            "Epoch [76/100], Step [ 19/ 78], Loss: 0.5367\n",
            "Epoch [76/100], Step [ 20/ 78], Loss: 0.5106\n",
            "Epoch [76/100], Step [ 21/ 78], Loss: 0.5377\n",
            "Epoch [76/100], Step [ 22/ 78], Loss: 0.5157\n",
            "Epoch [76/100], Step [ 23/ 78], Loss: 0.5529\n",
            "Epoch [76/100], Step [ 24/ 78], Loss: 0.5187\n",
            "Epoch [76/100], Step [ 25/ 78], Loss: 0.5595\n",
            "Epoch [76/100], Step [ 26/ 78], Loss: 0.5347\n",
            "Epoch [76/100], Step [ 27/ 78], Loss: 0.5251\n",
            "Epoch [76/100], Step [ 28/ 78], Loss: 0.5581\n",
            "Epoch [76/100], Step [ 29/ 78], Loss: 0.4953\n",
            "Epoch [76/100], Step [ 30/ 78], Loss: 0.5380\n",
            "Epoch [76/100], Step [ 31/ 78], Loss: 0.5733\n",
            "Epoch [76/100], Step [ 32/ 78], Loss: 0.4924\n",
            "Epoch [76/100], Step [ 33/ 78], Loss: 0.5448\n",
            "Epoch [76/100], Step [ 34/ 78], Loss: 0.5330\n",
            "Epoch [76/100], Step [ 35/ 78], Loss: 0.5438\n",
            "Epoch [76/100], Step [ 36/ 78], Loss: 0.5079\n",
            "Epoch [76/100], Step [ 37/ 78], Loss: 0.5384\n",
            "Epoch [76/100], Step [ 38/ 78], Loss: 0.5086\n",
            "Epoch [76/100], Step [ 39/ 78], Loss: 0.5317\n",
            "Epoch [76/100], Step [ 40/ 78], Loss: 0.5135\n",
            "Epoch [76/100], Step [ 41/ 78], Loss: 0.5163\n",
            "Epoch [76/100], Step [ 42/ 78], Loss: 0.5307\n",
            "Epoch [76/100], Step [ 43/ 78], Loss: 0.5500\n",
            "Epoch [76/100], Step [ 44/ 78], Loss: 0.5742\n",
            "Epoch [76/100], Step [ 45/ 78], Loss: 0.5308\n",
            "Epoch [76/100], Step [ 46/ 78], Loss: 0.5509\n",
            "Epoch [76/100], Step [ 47/ 78], Loss: 0.5549\n",
            "Epoch [76/100], Step [ 48/ 78], Loss: 0.5075\n",
            "Epoch [76/100], Step [ 49/ 78], Loss: 0.5212\n",
            "Epoch [76/100], Step [ 50/ 78], Loss: 0.5472\n",
            "Epoch [76/100], Step [ 51/ 78], Loss: 0.5459\n",
            "Epoch [76/100], Step [ 52/ 78], Loss: 0.5421\n",
            "Epoch [76/100], Step [ 53/ 78], Loss: 0.5248\n",
            "Epoch [76/100], Step [ 54/ 78], Loss: 0.5127\n",
            "Epoch [76/100], Step [ 55/ 78], Loss: 0.5474\n",
            "Epoch [76/100], Step [ 56/ 78], Loss: 0.5547\n",
            "Epoch [76/100], Step [ 57/ 78], Loss: 0.5205\n",
            "Epoch [76/100], Step [ 58/ 78], Loss: 0.5471\n",
            "Epoch [76/100], Step [ 59/ 78], Loss: 0.5284\n",
            "Epoch [76/100], Step [ 60/ 78], Loss: 0.5177\n",
            "Epoch [76/100], Step [ 61/ 78], Loss: 0.5507\n",
            "Epoch [76/100], Step [ 62/ 78], Loss: 0.5253\n",
            "Epoch [76/100], Step [ 63/ 78], Loss: 0.5032\n",
            "Epoch [77/100], Step [  1/ 78], Loss: 0.5175\n",
            "Epoch [77/100], Step [  2/ 78], Loss: 0.5816\n",
            "Epoch [77/100], Step [  3/ 78], Loss: 0.6169\n",
            "Epoch [77/100], Step [  4/ 78], Loss: 0.5726\n",
            "Epoch [77/100], Step [  5/ 78], Loss: 0.4743\n",
            "Epoch [77/100], Step [  6/ 78], Loss: 0.5281\n",
            "Epoch [77/100], Step [  7/ 78], Loss: 0.4923\n",
            "Epoch [77/100], Step [  8/ 78], Loss: 0.5399\n",
            "Epoch [77/100], Step [  9/ 78], Loss: 0.5557\n",
            "Epoch [77/100], Step [ 10/ 78], Loss: 0.5222\n",
            "Epoch [77/100], Step [ 11/ 78], Loss: 0.5761\n",
            "Epoch [77/100], Step [ 12/ 78], Loss: 0.5259\n",
            "Epoch [77/100], Step [ 13/ 78], Loss: 0.5421\n",
            "Epoch [77/100], Step [ 14/ 78], Loss: 0.5455\n",
            "Epoch [77/100], Step [ 15/ 78], Loss: 0.5538\n",
            "Epoch [77/100], Step [ 16/ 78], Loss: 0.5360\n",
            "Epoch [77/100], Step [ 17/ 78], Loss: 0.5663\n",
            "Epoch [77/100], Step [ 18/ 78], Loss: 0.5738\n",
            "Epoch [77/100], Step [ 19/ 78], Loss: 0.5029\n",
            "Epoch [77/100], Step [ 20/ 78], Loss: 0.5594\n",
            "Epoch [77/100], Step [ 21/ 78], Loss: 0.5496\n",
            "Epoch [77/100], Step [ 22/ 78], Loss: 0.5258\n",
            "Epoch [77/100], Step [ 23/ 78], Loss: 0.5312\n",
            "Epoch [77/100], Step [ 24/ 78], Loss: 0.5556\n",
            "Epoch [77/100], Step [ 25/ 78], Loss: 0.5215\n",
            "Epoch [77/100], Step [ 26/ 78], Loss: 0.5326\n",
            "Epoch [77/100], Step [ 27/ 78], Loss: 0.5363\n",
            "Epoch [77/100], Step [ 28/ 78], Loss: 0.5334\n",
            "Epoch [77/100], Step [ 29/ 78], Loss: 0.5304\n",
            "Epoch [77/100], Step [ 30/ 78], Loss: 0.5179\n",
            "Epoch [77/100], Step [ 31/ 78], Loss: 0.5388\n",
            "Epoch [77/100], Step [ 32/ 78], Loss: 0.5848\n",
            "Epoch [77/100], Step [ 33/ 78], Loss: 0.5383\n",
            "Epoch [77/100], Step [ 34/ 78], Loss: 0.5811\n",
            "Epoch [77/100], Step [ 35/ 78], Loss: 0.5209\n",
            "Epoch [77/100], Step [ 36/ 78], Loss: 0.5442\n",
            "Epoch [77/100], Step [ 37/ 78], Loss: 0.5473\n",
            "Epoch [77/100], Step [ 38/ 78], Loss: 0.5389\n",
            "Epoch [77/100], Step [ 39/ 78], Loss: 0.5174\n",
            "Epoch [77/100], Step [ 40/ 78], Loss: 0.5385\n",
            "Epoch [77/100], Step [ 41/ 78], Loss: 0.5263\n",
            "Epoch [77/100], Step [ 42/ 78], Loss: 0.5475\n",
            "Epoch [77/100], Step [ 43/ 78], Loss: 0.5297\n",
            "Epoch [77/100], Step [ 44/ 78], Loss: 0.5655\n",
            "Epoch [77/100], Step [ 45/ 78], Loss: 0.4831\n",
            "Epoch [77/100], Step [ 46/ 78], Loss: 0.5818\n",
            "Epoch [77/100], Step [ 47/ 78], Loss: 0.5589\n",
            "Epoch [77/100], Step [ 48/ 78], Loss: 0.5399\n",
            "Epoch [77/100], Step [ 49/ 78], Loss: 0.5633\n",
            "Epoch [77/100], Step [ 50/ 78], Loss: 0.5434\n",
            "Epoch [77/100], Step [ 51/ 78], Loss: 0.4883\n",
            "Epoch [77/100], Step [ 52/ 78], Loss: 0.5073\n",
            "Epoch [77/100], Step [ 53/ 78], Loss: 0.5890\n",
            "Epoch [77/100], Step [ 54/ 78], Loss: 0.5387\n",
            "Epoch [77/100], Step [ 55/ 78], Loss: 0.5461\n",
            "Epoch [77/100], Step [ 56/ 78], Loss: 0.4884\n",
            "Epoch [77/100], Step [ 57/ 78], Loss: 0.5130\n",
            "Epoch [77/100], Step [ 58/ 78], Loss: 0.5202\n",
            "Epoch [77/100], Step [ 59/ 78], Loss: 0.5047\n",
            "Epoch [77/100], Step [ 60/ 78], Loss: 0.5175\n",
            "Epoch [77/100], Step [ 61/ 78], Loss: 0.5368\n",
            "Epoch [77/100], Step [ 62/ 78], Loss: 0.6032\n",
            "Epoch [77/100], Step [ 63/ 78], Loss: 0.6195\n",
            "Epoch [78/100], Step [  1/ 78], Loss: 0.5731\n",
            "Epoch [78/100], Step [  2/ 78], Loss: 0.5551\n",
            "Epoch [78/100], Step [  3/ 78], Loss: 0.5750\n",
            "Epoch [78/100], Step [  4/ 78], Loss: 0.5151\n",
            "Epoch [78/100], Step [  5/ 78], Loss: 0.5359\n",
            "Epoch [78/100], Step [  6/ 78], Loss: 0.5499\n",
            "Epoch [78/100], Step [  7/ 78], Loss: 0.5331\n",
            "Epoch [78/100], Step [  8/ 78], Loss: 0.5228\n",
            "Epoch [78/100], Step [  9/ 78], Loss: 0.5311\n",
            "Epoch [78/100], Step [ 10/ 78], Loss: 0.5239\n",
            "Epoch [78/100], Step [ 11/ 78], Loss: 0.4913\n",
            "Epoch [78/100], Step [ 12/ 78], Loss: 0.5207\n",
            "Epoch [78/100], Step [ 13/ 78], Loss: 0.5491\n",
            "Epoch [78/100], Step [ 14/ 78], Loss: 0.5344\n",
            "Epoch [78/100], Step [ 15/ 78], Loss: 0.5504\n",
            "Epoch [78/100], Step [ 16/ 78], Loss: 0.5227\n",
            "Epoch [78/100], Step [ 17/ 78], Loss: 0.5113\n",
            "Epoch [78/100], Step [ 18/ 78], Loss: 0.5236\n",
            "Epoch [78/100], Step [ 19/ 78], Loss: 0.5570\n",
            "Epoch [78/100], Step [ 20/ 78], Loss: 0.5009\n",
            "Epoch [78/100], Step [ 21/ 78], Loss: 0.4950\n",
            "Epoch [78/100], Step [ 22/ 78], Loss: 0.5370\n",
            "Epoch [78/100], Step [ 23/ 78], Loss: 0.5567\n",
            "Epoch [78/100], Step [ 24/ 78], Loss: 0.5152\n",
            "Epoch [78/100], Step [ 25/ 78], Loss: 0.5080\n",
            "Epoch [78/100], Step [ 26/ 78], Loss: 0.5430\n",
            "Epoch [78/100], Step [ 27/ 78], Loss: 0.5195\n",
            "Epoch [78/100], Step [ 28/ 78], Loss: 0.5531\n",
            "Epoch [78/100], Step [ 29/ 78], Loss: 0.5616\n",
            "Epoch [78/100], Step [ 30/ 78], Loss: 0.5194\n",
            "Epoch [78/100], Step [ 31/ 78], Loss: 0.5452\n",
            "Epoch [78/100], Step [ 32/ 78], Loss: 0.5231\n",
            "Epoch [78/100], Step [ 33/ 78], Loss: 0.5187\n",
            "Epoch [78/100], Step [ 34/ 78], Loss: 0.5213\n",
            "Epoch [78/100], Step [ 35/ 78], Loss: 0.5214\n",
            "Epoch [78/100], Step [ 36/ 78], Loss: 0.5636\n",
            "Epoch [78/100], Step [ 37/ 78], Loss: 0.5304\n",
            "Epoch [78/100], Step [ 38/ 78], Loss: 0.5123\n",
            "Epoch [78/100], Step [ 39/ 78], Loss: 0.5307\n",
            "Epoch [78/100], Step [ 40/ 78], Loss: 0.5329\n",
            "Epoch [78/100], Step [ 41/ 78], Loss: 0.5428\n",
            "Epoch [78/100], Step [ 42/ 78], Loss: 0.5532\n",
            "Epoch [78/100], Step [ 43/ 78], Loss: 0.4880\n",
            "Epoch [78/100], Step [ 44/ 78], Loss: 0.4969\n",
            "Epoch [78/100], Step [ 45/ 78], Loss: 0.5248\n",
            "Epoch [78/100], Step [ 46/ 78], Loss: 0.5345\n",
            "Epoch [78/100], Step [ 47/ 78], Loss: 0.5053\n",
            "Epoch [78/100], Step [ 48/ 78], Loss: 0.5813\n",
            "Epoch [78/100], Step [ 49/ 78], Loss: 0.5338\n",
            "Epoch [78/100], Step [ 50/ 78], Loss: 0.5639\n",
            "Epoch [78/100], Step [ 51/ 78], Loss: 0.5708\n",
            "Epoch [78/100], Step [ 52/ 78], Loss: 0.5360\n",
            "Epoch [78/100], Step [ 53/ 78], Loss: 0.5228\n",
            "Epoch [78/100], Step [ 54/ 78], Loss: 0.5403\n",
            "Epoch [78/100], Step [ 55/ 78], Loss: 0.5334\n",
            "Epoch [78/100], Step [ 56/ 78], Loss: 0.5037\n",
            "Epoch [78/100], Step [ 57/ 78], Loss: 0.5465\n",
            "Epoch [78/100], Step [ 58/ 78], Loss: 0.5473\n",
            "Epoch [78/100], Step [ 59/ 78], Loss: 0.5483\n",
            "Epoch [78/100], Step [ 60/ 78], Loss: 0.5650\n",
            "Epoch [78/100], Step [ 61/ 78], Loss: 0.5381\n",
            "Epoch [78/100], Step [ 62/ 78], Loss: 0.5438\n",
            "Epoch [78/100], Step [ 63/ 78], Loss: 0.5144\n",
            "Epoch [79/100], Step [  1/ 78], Loss: 0.5289\n",
            "Epoch [79/100], Step [  2/ 78], Loss: 0.5178\n",
            "Epoch [79/100], Step [  3/ 78], Loss: 0.5559\n",
            "Epoch [79/100], Step [  4/ 78], Loss: 0.5150\n",
            "Epoch [79/100], Step [  5/ 78], Loss: 0.5391\n",
            "Epoch [79/100], Step [  6/ 78], Loss: 0.5638\n",
            "Epoch [79/100], Step [  7/ 78], Loss: 0.5399\n",
            "Epoch [79/100], Step [  8/ 78], Loss: 0.5355\n",
            "Epoch [79/100], Step [  9/ 78], Loss: 0.4984\n",
            "Epoch [79/100], Step [ 10/ 78], Loss: 0.4932\n",
            "Epoch [79/100], Step [ 11/ 78], Loss: 0.5454\n",
            "Epoch [79/100], Step [ 12/ 78], Loss: 0.5409\n",
            "Epoch [79/100], Step [ 13/ 78], Loss: 0.5474\n",
            "Epoch [79/100], Step [ 14/ 78], Loss: 0.5107\n",
            "Epoch [79/100], Step [ 15/ 78], Loss: 0.5257\n",
            "Epoch [79/100], Step [ 16/ 78], Loss: 0.5525\n",
            "Epoch [79/100], Step [ 17/ 78], Loss: 0.4909\n",
            "Epoch [79/100], Step [ 18/ 78], Loss: 0.5174\n",
            "Epoch [79/100], Step [ 19/ 78], Loss: 0.5046\n",
            "Epoch [79/100], Step [ 20/ 78], Loss: 0.5263\n",
            "Epoch [79/100], Step [ 21/ 78], Loss: 0.5651\n",
            "Epoch [79/100], Step [ 22/ 78], Loss: 0.5288\n",
            "Epoch [79/100], Step [ 23/ 78], Loss: 0.4964\n",
            "Epoch [79/100], Step [ 24/ 78], Loss: 0.5238\n",
            "Epoch [79/100], Step [ 25/ 78], Loss: 0.5164\n",
            "Epoch [79/100], Step [ 26/ 78], Loss: 0.5111\n",
            "Epoch [79/100], Step [ 27/ 78], Loss: 0.4920\n",
            "Epoch [79/100], Step [ 28/ 78], Loss: 0.5392\n",
            "Epoch [79/100], Step [ 29/ 78], Loss: 0.5575\n",
            "Epoch [79/100], Step [ 30/ 78], Loss: 0.5291\n",
            "Epoch [79/100], Step [ 31/ 78], Loss: 0.4948\n",
            "Epoch [79/100], Step [ 32/ 78], Loss: 0.5270\n",
            "Epoch [79/100], Step [ 33/ 78], Loss: 0.5291\n",
            "Epoch [79/100], Step [ 34/ 78], Loss: 0.4993\n",
            "Epoch [79/100], Step [ 35/ 78], Loss: 0.5424\n",
            "Epoch [79/100], Step [ 36/ 78], Loss: 0.5841\n",
            "Epoch [79/100], Step [ 37/ 78], Loss: 0.5610\n",
            "Epoch [79/100], Step [ 38/ 78], Loss: 0.5291\n",
            "Epoch [79/100], Step [ 39/ 78], Loss: 0.5047\n",
            "Epoch [79/100], Step [ 40/ 78], Loss: 0.4939\n",
            "Epoch [79/100], Step [ 41/ 78], Loss: 0.5007\n",
            "Epoch [79/100], Step [ 42/ 78], Loss: 0.5334\n",
            "Epoch [79/100], Step [ 43/ 78], Loss: 0.5390\n",
            "Epoch [79/100], Step [ 44/ 78], Loss: 0.5472\n",
            "Epoch [79/100], Step [ 45/ 78], Loss: 0.5389\n",
            "Epoch [79/100], Step [ 46/ 78], Loss: 0.5257\n",
            "Epoch [79/100], Step [ 47/ 78], Loss: 0.5418\n",
            "Epoch [79/100], Step [ 48/ 78], Loss: 0.5591\n",
            "Epoch [79/100], Step [ 49/ 78], Loss: 0.4970\n",
            "Epoch [79/100], Step [ 50/ 78], Loss: 0.5367\n",
            "Epoch [79/100], Step [ 51/ 78], Loss: 0.5572\n",
            "Epoch [79/100], Step [ 52/ 78], Loss: 0.5875\n",
            "Epoch [79/100], Step [ 53/ 78], Loss: 0.5506\n",
            "Epoch [79/100], Step [ 54/ 78], Loss: 0.5144\n",
            "Epoch [79/100], Step [ 55/ 78], Loss: 0.5277\n",
            "Epoch [79/100], Step [ 56/ 78], Loss: 0.5406\n",
            "Epoch [79/100], Step [ 57/ 78], Loss: 0.5442\n",
            "Epoch [79/100], Step [ 58/ 78], Loss: 0.5600\n",
            "Epoch [79/100], Step [ 59/ 78], Loss: 0.5453\n",
            "Epoch [79/100], Step [ 60/ 78], Loss: 0.5177\n",
            "Epoch [79/100], Step [ 61/ 78], Loss: 0.5747\n",
            "Epoch [79/100], Step [ 62/ 78], Loss: 0.5680\n",
            "Epoch [79/100], Step [ 63/ 78], Loss: 0.5397\n",
            "Epoch [80/100], Step [  1/ 78], Loss: 0.5410\n",
            "Epoch [80/100], Step [  2/ 78], Loss: 0.5168\n",
            "Epoch [80/100], Step [  3/ 78], Loss: 0.5020\n",
            "Epoch [80/100], Step [  4/ 78], Loss: 0.5253\n",
            "Epoch [80/100], Step [  5/ 78], Loss: 0.5520\n",
            "Epoch [80/100], Step [  6/ 78], Loss: 0.4894\n",
            "Epoch [80/100], Step [  7/ 78], Loss: 0.5217\n",
            "Epoch [80/100], Step [  8/ 78], Loss: 0.5079\n",
            "Epoch [80/100], Step [  9/ 78], Loss: 0.5607\n",
            "Epoch [80/100], Step [ 10/ 78], Loss: 0.5322\n",
            "Epoch [80/100], Step [ 11/ 78], Loss: 0.5408\n",
            "Epoch [80/100], Step [ 12/ 78], Loss: 0.5526\n",
            "Epoch [80/100], Step [ 13/ 78], Loss: 0.5010\n",
            "Epoch [80/100], Step [ 14/ 78], Loss: 0.5166\n",
            "Epoch [80/100], Step [ 15/ 78], Loss: 0.5120\n",
            "Epoch [80/100], Step [ 16/ 78], Loss: 0.5193\n",
            "Epoch [80/100], Step [ 17/ 78], Loss: 0.5044\n",
            "Epoch [80/100], Step [ 18/ 78], Loss: 0.5315\n",
            "Epoch [80/100], Step [ 19/ 78], Loss: 0.5246\n",
            "Epoch [80/100], Step [ 20/ 78], Loss: 0.5634\n",
            "Epoch [80/100], Step [ 21/ 78], Loss: 0.4988\n",
            "Epoch [80/100], Step [ 22/ 78], Loss: 0.4924\n",
            "Epoch [80/100], Step [ 23/ 78], Loss: 0.5324\n",
            "Epoch [80/100], Step [ 24/ 78], Loss: 0.5399\n",
            "Epoch [80/100], Step [ 25/ 78], Loss: 0.4928\n",
            "Epoch [80/100], Step [ 26/ 78], Loss: 0.4897\n",
            "Epoch [80/100], Step [ 27/ 78], Loss: 0.5100\n",
            "Epoch [80/100], Step [ 28/ 78], Loss: 0.5384\n",
            "Epoch [80/100], Step [ 29/ 78], Loss: 0.5527\n",
            "Epoch [80/100], Step [ 30/ 78], Loss: 0.5217\n",
            "Epoch [80/100], Step [ 31/ 78], Loss: 0.5132\n",
            "Epoch [80/100], Step [ 32/ 78], Loss: 0.5247\n",
            "Epoch [80/100], Step [ 33/ 78], Loss: 0.5201\n",
            "Epoch [80/100], Step [ 34/ 78], Loss: 0.5128\n",
            "Epoch [80/100], Step [ 35/ 78], Loss: 0.5211\n",
            "Epoch [80/100], Step [ 36/ 78], Loss: 0.5439\n",
            "Epoch [80/100], Step [ 37/ 78], Loss: 0.5045\n",
            "Epoch [80/100], Step [ 38/ 78], Loss: 0.5222\n",
            "Epoch [80/100], Step [ 39/ 78], Loss: 0.5242\n",
            "Epoch [80/100], Step [ 40/ 78], Loss: 0.5982\n",
            "Epoch [80/100], Step [ 41/ 78], Loss: 0.5828\n",
            "Epoch [80/100], Step [ 42/ 78], Loss: 0.5066\n",
            "Epoch [80/100], Step [ 43/ 78], Loss: 0.4853\n",
            "Epoch [80/100], Step [ 44/ 78], Loss: 0.5303\n",
            "Epoch [80/100], Step [ 45/ 78], Loss: 0.5313\n",
            "Epoch [80/100], Step [ 46/ 78], Loss: 0.5268\n",
            "Epoch [80/100], Step [ 47/ 78], Loss: 0.5278\n",
            "Epoch [80/100], Step [ 48/ 78], Loss: 0.5332\n",
            "Epoch [80/100], Step [ 49/ 78], Loss: 0.5610\n",
            "Epoch [80/100], Step [ 50/ 78], Loss: 0.5073\n",
            "Epoch [80/100], Step [ 51/ 78], Loss: 0.5194\n",
            "Epoch [80/100], Step [ 52/ 78], Loss: 0.5573\n",
            "Epoch [80/100], Step [ 53/ 78], Loss: 0.5001\n",
            "Epoch [80/100], Step [ 54/ 78], Loss: 0.5496\n",
            "Epoch [80/100], Step [ 55/ 78], Loss: 0.5573\n",
            "Epoch [80/100], Step [ 56/ 78], Loss: 0.5358\n",
            "Epoch [80/100], Step [ 57/ 78], Loss: 0.5367\n",
            "Epoch [80/100], Step [ 58/ 78], Loss: 0.5366\n",
            "Epoch [80/100], Step [ 59/ 78], Loss: 0.5366\n",
            "Epoch [80/100], Step [ 60/ 78], Loss: 0.4984\n",
            "Epoch [80/100], Step [ 61/ 78], Loss: 0.5303\n",
            "Epoch [80/100], Step [ 62/ 78], Loss: 0.5489\n",
            "Epoch [80/100], Step [ 63/ 78], Loss: 0.5304\n",
            "Epoch [81/100], Step [  1/ 78], Loss: 0.5278\n",
            "Epoch [81/100], Step [  2/ 78], Loss: 0.5032\n",
            "Epoch [81/100], Step [  3/ 78], Loss: 0.5439\n",
            "Epoch [81/100], Step [  4/ 78], Loss: 0.5414\n",
            "Epoch [81/100], Step [  5/ 78], Loss: 0.5340\n",
            "Epoch [81/100], Step [  6/ 78], Loss: 0.5375\n",
            "Epoch [81/100], Step [  7/ 78], Loss: 0.5662\n",
            "Epoch [81/100], Step [  8/ 78], Loss: 0.5241\n",
            "Epoch [81/100], Step [  9/ 78], Loss: 0.5659\n",
            "Epoch [81/100], Step [ 10/ 78], Loss: 0.5718\n",
            "Epoch [81/100], Step [ 11/ 78], Loss: 0.5515\n",
            "Epoch [81/100], Step [ 12/ 78], Loss: 0.5495\n",
            "Epoch [81/100], Step [ 13/ 78], Loss: 0.5156\n",
            "Epoch [81/100], Step [ 14/ 78], Loss: 0.5172\n",
            "Epoch [81/100], Step [ 15/ 78], Loss: 0.5129\n",
            "Epoch [81/100], Step [ 16/ 78], Loss: 0.5392\n",
            "Epoch [81/100], Step [ 17/ 78], Loss: 0.5411\n",
            "Epoch [81/100], Step [ 18/ 78], Loss: 0.5249\n",
            "Epoch [81/100], Step [ 19/ 78], Loss: 0.5157\n",
            "Epoch [81/100], Step [ 20/ 78], Loss: 0.5310\n",
            "Epoch [81/100], Step [ 21/ 78], Loss: 0.5431\n",
            "Epoch [81/100], Step [ 22/ 78], Loss: 0.5295\n",
            "Epoch [81/100], Step [ 23/ 78], Loss: 0.5642\n",
            "Epoch [81/100], Step [ 24/ 78], Loss: 0.4840\n",
            "Epoch [81/100], Step [ 25/ 78], Loss: 0.5200\n",
            "Epoch [81/100], Step [ 26/ 78], Loss: 0.5248\n",
            "Epoch [81/100], Step [ 27/ 78], Loss: 0.5508\n",
            "Epoch [81/100], Step [ 28/ 78], Loss: 0.5603\n",
            "Epoch [81/100], Step [ 29/ 78], Loss: 0.5324\n",
            "Epoch [81/100], Step [ 30/ 78], Loss: 0.5115\n",
            "Epoch [81/100], Step [ 31/ 78], Loss: 0.5324\n",
            "Epoch [81/100], Step [ 32/ 78], Loss: 0.5008\n",
            "Epoch [81/100], Step [ 33/ 78], Loss: 0.5128\n",
            "Epoch [81/100], Step [ 34/ 78], Loss: 0.4989\n",
            "Epoch [81/100], Step [ 35/ 78], Loss: 0.5296\n",
            "Epoch [81/100], Step [ 36/ 78], Loss: 0.5054\n",
            "Epoch [81/100], Step [ 37/ 78], Loss: 0.4838\n",
            "Epoch [81/100], Step [ 38/ 78], Loss: 0.5381\n",
            "Epoch [81/100], Step [ 39/ 78], Loss: 0.5367\n",
            "Epoch [81/100], Step [ 40/ 78], Loss: 0.5508\n",
            "Epoch [81/100], Step [ 41/ 78], Loss: 0.5323\n",
            "Epoch [81/100], Step [ 42/ 78], Loss: 0.5413\n",
            "Epoch [81/100], Step [ 43/ 78], Loss: 0.5245\n",
            "Epoch [81/100], Step [ 44/ 78], Loss: 0.5662\n",
            "Epoch [81/100], Step [ 45/ 78], Loss: 0.5404\n",
            "Epoch [81/100], Step [ 46/ 78], Loss: 0.5135\n",
            "Epoch [81/100], Step [ 47/ 78], Loss: 0.4907\n",
            "Epoch [81/100], Step [ 48/ 78], Loss: 0.5428\n",
            "Epoch [81/100], Step [ 49/ 78], Loss: 0.5095\n",
            "Epoch [81/100], Step [ 50/ 78], Loss: 0.5403\n",
            "Epoch [81/100], Step [ 51/ 78], Loss: 0.4735\n",
            "Epoch [81/100], Step [ 52/ 78], Loss: 0.5280\n",
            "Epoch [81/100], Step [ 53/ 78], Loss: 0.5005\n",
            "Epoch [81/100], Step [ 54/ 78], Loss: 0.5243\n",
            "Epoch [81/100], Step [ 55/ 78], Loss: 0.5028\n",
            "Epoch [81/100], Step [ 56/ 78], Loss: 0.5356\n",
            "Epoch [81/100], Step [ 57/ 78], Loss: 0.5349\n",
            "Epoch [81/100], Step [ 58/ 78], Loss: 0.5059\n",
            "Epoch [81/100], Step [ 59/ 78], Loss: 0.5540\n",
            "Epoch [81/100], Step [ 60/ 78], Loss: 0.5092\n",
            "Epoch [81/100], Step [ 61/ 78], Loss: 0.5201\n",
            "Epoch [81/100], Step [ 62/ 78], Loss: 0.5574\n",
            "Epoch [81/100], Step [ 63/ 78], Loss: 0.5587\n",
            "Epoch [82/100], Step [  1/ 78], Loss: 0.5635\n",
            "Epoch [82/100], Step [  2/ 78], Loss: 0.5160\n",
            "Epoch [82/100], Step [  3/ 78], Loss: 0.5055\n",
            "Epoch [82/100], Step [  4/ 78], Loss: 0.5039\n",
            "Epoch [82/100], Step [  5/ 78], Loss: 0.5276\n",
            "Epoch [82/100], Step [  6/ 78], Loss: 0.5798\n",
            "Epoch [82/100], Step [  7/ 78], Loss: 0.4990\n",
            "Epoch [82/100], Step [  8/ 78], Loss: 0.5111\n",
            "Epoch [82/100], Step [  9/ 78], Loss: 0.5175\n",
            "Epoch [82/100], Step [ 10/ 78], Loss: 0.5060\n",
            "Epoch [82/100], Step [ 11/ 78], Loss: 0.5133\n",
            "Epoch [82/100], Step [ 12/ 78], Loss: 0.5273\n",
            "Epoch [82/100], Step [ 13/ 78], Loss: 0.4987\n",
            "Epoch [82/100], Step [ 14/ 78], Loss: 0.5230\n",
            "Epoch [82/100], Step [ 15/ 78], Loss: 0.5102\n",
            "Epoch [82/100], Step [ 16/ 78], Loss: 0.5409\n",
            "Epoch [82/100], Step [ 17/ 78], Loss: 0.5184\n",
            "Epoch [82/100], Step [ 18/ 78], Loss: 0.5075\n",
            "Epoch [82/100], Step [ 19/ 78], Loss: 0.5217\n",
            "Epoch [82/100], Step [ 20/ 78], Loss: 0.5252\n",
            "Epoch [82/100], Step [ 21/ 78], Loss: 0.5249\n",
            "Epoch [82/100], Step [ 22/ 78], Loss: 0.5623\n",
            "Epoch [82/100], Step [ 23/ 78], Loss: 0.5377\n",
            "Epoch [82/100], Step [ 24/ 78], Loss: 0.4803\n",
            "Epoch [82/100], Step [ 25/ 78], Loss: 0.5441\n",
            "Epoch [82/100], Step [ 26/ 78], Loss: 0.5126\n",
            "Epoch [82/100], Step [ 27/ 78], Loss: 0.5555\n",
            "Epoch [82/100], Step [ 28/ 78], Loss: 0.5240\n",
            "Epoch [82/100], Step [ 29/ 78], Loss: 0.5417\n",
            "Epoch [82/100], Step [ 30/ 78], Loss: 0.5660\n",
            "Epoch [82/100], Step [ 31/ 78], Loss: 0.5588\n",
            "Epoch [82/100], Step [ 32/ 78], Loss: 0.5442\n",
            "Epoch [82/100], Step [ 33/ 78], Loss: 0.5400\n",
            "Epoch [82/100], Step [ 34/ 78], Loss: 0.5267\n",
            "Epoch [82/100], Step [ 35/ 78], Loss: 0.5149\n",
            "Epoch [82/100], Step [ 36/ 78], Loss: 0.5188\n",
            "Epoch [82/100], Step [ 37/ 78], Loss: 0.5110\n",
            "Epoch [82/100], Step [ 38/ 78], Loss: 0.5322\n",
            "Epoch [82/100], Step [ 39/ 78], Loss: 0.5568\n",
            "Epoch [82/100], Step [ 40/ 78], Loss: 0.5486\n",
            "Epoch [82/100], Step [ 41/ 78], Loss: 0.5479\n",
            "Epoch [82/100], Step [ 42/ 78], Loss: 0.5524\n",
            "Epoch [82/100], Step [ 43/ 78], Loss: 0.5437\n",
            "Epoch [82/100], Step [ 44/ 78], Loss: 0.5828\n",
            "Epoch [82/100], Step [ 45/ 78], Loss: 0.5363\n",
            "Epoch [82/100], Step [ 46/ 78], Loss: 0.5174\n",
            "Epoch [82/100], Step [ 47/ 78], Loss: 0.4923\n",
            "Epoch [82/100], Step [ 48/ 78], Loss: 0.4946\n",
            "Epoch [82/100], Step [ 49/ 78], Loss: 0.5141\n",
            "Epoch [82/100], Step [ 50/ 78], Loss: 0.5468\n",
            "Epoch [82/100], Step [ 51/ 78], Loss: 0.5807\n",
            "Epoch [82/100], Step [ 52/ 78], Loss: 0.5170\n",
            "Epoch [82/100], Step [ 53/ 78], Loss: 0.5172\n",
            "Epoch [82/100], Step [ 54/ 78], Loss: 0.5177\n",
            "Epoch [82/100], Step [ 55/ 78], Loss: 0.5408\n",
            "Epoch [82/100], Step [ 56/ 78], Loss: 0.4955\n",
            "Epoch [82/100], Step [ 57/ 78], Loss: 0.5051\n",
            "Epoch [82/100], Step [ 58/ 78], Loss: 0.5052\n",
            "Epoch [82/100], Step [ 59/ 78], Loss: 0.5395\n",
            "Epoch [82/100], Step [ 60/ 78], Loss: 0.5359\n",
            "Epoch [82/100], Step [ 61/ 78], Loss: 0.5740\n",
            "Epoch [82/100], Step [ 62/ 78], Loss: 0.5795\n",
            "Epoch [82/100], Step [ 63/ 78], Loss: 0.5648\n",
            "Epoch [83/100], Step [  1/ 78], Loss: 0.5822\n",
            "Epoch [83/100], Step [  2/ 78], Loss: 0.5393\n",
            "Epoch [83/100], Step [  3/ 78], Loss: 0.5277\n",
            "Epoch [83/100], Step [  4/ 78], Loss: 0.5683\n",
            "Epoch [83/100], Step [  5/ 78], Loss: 0.5167\n",
            "Epoch [83/100], Step [  6/ 78], Loss: 0.5266\n",
            "Epoch [83/100], Step [  7/ 78], Loss: 0.5578\n",
            "Epoch [83/100], Step [  8/ 78], Loss: 0.5071\n",
            "Epoch [83/100], Step [  9/ 78], Loss: 0.5040\n",
            "Epoch [83/100], Step [ 10/ 78], Loss: 0.5112\n",
            "Epoch [83/100], Step [ 11/ 78], Loss: 0.5301\n",
            "Epoch [83/100], Step [ 12/ 78], Loss: 0.5334\n",
            "Epoch [83/100], Step [ 13/ 78], Loss: 0.5336\n",
            "Epoch [83/100], Step [ 14/ 78], Loss: 0.5399\n",
            "Epoch [83/100], Step [ 15/ 78], Loss: 0.5260\n",
            "Epoch [83/100], Step [ 16/ 78], Loss: 0.5609\n",
            "Epoch [83/100], Step [ 17/ 78], Loss: 0.5194\n",
            "Epoch [83/100], Step [ 18/ 78], Loss: 0.5397\n",
            "Epoch [83/100], Step [ 19/ 78], Loss: 0.5324\n",
            "Epoch [83/100], Step [ 20/ 78], Loss: 0.5266\n",
            "Epoch [83/100], Step [ 21/ 78], Loss: 0.4846\n",
            "Epoch [83/100], Step [ 22/ 78], Loss: 0.5466\n",
            "Epoch [83/100], Step [ 23/ 78], Loss: 0.5295\n",
            "Epoch [83/100], Step [ 24/ 78], Loss: 0.5364\n",
            "Epoch [83/100], Step [ 25/ 78], Loss: 0.5434\n",
            "Epoch [83/100], Step [ 26/ 78], Loss: 0.5159\n",
            "Epoch [83/100], Step [ 27/ 78], Loss: 0.5191\n",
            "Epoch [83/100], Step [ 28/ 78], Loss: 0.5202\n",
            "Epoch [83/100], Step [ 29/ 78], Loss: 0.4970\n",
            "Epoch [83/100], Step [ 30/ 78], Loss: 0.5338\n",
            "Epoch [83/100], Step [ 31/ 78], Loss: 0.5033\n",
            "Epoch [83/100], Step [ 32/ 78], Loss: 0.5399\n",
            "Epoch [83/100], Step [ 33/ 78], Loss: 0.5315\n",
            "Epoch [83/100], Step [ 34/ 78], Loss: 0.5370\n",
            "Epoch [83/100], Step [ 35/ 78], Loss: 0.5145\n",
            "Epoch [83/100], Step [ 36/ 78], Loss: 0.5117\n",
            "Epoch [83/100], Step [ 37/ 78], Loss: 0.5264\n",
            "Epoch [83/100], Step [ 38/ 78], Loss: 0.5260\n",
            "Epoch [83/100], Step [ 39/ 78], Loss: 0.5477\n",
            "Epoch [83/100], Step [ 40/ 78], Loss: 0.5474\n",
            "Epoch [83/100], Step [ 41/ 78], Loss: 0.4777\n",
            "Epoch [83/100], Step [ 42/ 78], Loss: 0.5259\n",
            "Epoch [83/100], Step [ 43/ 78], Loss: 0.5214\n",
            "Epoch [83/100], Step [ 44/ 78], Loss: 0.5070\n",
            "Epoch [83/100], Step [ 45/ 78], Loss: 0.5416\n",
            "Epoch [83/100], Step [ 46/ 78], Loss: 0.5234\n",
            "Epoch [83/100], Step [ 47/ 78], Loss: 0.5285\n",
            "Epoch [83/100], Step [ 48/ 78], Loss: 0.4944\n",
            "Epoch [83/100], Step [ 49/ 78], Loss: 0.5057\n",
            "Epoch [83/100], Step [ 50/ 78], Loss: 0.5033\n",
            "Epoch [83/100], Step [ 51/ 78], Loss: 0.5239\n",
            "Epoch [83/100], Step [ 52/ 78], Loss: 0.5315\n",
            "Epoch [83/100], Step [ 53/ 78], Loss: 0.4937\n",
            "Epoch [83/100], Step [ 54/ 78], Loss: 0.5166\n",
            "Epoch [83/100], Step [ 55/ 78], Loss: 0.5185\n",
            "Epoch [83/100], Step [ 56/ 78], Loss: 0.5418\n",
            "Epoch [83/100], Step [ 57/ 78], Loss: 0.4904\n",
            "Epoch [83/100], Step [ 58/ 78], Loss: 0.5176\n",
            "Epoch [83/100], Step [ 59/ 78], Loss: 0.5088\n",
            "Epoch [83/100], Step [ 60/ 78], Loss: 0.5264\n",
            "Epoch [83/100], Step [ 61/ 78], Loss: 0.5281\n",
            "Epoch [83/100], Step [ 62/ 78], Loss: 0.5252\n",
            "Epoch [83/100], Step [ 63/ 78], Loss: 0.5466\n",
            "Epoch [84/100], Step [  1/ 78], Loss: 0.5241\n",
            "Epoch [84/100], Step [  2/ 78], Loss: 0.5067\n",
            "Epoch [84/100], Step [  3/ 78], Loss: 0.5428\n",
            "Epoch [84/100], Step [  4/ 78], Loss: 0.5502\n",
            "Epoch [84/100], Step [  5/ 78], Loss: 0.5263\n",
            "Epoch [84/100], Step [  6/ 78], Loss: 0.5312\n",
            "Epoch [84/100], Step [  7/ 78], Loss: 0.5103\n",
            "Epoch [84/100], Step [  8/ 78], Loss: 0.4883\n",
            "Epoch [84/100], Step [  9/ 78], Loss: 0.5157\n",
            "Epoch [84/100], Step [ 10/ 78], Loss: 0.5183\n",
            "Epoch [84/100], Step [ 11/ 78], Loss: 0.5436\n",
            "Epoch [84/100], Step [ 12/ 78], Loss: 0.5396\n",
            "Epoch [84/100], Step [ 13/ 78], Loss: 0.5043\n",
            "Epoch [84/100], Step [ 14/ 78], Loss: 0.5701\n",
            "Epoch [84/100], Step [ 15/ 78], Loss: 0.5212\n",
            "Epoch [84/100], Step [ 16/ 78], Loss: 0.5499\n",
            "Epoch [84/100], Step [ 17/ 78], Loss: 0.5397\n",
            "Epoch [84/100], Step [ 18/ 78], Loss: 0.5487\n",
            "Epoch [84/100], Step [ 19/ 78], Loss: 0.4903\n",
            "Epoch [84/100], Step [ 20/ 78], Loss: 0.5153\n",
            "Epoch [84/100], Step [ 21/ 78], Loss: 0.5426\n",
            "Epoch [84/100], Step [ 22/ 78], Loss: 0.5620\n",
            "Epoch [84/100], Step [ 23/ 78], Loss: 0.5649\n",
            "Epoch [84/100], Step [ 24/ 78], Loss: 0.5426\n",
            "Epoch [84/100], Step [ 25/ 78], Loss: 0.5170\n",
            "Epoch [84/100], Step [ 26/ 78], Loss: 0.4940\n",
            "Epoch [84/100], Step [ 27/ 78], Loss: 0.5036\n",
            "Epoch [84/100], Step [ 28/ 78], Loss: 0.4727\n",
            "Epoch [84/100], Step [ 29/ 78], Loss: 0.5282\n",
            "Epoch [84/100], Step [ 30/ 78], Loss: 0.5431\n",
            "Epoch [84/100], Step [ 31/ 78], Loss: 0.5465\n",
            "Epoch [84/100], Step [ 32/ 78], Loss: 0.5307\n",
            "Epoch [84/100], Step [ 33/ 78], Loss: 0.5395\n",
            "Epoch [84/100], Step [ 34/ 78], Loss: 0.5731\n",
            "Epoch [84/100], Step [ 35/ 78], Loss: 0.5078\n",
            "Epoch [84/100], Step [ 36/ 78], Loss: 0.5106\n",
            "Epoch [84/100], Step [ 37/ 78], Loss: 0.4949\n",
            "Epoch [84/100], Step [ 38/ 78], Loss: 0.5260\n",
            "Epoch [84/100], Step [ 39/ 78], Loss: 0.5372\n",
            "Epoch [84/100], Step [ 40/ 78], Loss: 0.5392\n",
            "Epoch [84/100], Step [ 41/ 78], Loss: 0.5044\n",
            "Epoch [84/100], Step [ 42/ 78], Loss: 0.5466\n",
            "Epoch [84/100], Step [ 43/ 78], Loss: 0.5030\n",
            "Epoch [84/100], Step [ 44/ 78], Loss: 0.5081\n",
            "Epoch [84/100], Step [ 45/ 78], Loss: 0.5211\n",
            "Epoch [84/100], Step [ 46/ 78], Loss: 0.5342\n",
            "Epoch [84/100], Step [ 47/ 78], Loss: 0.5321\n",
            "Epoch [84/100], Step [ 48/ 78], Loss: 0.5363\n",
            "Epoch [84/100], Step [ 49/ 78], Loss: 0.5248\n",
            "Epoch [84/100], Step [ 50/ 78], Loss: 0.5542\n",
            "Epoch [84/100], Step [ 51/ 78], Loss: 0.5321\n",
            "Epoch [84/100], Step [ 52/ 78], Loss: 0.5592\n",
            "Epoch [84/100], Step [ 53/ 78], Loss: 0.5167\n",
            "Epoch [84/100], Step [ 54/ 78], Loss: 0.5241\n",
            "Epoch [84/100], Step [ 55/ 78], Loss: 0.5510\n",
            "Epoch [84/100], Step [ 56/ 78], Loss: 0.5383\n",
            "Epoch [84/100], Step [ 57/ 78], Loss: 0.5156\n",
            "Epoch [84/100], Step [ 58/ 78], Loss: 0.5196\n",
            "Epoch [84/100], Step [ 59/ 78], Loss: 0.5121\n",
            "Epoch [84/100], Step [ 60/ 78], Loss: 0.5490\n",
            "Epoch [84/100], Step [ 61/ 78], Loss: 0.5136\n",
            "Epoch [84/100], Step [ 62/ 78], Loss: 0.5170\n",
            "Epoch [84/100], Step [ 63/ 78], Loss: 0.5367\n",
            "Epoch [85/100], Step [  1/ 78], Loss: 0.6003\n",
            "Epoch [85/100], Step [  2/ 78], Loss: 0.5293\n",
            "Epoch [85/100], Step [  3/ 78], Loss: 0.5694\n",
            "Epoch [85/100], Step [  4/ 78], Loss: 0.5222\n",
            "Epoch [85/100], Step [  5/ 78], Loss: 0.5507\n",
            "Epoch [85/100], Step [  6/ 78], Loss: 0.5076\n",
            "Epoch [85/100], Step [  7/ 78], Loss: 0.5165\n",
            "Epoch [85/100], Step [  8/ 78], Loss: 0.5547\n",
            "Epoch [85/100], Step [  9/ 78], Loss: 0.5200\n",
            "Epoch [85/100], Step [ 10/ 78], Loss: 0.5407\n",
            "Epoch [85/100], Step [ 11/ 78], Loss: 0.5577\n",
            "Epoch [85/100], Step [ 12/ 78], Loss: 0.5392\n",
            "Epoch [85/100], Step [ 13/ 78], Loss: 0.5421\n",
            "Epoch [85/100], Step [ 14/ 78], Loss: 0.5243\n",
            "Epoch [85/100], Step [ 15/ 78], Loss: 0.5198\n",
            "Epoch [85/100], Step [ 16/ 78], Loss: 0.5442\n",
            "Epoch [85/100], Step [ 17/ 78], Loss: 0.5069\n",
            "Epoch [85/100], Step [ 18/ 78], Loss: 0.5130\n",
            "Epoch [85/100], Step [ 19/ 78], Loss: 0.5112\n",
            "Epoch [85/100], Step [ 20/ 78], Loss: 0.5222\n",
            "Epoch [85/100], Step [ 21/ 78], Loss: 0.5342\n",
            "Epoch [85/100], Step [ 22/ 78], Loss: 0.5380\n",
            "Epoch [85/100], Step [ 23/ 78], Loss: 0.5507\n",
            "Epoch [85/100], Step [ 24/ 78], Loss: 0.4797\n",
            "Epoch [85/100], Step [ 25/ 78], Loss: 0.4969\n",
            "Epoch [85/100], Step [ 26/ 78], Loss: 0.5236\n",
            "Epoch [85/100], Step [ 27/ 78], Loss: 0.5443\n",
            "Epoch [85/100], Step [ 28/ 78], Loss: 0.5128\n",
            "Epoch [85/100], Step [ 29/ 78], Loss: 0.5313\n",
            "Epoch [85/100], Step [ 30/ 78], Loss: 0.5001\n",
            "Epoch [85/100], Step [ 31/ 78], Loss: 0.5143\n",
            "Epoch [85/100], Step [ 32/ 78], Loss: 0.5253\n",
            "Epoch [85/100], Step [ 33/ 78], Loss: 0.4964\n",
            "Epoch [85/100], Step [ 34/ 78], Loss: 0.5236\n",
            "Epoch [85/100], Step [ 35/ 78], Loss: 0.5571\n",
            "Epoch [85/100], Step [ 36/ 78], Loss: 0.5969\n",
            "Epoch [85/100], Step [ 37/ 78], Loss: 0.5228\n",
            "Epoch [85/100], Step [ 38/ 78], Loss: 0.5402\n",
            "Epoch [85/100], Step [ 39/ 78], Loss: 0.5275\n",
            "Epoch [85/100], Step [ 40/ 78], Loss: 0.4874\n",
            "Epoch [85/100], Step [ 41/ 78], Loss: 0.5337\n",
            "Epoch [85/100], Step [ 42/ 78], Loss: 0.5289\n",
            "Epoch [85/100], Step [ 43/ 78], Loss: 0.5077\n",
            "Epoch [85/100], Step [ 44/ 78], Loss: 0.5062\n",
            "Epoch [85/100], Step [ 45/ 78], Loss: 0.5334\n",
            "Epoch [85/100], Step [ 46/ 78], Loss: 0.5114\n",
            "Epoch [85/100], Step [ 47/ 78], Loss: 0.5176\n",
            "Epoch [85/100], Step [ 48/ 78], Loss: 0.5205\n",
            "Epoch [85/100], Step [ 49/ 78], Loss: 0.5423\n",
            "Epoch [85/100], Step [ 50/ 78], Loss: 0.4987\n",
            "Epoch [85/100], Step [ 51/ 78], Loss: 0.4795\n",
            "Epoch [85/100], Step [ 52/ 78], Loss: 0.5228\n",
            "Epoch [85/100], Step [ 53/ 78], Loss: 0.5430\n",
            "Epoch [85/100], Step [ 54/ 78], Loss: 0.5587\n",
            "Epoch [85/100], Step [ 55/ 78], Loss: 0.5243\n",
            "Epoch [85/100], Step [ 56/ 78], Loss: 0.5487\n",
            "Epoch [85/100], Step [ 57/ 78], Loss: 0.5583\n",
            "Epoch [85/100], Step [ 58/ 78], Loss: 0.5857\n",
            "Epoch [85/100], Step [ 59/ 78], Loss: 0.5272\n",
            "Epoch [85/100], Step [ 60/ 78], Loss: 0.5995\n",
            "Epoch [85/100], Step [ 61/ 78], Loss: 0.5447\n",
            "Epoch [85/100], Step [ 62/ 78], Loss: 0.5326\n",
            "Epoch [85/100], Step [ 63/ 78], Loss: 0.5942\n",
            "Epoch [86/100], Step [  1/ 78], Loss: 0.5472\n",
            "Epoch [86/100], Step [  2/ 78], Loss: 0.5239\n",
            "Epoch [86/100], Step [  3/ 78], Loss: 0.5462\n",
            "Epoch [86/100], Step [  4/ 78], Loss: 0.5351\n",
            "Epoch [86/100], Step [  5/ 78], Loss: 0.5424\n",
            "Epoch [86/100], Step [  6/ 78], Loss: 0.5010\n",
            "Epoch [86/100], Step [  7/ 78], Loss: 0.5219\n",
            "Epoch [86/100], Step [  8/ 78], Loss: 0.5083\n",
            "Epoch [86/100], Step [  9/ 78], Loss: 0.5495\n",
            "Epoch [86/100], Step [ 10/ 78], Loss: 0.4796\n",
            "Epoch [86/100], Step [ 11/ 78], Loss: 0.5330\n",
            "Epoch [86/100], Step [ 12/ 78], Loss: 0.4952\n",
            "Epoch [86/100], Step [ 13/ 78], Loss: 0.5202\n",
            "Epoch [86/100], Step [ 14/ 78], Loss: 0.5629\n",
            "Epoch [86/100], Step [ 15/ 78], Loss: 0.4970\n",
            "Epoch [86/100], Step [ 16/ 78], Loss: 0.5330\n",
            "Epoch [86/100], Step [ 17/ 78], Loss: 0.5043\n",
            "Epoch [86/100], Step [ 18/ 78], Loss: 0.5383\n",
            "Epoch [86/100], Step [ 19/ 78], Loss: 0.5023\n",
            "Epoch [86/100], Step [ 20/ 78], Loss: 0.5267\n",
            "Epoch [86/100], Step [ 21/ 78], Loss: 0.5091\n",
            "Epoch [86/100], Step [ 22/ 78], Loss: 0.5186\n",
            "Epoch [86/100], Step [ 23/ 78], Loss: 0.5416\n",
            "Epoch [86/100], Step [ 24/ 78], Loss: 0.5093\n",
            "Epoch [86/100], Step [ 25/ 78], Loss: 0.5139\n",
            "Epoch [86/100], Step [ 26/ 78], Loss: 0.5420\n",
            "Epoch [86/100], Step [ 27/ 78], Loss: 0.5597\n",
            "Epoch [86/100], Step [ 28/ 78], Loss: 0.5323\n",
            "Epoch [86/100], Step [ 29/ 78], Loss: 0.5415\n",
            "Epoch [86/100], Step [ 30/ 78], Loss: 0.5471\n",
            "Epoch [86/100], Step [ 31/ 78], Loss: 0.5124\n",
            "Epoch [86/100], Step [ 32/ 78], Loss: 0.5043\n",
            "Epoch [86/100], Step [ 33/ 78], Loss: 0.5090\n",
            "Epoch [86/100], Step [ 34/ 78], Loss: 0.5498\n",
            "Epoch [86/100], Step [ 35/ 78], Loss: 0.5191\n",
            "Epoch [86/100], Step [ 36/ 78], Loss: 0.5499\n",
            "Epoch [86/100], Step [ 37/ 78], Loss: 0.5374\n",
            "Epoch [86/100], Step [ 38/ 78], Loss: 0.5195\n",
            "Epoch [86/100], Step [ 39/ 78], Loss: 0.5194\n",
            "Epoch [86/100], Step [ 40/ 78], Loss: 0.5121\n",
            "Epoch [86/100], Step [ 41/ 78], Loss: 0.5534\n",
            "Epoch [86/100], Step [ 42/ 78], Loss: 0.5185\n",
            "Epoch [86/100], Step [ 43/ 78], Loss: 0.5345\n",
            "Epoch [86/100], Step [ 44/ 78], Loss: 0.5074\n",
            "Epoch [86/100], Step [ 45/ 78], Loss: 0.5705\n",
            "Epoch [86/100], Step [ 46/ 78], Loss: 0.5889\n",
            "Epoch [86/100], Step [ 47/ 78], Loss: 0.5143\n",
            "Epoch [86/100], Step [ 48/ 78], Loss: 0.5212\n",
            "Epoch [86/100], Step [ 49/ 78], Loss: 0.5412\n",
            "Epoch [86/100], Step [ 50/ 78], Loss: 0.5130\n",
            "Epoch [86/100], Step [ 51/ 78], Loss: 0.5188\n",
            "Epoch [86/100], Step [ 52/ 78], Loss: 0.4877\n",
            "Epoch [86/100], Step [ 53/ 78], Loss: 0.4772\n",
            "Epoch [86/100], Step [ 54/ 78], Loss: 0.4874\n",
            "Epoch [86/100], Step [ 55/ 78], Loss: 0.5113\n",
            "Epoch [86/100], Step [ 56/ 78], Loss: 0.5194\n",
            "Epoch [86/100], Step [ 57/ 78], Loss: 0.5344\n",
            "Epoch [86/100], Step [ 58/ 78], Loss: 0.5218\n",
            "Epoch [86/100], Step [ 59/ 78], Loss: 0.5378\n",
            "Epoch [86/100], Step [ 60/ 78], Loss: 0.5287\n",
            "Epoch [86/100], Step [ 61/ 78], Loss: 0.5205\n",
            "Epoch [86/100], Step [ 62/ 78], Loss: 0.5451\n",
            "Epoch [86/100], Step [ 63/ 78], Loss: 0.5582\n",
            "Epoch [87/100], Step [  1/ 78], Loss: 0.4880\n",
            "Epoch [87/100], Step [  2/ 78], Loss: 0.5073\n",
            "Epoch [87/100], Step [  3/ 78], Loss: 0.5350\n",
            "Epoch [87/100], Step [  4/ 78], Loss: 0.5429\n",
            "Epoch [87/100], Step [  5/ 78], Loss: 0.5411\n",
            "Epoch [87/100], Step [  6/ 78], Loss: 0.5344\n",
            "Epoch [87/100], Step [  7/ 78], Loss: 0.5064\n",
            "Epoch [87/100], Step [  8/ 78], Loss: 0.5084\n",
            "Epoch [87/100], Step [  9/ 78], Loss: 0.5406\n",
            "Epoch [87/100], Step [ 10/ 78], Loss: 0.5134\n",
            "Epoch [87/100], Step [ 11/ 78], Loss: 0.4882\n",
            "Epoch [87/100], Step [ 12/ 78], Loss: 0.5062\n",
            "Epoch [87/100], Step [ 13/ 78], Loss: 0.5454\n",
            "Epoch [87/100], Step [ 14/ 78], Loss: 0.5343\n",
            "Epoch [87/100], Step [ 15/ 78], Loss: 0.4930\n",
            "Epoch [87/100], Step [ 16/ 78], Loss: 0.5191\n",
            "Epoch [87/100], Step [ 17/ 78], Loss: 0.4993\n",
            "Epoch [87/100], Step [ 18/ 78], Loss: 0.5550\n",
            "Epoch [87/100], Step [ 19/ 78], Loss: 0.5992\n",
            "Epoch [87/100], Step [ 20/ 78], Loss: 0.5428\n",
            "Epoch [87/100], Step [ 21/ 78], Loss: 0.5393\n",
            "Epoch [87/100], Step [ 22/ 78], Loss: 0.5380\n",
            "Epoch [87/100], Step [ 23/ 78], Loss: 0.5369\n",
            "Epoch [87/100], Step [ 24/ 78], Loss: 0.5166\n",
            "Epoch [87/100], Step [ 25/ 78], Loss: 0.5243\n",
            "Epoch [87/100], Step [ 26/ 78], Loss: 0.5697\n",
            "Epoch [87/100], Step [ 27/ 78], Loss: 0.5592\n",
            "Epoch [87/100], Step [ 28/ 78], Loss: 0.5384\n",
            "Epoch [87/100], Step [ 29/ 78], Loss: 0.5429\n",
            "Epoch [87/100], Step [ 30/ 78], Loss: 0.5203\n",
            "Epoch [87/100], Step [ 31/ 78], Loss: 0.4932\n",
            "Epoch [87/100], Step [ 32/ 78], Loss: 0.4963\n",
            "Epoch [87/100], Step [ 33/ 78], Loss: 0.5267\n",
            "Epoch [87/100], Step [ 34/ 78], Loss: 0.5106\n",
            "Epoch [87/100], Step [ 35/ 78], Loss: 0.5185\n",
            "Epoch [87/100], Step [ 36/ 78], Loss: 0.5159\n",
            "Epoch [87/100], Step [ 37/ 78], Loss: 0.5215\n",
            "Epoch [87/100], Step [ 38/ 78], Loss: 0.5296\n",
            "Epoch [87/100], Step [ 39/ 78], Loss: 0.4853\n",
            "Epoch [87/100], Step [ 40/ 78], Loss: 0.5009\n",
            "Epoch [87/100], Step [ 41/ 78], Loss: 0.5061\n",
            "Epoch [87/100], Step [ 42/ 78], Loss: 0.5574\n",
            "Epoch [87/100], Step [ 43/ 78], Loss: 0.5267\n",
            "Epoch [87/100], Step [ 44/ 78], Loss: 0.5466\n",
            "Epoch [87/100], Step [ 45/ 78], Loss: 0.5237\n",
            "Epoch [87/100], Step [ 46/ 78], Loss: 0.5191\n",
            "Epoch [87/100], Step [ 47/ 78], Loss: 0.5335\n",
            "Epoch [87/100], Step [ 48/ 78], Loss: 0.5533\n",
            "Epoch [87/100], Step [ 49/ 78], Loss: 0.4872\n",
            "Epoch [87/100], Step [ 50/ 78], Loss: 0.5378\n",
            "Epoch [87/100], Step [ 51/ 78], Loss: 0.5274\n",
            "Epoch [87/100], Step [ 52/ 78], Loss: 0.5076\n",
            "Epoch [87/100], Step [ 53/ 78], Loss: 0.5241\n",
            "Epoch [87/100], Step [ 54/ 78], Loss: 0.4925\n",
            "Epoch [87/100], Step [ 55/ 78], Loss: 0.5129\n",
            "Epoch [87/100], Step [ 56/ 78], Loss: 0.5457\n",
            "Epoch [87/100], Step [ 57/ 78], Loss: 0.5453\n",
            "Epoch [87/100], Step [ 58/ 78], Loss: 0.5390\n",
            "Epoch [87/100], Step [ 59/ 78], Loss: 0.4915\n",
            "Epoch [87/100], Step [ 60/ 78], Loss: 0.5375\n",
            "Epoch [87/100], Step [ 61/ 78], Loss: 0.5227\n",
            "Epoch [87/100], Step [ 62/ 78], Loss: 0.4851\n",
            "Epoch [87/100], Step [ 63/ 78], Loss: 0.5399\n",
            "Epoch [88/100], Step [  1/ 78], Loss: 0.5351\n",
            "Epoch [88/100], Step [  2/ 78], Loss: 0.5264\n",
            "Epoch [88/100], Step [  3/ 78], Loss: 0.5378\n",
            "Epoch [88/100], Step [  4/ 78], Loss: 0.5089\n",
            "Epoch [88/100], Step [  5/ 78], Loss: 0.4930\n",
            "Epoch [88/100], Step [  6/ 78], Loss: 0.4739\n",
            "Epoch [88/100], Step [  7/ 78], Loss: 0.5251\n",
            "Epoch [88/100], Step [  8/ 78], Loss: 0.5456\n",
            "Epoch [88/100], Step [  9/ 78], Loss: 0.5349\n",
            "Epoch [88/100], Step [ 10/ 78], Loss: 0.5405\n",
            "Epoch [88/100], Step [ 11/ 78], Loss: 0.5048\n",
            "Epoch [88/100], Step [ 12/ 78], Loss: 0.5445\n",
            "Epoch [88/100], Step [ 13/ 78], Loss: 0.5552\n",
            "Epoch [88/100], Step [ 14/ 78], Loss: 0.5070\n",
            "Epoch [88/100], Step [ 15/ 78], Loss: 0.5207\n",
            "Epoch [88/100], Step [ 16/ 78], Loss: 0.5208\n",
            "Epoch [88/100], Step [ 17/ 78], Loss: 0.5175\n",
            "Epoch [88/100], Step [ 18/ 78], Loss: 0.5243\n",
            "Epoch [88/100], Step [ 19/ 78], Loss: 0.4883\n",
            "Epoch [88/100], Step [ 20/ 78], Loss: 0.5150\n",
            "Epoch [88/100], Step [ 21/ 78], Loss: 0.4748\n",
            "Epoch [88/100], Step [ 22/ 78], Loss: 0.4842\n",
            "Epoch [88/100], Step [ 23/ 78], Loss: 0.5305\n",
            "Epoch [88/100], Step [ 24/ 78], Loss: 0.5040\n",
            "Epoch [88/100], Step [ 25/ 78], Loss: 0.5115\n",
            "Epoch [88/100], Step [ 26/ 78], Loss: 0.4893\n",
            "Epoch [88/100], Step [ 27/ 78], Loss: 0.5266\n",
            "Epoch [88/100], Step [ 28/ 78], Loss: 0.5243\n",
            "Epoch [88/100], Step [ 29/ 78], Loss: 0.5266\n",
            "Epoch [88/100], Step [ 30/ 78], Loss: 0.5091\n",
            "Epoch [88/100], Step [ 31/ 78], Loss: 0.5401\n",
            "Epoch [88/100], Step [ 32/ 78], Loss: 0.5186\n",
            "Epoch [88/100], Step [ 33/ 78], Loss: 0.5151\n",
            "Epoch [88/100], Step [ 34/ 78], Loss: 0.5301\n",
            "Epoch [88/100], Step [ 35/ 78], Loss: 0.5363\n",
            "Epoch [88/100], Step [ 36/ 78], Loss: 0.5572\n",
            "Epoch [88/100], Step [ 37/ 78], Loss: 0.5187\n",
            "Epoch [88/100], Step [ 38/ 78], Loss: 0.5137\n",
            "Epoch [88/100], Step [ 39/ 78], Loss: 0.5020\n",
            "Epoch [88/100], Step [ 40/ 78], Loss: 0.5075\n",
            "Epoch [88/100], Step [ 41/ 78], Loss: 0.5180\n",
            "Epoch [88/100], Step [ 42/ 78], Loss: 0.5078\n",
            "Epoch [88/100], Step [ 43/ 78], Loss: 0.4987\n",
            "Epoch [88/100], Step [ 44/ 78], Loss: 0.5449\n",
            "Epoch [88/100], Step [ 45/ 78], Loss: 0.5743\n",
            "Epoch [88/100], Step [ 46/ 78], Loss: 0.5615\n",
            "Epoch [88/100], Step [ 47/ 78], Loss: 0.5325\n",
            "Epoch [88/100], Step [ 48/ 78], Loss: 0.5182\n",
            "Epoch [88/100], Step [ 49/ 78], Loss: 0.5095\n",
            "Epoch [88/100], Step [ 50/ 78], Loss: 0.5140\n",
            "Epoch [88/100], Step [ 51/ 78], Loss: 0.4979\n",
            "Epoch [88/100], Step [ 52/ 78], Loss: 0.5055\n",
            "Epoch [88/100], Step [ 53/ 78], Loss: 0.5008\n",
            "Epoch [88/100], Step [ 54/ 78], Loss: 0.5316\n",
            "Epoch [88/100], Step [ 55/ 78], Loss: 0.5494\n",
            "Epoch [88/100], Step [ 56/ 78], Loss: 0.4908\n",
            "Epoch [88/100], Step [ 57/ 78], Loss: 0.5069\n",
            "Epoch [88/100], Step [ 58/ 78], Loss: 0.5324\n",
            "Epoch [88/100], Step [ 59/ 78], Loss: 0.4948\n",
            "Epoch [88/100], Step [ 60/ 78], Loss: 0.5421\n",
            "Epoch [88/100], Step [ 61/ 78], Loss: 0.5377\n",
            "Epoch [88/100], Step [ 62/ 78], Loss: 0.5397\n",
            "Epoch [88/100], Step [ 63/ 78], Loss: 0.5315\n",
            "Epoch [89/100], Step [  1/ 78], Loss: 0.5195\n",
            "Epoch [89/100], Step [  2/ 78], Loss: 0.5521\n",
            "Epoch [89/100], Step [  3/ 78], Loss: 0.5168\n",
            "Epoch [89/100], Step [  4/ 78], Loss: 0.5471\n",
            "Epoch [89/100], Step [  5/ 78], Loss: 0.5178\n",
            "Epoch [89/100], Step [  6/ 78], Loss: 0.5484\n",
            "Epoch [89/100], Step [  7/ 78], Loss: 0.5239\n",
            "Epoch [89/100], Step [  8/ 78], Loss: 0.5197\n",
            "Epoch [89/100], Step [  9/ 78], Loss: 0.5176\n",
            "Epoch [89/100], Step [ 10/ 78], Loss: 0.5031\n",
            "Epoch [89/100], Step [ 11/ 78], Loss: 0.5078\n",
            "Epoch [89/100], Step [ 12/ 78], Loss: 0.5384\n",
            "Epoch [89/100], Step [ 13/ 78], Loss: 0.5082\n",
            "Epoch [89/100], Step [ 14/ 78], Loss: 0.5023\n",
            "Epoch [89/100], Step [ 15/ 78], Loss: 0.5340\n",
            "Epoch [89/100], Step [ 16/ 78], Loss: 0.5503\n",
            "Epoch [89/100], Step [ 17/ 78], Loss: 0.5722\n",
            "Epoch [89/100], Step [ 18/ 78], Loss: 0.5153\n",
            "Epoch [89/100], Step [ 19/ 78], Loss: 0.5104\n",
            "Epoch [89/100], Step [ 20/ 78], Loss: 0.5169\n",
            "Epoch [89/100], Step [ 21/ 78], Loss: 0.5222\n",
            "Epoch [89/100], Step [ 22/ 78], Loss: 0.5076\n",
            "Epoch [89/100], Step [ 23/ 78], Loss: 0.5301\n",
            "Epoch [89/100], Step [ 24/ 78], Loss: 0.5184\n",
            "Epoch [89/100], Step [ 25/ 78], Loss: 0.4941\n",
            "Epoch [89/100], Step [ 26/ 78], Loss: 0.4964\n",
            "Epoch [89/100], Step [ 27/ 78], Loss: 0.5250\n",
            "Epoch [89/100], Step [ 28/ 78], Loss: 0.5207\n",
            "Epoch [89/100], Step [ 29/ 78], Loss: 0.4996\n",
            "Epoch [89/100], Step [ 30/ 78], Loss: 0.5244\n",
            "Epoch [89/100], Step [ 31/ 78], Loss: 0.5098\n",
            "Epoch [89/100], Step [ 32/ 78], Loss: 0.5003\n",
            "Epoch [89/100], Step [ 33/ 78], Loss: 0.5058\n",
            "Epoch [89/100], Step [ 34/ 78], Loss: 0.4993\n",
            "Epoch [89/100], Step [ 35/ 78], Loss: 0.5357\n",
            "Epoch [89/100], Step [ 36/ 78], Loss: 0.5547\n",
            "Epoch [89/100], Step [ 37/ 78], Loss: 0.5334\n",
            "Epoch [89/100], Step [ 38/ 78], Loss: 0.5528\n",
            "Epoch [89/100], Step [ 39/ 78], Loss: 0.5422\n",
            "Epoch [89/100], Step [ 40/ 78], Loss: 0.5081\n",
            "Epoch [89/100], Step [ 41/ 78], Loss: 0.5184\n",
            "Epoch [89/100], Step [ 42/ 78], Loss: 0.5254\n",
            "Epoch [89/100], Step [ 43/ 78], Loss: 0.4789\n",
            "Epoch [89/100], Step [ 44/ 78], Loss: 0.5243\n",
            "Epoch [89/100], Step [ 45/ 78], Loss: 0.5221\n",
            "Epoch [89/100], Step [ 46/ 78], Loss: 0.5196\n",
            "Epoch [89/100], Step [ 47/ 78], Loss: 0.5155\n",
            "Epoch [89/100], Step [ 48/ 78], Loss: 0.5187\n",
            "Epoch [89/100], Step [ 49/ 78], Loss: 0.4912\n",
            "Epoch [89/100], Step [ 50/ 78], Loss: 0.5184\n",
            "Epoch [89/100], Step [ 51/ 78], Loss: 0.5161\n",
            "Epoch [89/100], Step [ 52/ 78], Loss: 0.5012\n",
            "Epoch [89/100], Step [ 53/ 78], Loss: 0.5446\n",
            "Epoch [89/100], Step [ 54/ 78], Loss: 0.5241\n",
            "Epoch [89/100], Step [ 55/ 78], Loss: 0.5060\n",
            "Epoch [89/100], Step [ 56/ 78], Loss: 0.5209\n",
            "Epoch [89/100], Step [ 57/ 78], Loss: 0.5265\n",
            "Epoch [89/100], Step [ 58/ 78], Loss: 0.5149\n",
            "Epoch [89/100], Step [ 59/ 78], Loss: 0.5315\n",
            "Epoch [89/100], Step [ 60/ 78], Loss: 0.5289\n",
            "Epoch [89/100], Step [ 61/ 78], Loss: 0.5532\n",
            "Epoch [89/100], Step [ 62/ 78], Loss: 0.5205\n",
            "Epoch [89/100], Step [ 63/ 78], Loss: 0.5226\n",
            "Epoch [90/100], Step [  1/ 78], Loss: 0.5077\n",
            "Epoch [90/100], Step [  2/ 78], Loss: 0.5209\n",
            "Epoch [90/100], Step [  3/ 78], Loss: 0.5397\n",
            "Epoch [90/100], Step [  4/ 78], Loss: 0.5070\n",
            "Epoch [90/100], Step [  5/ 78], Loss: 0.5284\n",
            "Epoch [90/100], Step [  6/ 78], Loss: 0.5159\n",
            "Epoch [90/100], Step [  7/ 78], Loss: 0.5036\n",
            "Epoch [90/100], Step [  8/ 78], Loss: 0.4930\n",
            "Epoch [90/100], Step [  9/ 78], Loss: 0.5057\n",
            "Epoch [90/100], Step [ 10/ 78], Loss: 0.5249\n",
            "Epoch [90/100], Step [ 11/ 78], Loss: 0.4895\n",
            "Epoch [90/100], Step [ 12/ 78], Loss: 0.4753\n",
            "Epoch [90/100], Step [ 13/ 78], Loss: 0.5142\n",
            "Epoch [90/100], Step [ 14/ 78], Loss: 0.5128\n",
            "Epoch [90/100], Step [ 15/ 78], Loss: 0.5095\n",
            "Epoch [90/100], Step [ 16/ 78], Loss: 0.5254\n",
            "Epoch [90/100], Step [ 17/ 78], Loss: 0.4899\n",
            "Epoch [90/100], Step [ 18/ 78], Loss: 0.4693\n",
            "Epoch [90/100], Step [ 19/ 78], Loss: 0.5384\n",
            "Epoch [90/100], Step [ 20/ 78], Loss: 0.5303\n",
            "Epoch [90/100], Step [ 21/ 78], Loss: 0.5002\n",
            "Epoch [90/100], Step [ 22/ 78], Loss: 0.5387\n",
            "Epoch [90/100], Step [ 23/ 78], Loss: 0.5236\n",
            "Epoch [90/100], Step [ 24/ 78], Loss: 0.5225\n",
            "Epoch [90/100], Step [ 25/ 78], Loss: 0.5673\n",
            "Epoch [90/100], Step [ 26/ 78], Loss: 0.5565\n",
            "Epoch [90/100], Step [ 27/ 78], Loss: 0.5157\n",
            "Epoch [90/100], Step [ 28/ 78], Loss: 0.4936\n",
            "Epoch [90/100], Step [ 29/ 78], Loss: 0.4989\n",
            "Epoch [90/100], Step [ 30/ 78], Loss: 0.5113\n",
            "Epoch [90/100], Step [ 31/ 78], Loss: 0.4893\n",
            "Epoch [90/100], Step [ 32/ 78], Loss: 0.5062\n",
            "Epoch [90/100], Step [ 33/ 78], Loss: 0.5083\n",
            "Epoch [90/100], Step [ 34/ 78], Loss: 0.4862\n",
            "Epoch [90/100], Step [ 35/ 78], Loss: 0.4862\n",
            "Epoch [90/100], Step [ 36/ 78], Loss: 0.5259\n",
            "Epoch [90/100], Step [ 37/ 78], Loss: 0.5154\n",
            "Epoch [90/100], Step [ 38/ 78], Loss: 0.5355\n",
            "Epoch [90/100], Step [ 39/ 78], Loss: 0.4975\n",
            "Epoch [90/100], Step [ 40/ 78], Loss: 0.5057\n",
            "Epoch [90/100], Step [ 41/ 78], Loss: 0.5274\n",
            "Epoch [90/100], Step [ 42/ 78], Loss: 0.5152\n",
            "Epoch [90/100], Step [ 43/ 78], Loss: 0.5113\n",
            "Epoch [90/100], Step [ 44/ 78], Loss: 0.5560\n",
            "Epoch [90/100], Step [ 45/ 78], Loss: 0.5863\n",
            "Epoch [90/100], Step [ 46/ 78], Loss: 0.5698\n",
            "Epoch [90/100], Step [ 47/ 78], Loss: 0.4936\n",
            "Epoch [90/100], Step [ 48/ 78], Loss: 0.5513\n",
            "Epoch [90/100], Step [ 49/ 78], Loss: 0.5386\n",
            "Epoch [90/100], Step [ 50/ 78], Loss: 0.5022\n",
            "Epoch [90/100], Step [ 51/ 78], Loss: 0.4793\n",
            "Epoch [90/100], Step [ 52/ 78], Loss: 0.5420\n",
            "Epoch [90/100], Step [ 53/ 78], Loss: 0.5008\n",
            "Epoch [90/100], Step [ 54/ 78], Loss: 0.5284\n",
            "Epoch [90/100], Step [ 55/ 78], Loss: 0.5294\n",
            "Epoch [90/100], Step [ 56/ 78], Loss: 0.4947\n",
            "Epoch [90/100], Step [ 57/ 78], Loss: 0.5093\n",
            "Epoch [90/100], Step [ 58/ 78], Loss: 0.5368\n",
            "Epoch [90/100], Step [ 59/ 78], Loss: 0.5493\n",
            "Epoch [90/100], Step [ 60/ 78], Loss: 0.5115\n",
            "Epoch [90/100], Step [ 61/ 78], Loss: 0.4888\n",
            "Epoch [90/100], Step [ 62/ 78], Loss: 0.4780\n",
            "Epoch [90/100], Step [ 63/ 78], Loss: 0.5805\n",
            "Epoch [91/100], Step [  1/ 78], Loss: 0.5101\n",
            "Epoch [91/100], Step [  2/ 78], Loss: 0.5225\n",
            "Epoch [91/100], Step [  3/ 78], Loss: 0.4717\n",
            "Epoch [91/100], Step [  4/ 78], Loss: 0.5463\n",
            "Epoch [91/100], Step [  5/ 78], Loss: 0.4923\n",
            "Epoch [91/100], Step [  6/ 78], Loss: 0.5006\n",
            "Epoch [91/100], Step [  7/ 78], Loss: 0.5110\n",
            "Epoch [91/100], Step [  8/ 78], Loss: 0.4940\n",
            "Epoch [91/100], Step [  9/ 78], Loss: 0.5257\n",
            "Epoch [91/100], Step [ 10/ 78], Loss: 0.5405\n",
            "Epoch [91/100], Step [ 11/ 78], Loss: 0.4876\n",
            "Epoch [91/100], Step [ 12/ 78], Loss: 0.4885\n",
            "Epoch [91/100], Step [ 13/ 78], Loss: 0.5199\n",
            "Epoch [91/100], Step [ 14/ 78], Loss: 0.5032\n",
            "Epoch [91/100], Step [ 15/ 78], Loss: 0.5071\n",
            "Epoch [91/100], Step [ 16/ 78], Loss: 0.5180\n",
            "Epoch [91/100], Step [ 17/ 78], Loss: 0.5469\n",
            "Epoch [91/100], Step [ 18/ 78], Loss: 0.5249\n",
            "Epoch [91/100], Step [ 19/ 78], Loss: 0.5048\n",
            "Epoch [91/100], Step [ 20/ 78], Loss: 0.5511\n",
            "Epoch [91/100], Step [ 21/ 78], Loss: 0.5241\n",
            "Epoch [91/100], Step [ 22/ 78], Loss: 0.5621\n",
            "Epoch [91/100], Step [ 23/ 78], Loss: 0.4653\n",
            "Epoch [91/100], Step [ 24/ 78], Loss: 0.5375\n",
            "Epoch [91/100], Step [ 25/ 78], Loss: 0.5201\n",
            "Epoch [91/100], Step [ 26/ 78], Loss: 0.5346\n",
            "Epoch [91/100], Step [ 27/ 78], Loss: 0.4893\n",
            "Epoch [91/100], Step [ 28/ 78], Loss: 0.5486\n",
            "Epoch [91/100], Step [ 29/ 78], Loss: 0.5260\n",
            "Epoch [91/100], Step [ 30/ 78], Loss: 0.5054\n",
            "Epoch [91/100], Step [ 31/ 78], Loss: 0.5071\n",
            "Epoch [91/100], Step [ 32/ 78], Loss: 0.4877\n",
            "Epoch [91/100], Step [ 33/ 78], Loss: 0.4787\n",
            "Epoch [91/100], Step [ 34/ 78], Loss: 0.5378\n",
            "Epoch [91/100], Step [ 35/ 78], Loss: 0.5037\n",
            "Epoch [91/100], Step [ 36/ 78], Loss: 0.5023\n",
            "Epoch [91/100], Step [ 37/ 78], Loss: 0.4878\n",
            "Epoch [91/100], Step [ 38/ 78], Loss: 0.5344\n",
            "Epoch [91/100], Step [ 39/ 78], Loss: 0.5566\n",
            "Epoch [91/100], Step [ 40/ 78], Loss: 0.5903\n",
            "Epoch [91/100], Step [ 41/ 78], Loss: 0.5320\n",
            "Epoch [91/100], Step [ 42/ 78], Loss: 0.5409\n",
            "Epoch [91/100], Step [ 43/ 78], Loss: 0.4783\n",
            "Epoch [91/100], Step [ 44/ 78], Loss: 0.4969\n",
            "Epoch [91/100], Step [ 45/ 78], Loss: 0.5354\n",
            "Epoch [91/100], Step [ 46/ 78], Loss: 0.5024\n",
            "Epoch [91/100], Step [ 47/ 78], Loss: 0.4841\n",
            "Epoch [91/100], Step [ 48/ 78], Loss: 0.4964\n",
            "Epoch [91/100], Step [ 49/ 78], Loss: 0.5121\n",
            "Epoch [91/100], Step [ 50/ 78], Loss: 0.5067\n",
            "Epoch [91/100], Step [ 51/ 78], Loss: 0.4928\n",
            "Epoch [91/100], Step [ 52/ 78], Loss: 0.5705\n",
            "Epoch [91/100], Step [ 53/ 78], Loss: 0.5048\n",
            "Epoch [91/100], Step [ 54/ 78], Loss: 0.5065\n",
            "Epoch [91/100], Step [ 55/ 78], Loss: 0.5515\n",
            "Epoch [91/100], Step [ 56/ 78], Loss: 0.5055\n",
            "Epoch [91/100], Step [ 57/ 78], Loss: 0.5462\n",
            "Epoch [91/100], Step [ 58/ 78], Loss: 0.5010\n",
            "Epoch [91/100], Step [ 59/ 78], Loss: 0.5062\n",
            "Epoch [91/100], Step [ 60/ 78], Loss: 0.5437\n",
            "Epoch [91/100], Step [ 61/ 78], Loss: 0.5193\n",
            "Epoch [91/100], Step [ 62/ 78], Loss: 0.4775\n",
            "Epoch [91/100], Step [ 63/ 78], Loss: 0.5178\n",
            "Epoch [92/100], Step [  1/ 78], Loss: 0.4849\n",
            "Epoch [92/100], Step [  2/ 78], Loss: 0.4888\n",
            "Epoch [92/100], Step [  3/ 78], Loss: 0.5032\n",
            "Epoch [92/100], Step [  4/ 78], Loss: 0.4793\n",
            "Epoch [92/100], Step [  5/ 78], Loss: 0.5369\n",
            "Epoch [92/100], Step [  6/ 78], Loss: 0.5391\n",
            "Epoch [92/100], Step [  7/ 78], Loss: 0.4700\n",
            "Epoch [92/100], Step [  8/ 78], Loss: 0.5077\n",
            "Epoch [92/100], Step [  9/ 78], Loss: 0.4893\n",
            "Epoch [92/100], Step [ 10/ 78], Loss: 0.5282\n",
            "Epoch [92/100], Step [ 11/ 78], Loss: 0.5006\n",
            "Epoch [92/100], Step [ 12/ 78], Loss: 0.4972\n",
            "Epoch [92/100], Step [ 13/ 78], Loss: 0.5249\n",
            "Epoch [92/100], Step [ 14/ 78], Loss: 0.4902\n",
            "Epoch [92/100], Step [ 15/ 78], Loss: 0.5021\n",
            "Epoch [92/100], Step [ 16/ 78], Loss: 0.5078\n",
            "Epoch [92/100], Step [ 17/ 78], Loss: 0.4989\n",
            "Epoch [92/100], Step [ 18/ 78], Loss: 0.5229\n",
            "Epoch [92/100], Step [ 19/ 78], Loss: 0.5286\n",
            "Epoch [92/100], Step [ 20/ 78], Loss: 0.5249\n",
            "Epoch [92/100], Step [ 21/ 78], Loss: 0.4948\n",
            "Epoch [92/100], Step [ 22/ 78], Loss: 0.5675\n",
            "Epoch [92/100], Step [ 23/ 78], Loss: 0.4962\n",
            "Epoch [92/100], Step [ 24/ 78], Loss: 0.4974\n",
            "Epoch [92/100], Step [ 25/ 78], Loss: 0.4767\n",
            "Epoch [92/100], Step [ 26/ 78], Loss: 0.5196\n",
            "Epoch [92/100], Step [ 27/ 78], Loss: 0.4718\n",
            "Epoch [92/100], Step [ 28/ 78], Loss: 0.5314\n",
            "Epoch [92/100], Step [ 29/ 78], Loss: 0.5385\n",
            "Epoch [92/100], Step [ 30/ 78], Loss: 0.5411\n",
            "Epoch [92/100], Step [ 31/ 78], Loss: 0.5415\n",
            "Epoch [92/100], Step [ 32/ 78], Loss: 0.5201\n",
            "Epoch [92/100], Step [ 33/ 78], Loss: 0.5365\n",
            "Epoch [92/100], Step [ 34/ 78], Loss: 0.5216\n",
            "Epoch [92/100], Step [ 35/ 78], Loss: 0.5061\n",
            "Epoch [92/100], Step [ 36/ 78], Loss: 0.5318\n",
            "Epoch [92/100], Step [ 37/ 78], Loss: 0.5115\n",
            "Epoch [92/100], Step [ 38/ 78], Loss: 0.5123\n",
            "Epoch [92/100], Step [ 39/ 78], Loss: 0.5380\n",
            "Epoch [92/100], Step [ 40/ 78], Loss: 0.4735\n",
            "Epoch [92/100], Step [ 41/ 78], Loss: 0.5400\n",
            "Epoch [92/100], Step [ 42/ 78], Loss: 0.5245\n",
            "Epoch [92/100], Step [ 43/ 78], Loss: 0.5044\n",
            "Epoch [92/100], Step [ 44/ 78], Loss: 0.4761\n",
            "Epoch [92/100], Step [ 45/ 78], Loss: 0.5091\n",
            "Epoch [92/100], Step [ 46/ 78], Loss: 0.5343\n",
            "Epoch [92/100], Step [ 47/ 78], Loss: 0.5018\n",
            "Epoch [92/100], Step [ 48/ 78], Loss: 0.4833\n",
            "Epoch [92/100], Step [ 49/ 78], Loss: 0.4594\n",
            "Epoch [92/100], Step [ 50/ 78], Loss: 0.5377\n",
            "Epoch [92/100], Step [ 51/ 78], Loss: 0.5101\n",
            "Epoch [92/100], Step [ 52/ 78], Loss: 0.5133\n",
            "Epoch [92/100], Step [ 53/ 78], Loss: 0.5104\n",
            "Epoch [92/100], Step [ 54/ 78], Loss: 0.5022\n",
            "Epoch [92/100], Step [ 55/ 78], Loss: 0.5374\n",
            "Epoch [92/100], Step [ 56/ 78], Loss: 0.5640\n",
            "Epoch [92/100], Step [ 57/ 78], Loss: 0.5039\n",
            "Epoch [92/100], Step [ 58/ 78], Loss: 0.5288\n",
            "Epoch [92/100], Step [ 59/ 78], Loss: 0.5294\n",
            "Epoch [92/100], Step [ 60/ 78], Loss: 0.5026\n",
            "Epoch [92/100], Step [ 61/ 78], Loss: 0.5537\n",
            "Epoch [92/100], Step [ 62/ 78], Loss: 0.5544\n",
            "Epoch [92/100], Step [ 63/ 78], Loss: 0.5140\n",
            "Epoch [93/100], Step [  1/ 78], Loss: 0.5504\n",
            "Epoch [93/100], Step [  2/ 78], Loss: 0.5246\n",
            "Epoch [93/100], Step [  3/ 78], Loss: 0.5239\n",
            "Epoch [93/100], Step [  4/ 78], Loss: 0.4986\n",
            "Epoch [93/100], Step [  5/ 78], Loss: 0.5358\n",
            "Epoch [93/100], Step [  6/ 78], Loss: 0.4906\n",
            "Epoch [93/100], Step [  7/ 78], Loss: 0.4708\n",
            "Epoch [93/100], Step [  8/ 78], Loss: 0.5017\n",
            "Epoch [93/100], Step [  9/ 78], Loss: 0.5313\n",
            "Epoch [93/100], Step [ 10/ 78], Loss: 0.5053\n",
            "Epoch [93/100], Step [ 11/ 78], Loss: 0.5158\n",
            "Epoch [93/100], Step [ 12/ 78], Loss: 0.5085\n",
            "Epoch [93/100], Step [ 13/ 78], Loss: 0.5173\n",
            "Epoch [93/100], Step [ 14/ 78], Loss: 0.5316\n",
            "Epoch [93/100], Step [ 15/ 78], Loss: 0.5621\n",
            "Epoch [93/100], Step [ 16/ 78], Loss: 0.5377\n",
            "Epoch [93/100], Step [ 17/ 78], Loss: 0.5286\n",
            "Epoch [93/100], Step [ 18/ 78], Loss: 0.5073\n",
            "Epoch [93/100], Step [ 19/ 78], Loss: 0.4925\n",
            "Epoch [93/100], Step [ 20/ 78], Loss: 0.4884\n",
            "Epoch [93/100], Step [ 21/ 78], Loss: 0.5326\n",
            "Epoch [93/100], Step [ 22/ 78], Loss: 0.5686\n",
            "Epoch [93/100], Step [ 23/ 78], Loss: 0.5444\n",
            "Epoch [93/100], Step [ 24/ 78], Loss: 0.4995\n",
            "Epoch [93/100], Step [ 25/ 78], Loss: 0.5242\n",
            "Epoch [93/100], Step [ 26/ 78], Loss: 0.5105\n",
            "Epoch [93/100], Step [ 27/ 78], Loss: 0.4899\n",
            "Epoch [93/100], Step [ 28/ 78], Loss: 0.5433\n",
            "Epoch [93/100], Step [ 29/ 78], Loss: 0.5058\n",
            "Epoch [93/100], Step [ 30/ 78], Loss: 0.5065\n",
            "Epoch [93/100], Step [ 31/ 78], Loss: 0.5046\n",
            "Epoch [93/100], Step [ 32/ 78], Loss: 0.5169\n",
            "Epoch [93/100], Step [ 33/ 78], Loss: 0.4950\n",
            "Epoch [93/100], Step [ 34/ 78], Loss: 0.4507\n",
            "Epoch [93/100], Step [ 35/ 78], Loss: 0.5321\n",
            "Epoch [93/100], Step [ 36/ 78], Loss: 0.5005\n",
            "Epoch [93/100], Step [ 37/ 78], Loss: 0.5022\n",
            "Epoch [93/100], Step [ 38/ 78], Loss: 0.5281\n",
            "Epoch [93/100], Step [ 39/ 78], Loss: 0.4966\n",
            "Epoch [93/100], Step [ 40/ 78], Loss: 0.4829\n",
            "Epoch [93/100], Step [ 41/ 78], Loss: 0.5453\n",
            "Epoch [93/100], Step [ 42/ 78], Loss: 0.5553\n",
            "Epoch [93/100], Step [ 43/ 78], Loss: 0.5301\n",
            "Epoch [93/100], Step [ 44/ 78], Loss: 0.5561\n",
            "Epoch [93/100], Step [ 45/ 78], Loss: 0.5089\n",
            "Epoch [93/100], Step [ 46/ 78], Loss: 0.5093\n",
            "Epoch [93/100], Step [ 47/ 78], Loss: 0.5141\n",
            "Epoch [93/100], Step [ 48/ 78], Loss: 0.5259\n",
            "Epoch [93/100], Step [ 49/ 78], Loss: 0.4758\n",
            "Epoch [93/100], Step [ 50/ 78], Loss: 0.5434\n",
            "Epoch [93/100], Step [ 51/ 78], Loss: 0.5298\n",
            "Epoch [93/100], Step [ 52/ 78], Loss: 0.5027\n",
            "Epoch [93/100], Step [ 53/ 78], Loss: 0.5021\n",
            "Epoch [93/100], Step [ 54/ 78], Loss: 0.4558\n",
            "Epoch [93/100], Step [ 55/ 78], Loss: 0.4872\n",
            "Epoch [93/100], Step [ 56/ 78], Loss: 0.4964\n",
            "Epoch [93/100], Step [ 57/ 78], Loss: 0.4757\n",
            "Epoch [93/100], Step [ 58/ 78], Loss: 0.5050\n",
            "Epoch [93/100], Step [ 59/ 78], Loss: 0.5859\n",
            "Epoch [93/100], Step [ 60/ 78], Loss: 0.5491\n",
            "Epoch [93/100], Step [ 61/ 78], Loss: 0.5867\n",
            "Epoch [93/100], Step [ 62/ 78], Loss: 0.4942\n",
            "Epoch [93/100], Step [ 63/ 78], Loss: 0.5227\n",
            "Epoch [94/100], Step [  1/ 78], Loss: 0.5394\n",
            "Epoch [94/100], Step [  2/ 78], Loss: 0.5331\n",
            "Epoch [94/100], Step [  3/ 78], Loss: 0.5352\n",
            "Epoch [94/100], Step [  4/ 78], Loss: 0.5470\n",
            "Epoch [94/100], Step [  5/ 78], Loss: 0.5146\n",
            "Epoch [94/100], Step [  6/ 78], Loss: 0.4904\n",
            "Epoch [94/100], Step [  7/ 78], Loss: 0.5439\n",
            "Epoch [94/100], Step [  8/ 78], Loss: 0.5737\n",
            "Epoch [94/100], Step [  9/ 78], Loss: 0.5258\n",
            "Epoch [94/100], Step [ 10/ 78], Loss: 0.5058\n",
            "Epoch [94/100], Step [ 11/ 78], Loss: 0.4664\n",
            "Epoch [94/100], Step [ 12/ 78], Loss: 0.5258\n",
            "Epoch [94/100], Step [ 13/ 78], Loss: 0.5017\n",
            "Epoch [94/100], Step [ 14/ 78], Loss: 0.5157\n",
            "Epoch [94/100], Step [ 15/ 78], Loss: 0.5468\n",
            "Epoch [94/100], Step [ 16/ 78], Loss: 0.4927\n",
            "Epoch [94/100], Step [ 17/ 78], Loss: 0.5126\n",
            "Epoch [94/100], Step [ 18/ 78], Loss: 0.5299\n",
            "Epoch [94/100], Step [ 19/ 78], Loss: 0.5046\n",
            "Epoch [94/100], Step [ 20/ 78], Loss: 0.5129\n",
            "Epoch [94/100], Step [ 21/ 78], Loss: 0.5038\n",
            "Epoch [94/100], Step [ 22/ 78], Loss: 0.5197\n",
            "Epoch [94/100], Step [ 23/ 78], Loss: 0.5043\n",
            "Epoch [94/100], Step [ 24/ 78], Loss: 0.5412\n",
            "Epoch [94/100], Step [ 25/ 78], Loss: 0.5091\n",
            "Epoch [94/100], Step [ 26/ 78], Loss: 0.5076\n",
            "Epoch [94/100], Step [ 27/ 78], Loss: 0.5319\n",
            "Epoch [94/100], Step [ 28/ 78], Loss: 0.4805\n",
            "Epoch [94/100], Step [ 29/ 78], Loss: 0.4993\n",
            "Epoch [94/100], Step [ 30/ 78], Loss: 0.5157\n",
            "Epoch [94/100], Step [ 31/ 78], Loss: 0.5033\n",
            "Epoch [94/100], Step [ 32/ 78], Loss: 0.5709\n",
            "Epoch [94/100], Step [ 33/ 78], Loss: 0.5339\n",
            "Epoch [94/100], Step [ 34/ 78], Loss: 0.5096\n",
            "Epoch [94/100], Step [ 35/ 78], Loss: 0.4976\n",
            "Epoch [94/100], Step [ 36/ 78], Loss: 0.5258\n",
            "Epoch [94/100], Step [ 37/ 78], Loss: 0.4807\n",
            "Epoch [94/100], Step [ 38/ 78], Loss: 0.5308\n",
            "Epoch [94/100], Step [ 39/ 78], Loss: 0.5406\n",
            "Epoch [94/100], Step [ 40/ 78], Loss: 0.4512\n",
            "Epoch [94/100], Step [ 41/ 78], Loss: 0.5061\n",
            "Epoch [94/100], Step [ 42/ 78], Loss: 0.4991\n",
            "Epoch [94/100], Step [ 43/ 78], Loss: 0.5119\n",
            "Epoch [94/100], Step [ 44/ 78], Loss: 0.5065\n",
            "Epoch [94/100], Step [ 45/ 78], Loss: 0.5367\n",
            "Epoch [94/100], Step [ 46/ 78], Loss: 0.5296\n",
            "Epoch [94/100], Step [ 47/ 78], Loss: 0.5521\n",
            "Epoch [94/100], Step [ 48/ 78], Loss: 0.5487\n",
            "Epoch [94/100], Step [ 49/ 78], Loss: 0.5207\n",
            "Epoch [94/100], Step [ 50/ 78], Loss: 0.5149\n",
            "Epoch [94/100], Step [ 51/ 78], Loss: 0.5280\n",
            "Epoch [94/100], Step [ 52/ 78], Loss: 0.4884\n",
            "Epoch [94/100], Step [ 53/ 78], Loss: 0.4976\n",
            "Epoch [94/100], Step [ 54/ 78], Loss: 0.4695\n",
            "Epoch [94/100], Step [ 55/ 78], Loss: 0.5268\n",
            "Epoch [94/100], Step [ 56/ 78], Loss: 0.5062\n",
            "Epoch [94/100], Step [ 57/ 78], Loss: 0.4806\n",
            "Epoch [94/100], Step [ 58/ 78], Loss: 0.5095\n",
            "Epoch [94/100], Step [ 59/ 78], Loss: 0.5455\n",
            "Epoch [94/100], Step [ 60/ 78], Loss: 0.5421\n",
            "Epoch [94/100], Step [ 61/ 78], Loss: 0.4747\n",
            "Epoch [94/100], Step [ 62/ 78], Loss: 0.4962\n",
            "Epoch [94/100], Step [ 63/ 78], Loss: 0.5477\n",
            "Epoch [95/100], Step [  1/ 78], Loss: 0.5437\n",
            "Epoch [95/100], Step [  2/ 78], Loss: 0.5191\n",
            "Epoch [95/100], Step [  3/ 78], Loss: 0.5019\n",
            "Epoch [95/100], Step [  4/ 78], Loss: 0.5379\n",
            "Epoch [95/100], Step [  5/ 78], Loss: 0.4864\n",
            "Epoch [95/100], Step [  6/ 78], Loss: 0.4838\n",
            "Epoch [95/100], Step [  7/ 78], Loss: 0.5200\n",
            "Epoch [95/100], Step [  8/ 78], Loss: 0.5144\n",
            "Epoch [95/100], Step [  9/ 78], Loss: 0.5126\n",
            "Epoch [95/100], Step [ 10/ 78], Loss: 0.4837\n",
            "Epoch [95/100], Step [ 11/ 78], Loss: 0.4894\n",
            "Epoch [95/100], Step [ 12/ 78], Loss: 0.5076\n",
            "Epoch [95/100], Step [ 13/ 78], Loss: 0.5155\n",
            "Epoch [95/100], Step [ 14/ 78], Loss: 0.5155\n",
            "Epoch [95/100], Step [ 15/ 78], Loss: 0.5590\n",
            "Epoch [95/100], Step [ 16/ 78], Loss: 0.5460\n",
            "Epoch [95/100], Step [ 17/ 78], Loss: 0.4832\n",
            "Epoch [95/100], Step [ 18/ 78], Loss: 0.5231\n",
            "Epoch [95/100], Step [ 19/ 78], Loss: 0.5138\n",
            "Epoch [95/100], Step [ 20/ 78], Loss: 0.5371\n",
            "Epoch [95/100], Step [ 21/ 78], Loss: 0.5309\n",
            "Epoch [95/100], Step [ 22/ 78], Loss: 0.4999\n",
            "Epoch [95/100], Step [ 23/ 78], Loss: 0.5015\n",
            "Epoch [95/100], Step [ 24/ 78], Loss: 0.5011\n",
            "Epoch [95/100], Step [ 25/ 78], Loss: 0.4996\n",
            "Epoch [95/100], Step [ 26/ 78], Loss: 0.5086\n",
            "Epoch [95/100], Step [ 27/ 78], Loss: 0.5149\n",
            "Epoch [95/100], Step [ 28/ 78], Loss: 0.5317\n",
            "Epoch [95/100], Step [ 29/ 78], Loss: 0.5302\n",
            "Epoch [95/100], Step [ 30/ 78], Loss: 0.5605\n",
            "Epoch [95/100], Step [ 31/ 78], Loss: 0.5144\n",
            "Epoch [95/100], Step [ 32/ 78], Loss: 0.5102\n",
            "Epoch [95/100], Step [ 33/ 78], Loss: 0.4798\n",
            "Epoch [95/100], Step [ 34/ 78], Loss: 0.4880\n",
            "Epoch [95/100], Step [ 35/ 78], Loss: 0.5199\n",
            "Epoch [95/100], Step [ 36/ 78], Loss: 0.5427\n",
            "Epoch [95/100], Step [ 37/ 78], Loss: 0.6266\n",
            "Epoch [95/100], Step [ 38/ 78], Loss: 0.5242\n",
            "Epoch [95/100], Step [ 39/ 78], Loss: 0.5077\n",
            "Epoch [95/100], Step [ 40/ 78], Loss: 0.4864\n",
            "Epoch [95/100], Step [ 41/ 78], Loss: 0.5043\n",
            "Epoch [95/100], Step [ 42/ 78], Loss: 0.5083\n",
            "Epoch [95/100], Step [ 43/ 78], Loss: 0.5106\n",
            "Epoch [95/100], Step [ 44/ 78], Loss: 0.4490\n",
            "Epoch [95/100], Step [ 45/ 78], Loss: 0.4908\n",
            "Epoch [95/100], Step [ 46/ 78], Loss: 0.5043\n",
            "Epoch [95/100], Step [ 47/ 78], Loss: 0.5368\n",
            "Epoch [95/100], Step [ 48/ 78], Loss: 0.4933\n",
            "Epoch [95/100], Step [ 49/ 78], Loss: 0.4819\n",
            "Epoch [95/100], Step [ 50/ 78], Loss: 0.5049\n",
            "Epoch [95/100], Step [ 51/ 78], Loss: 0.4958\n",
            "Epoch [95/100], Step [ 52/ 78], Loss: 0.4820\n",
            "Epoch [95/100], Step [ 53/ 78], Loss: 0.5090\n",
            "Epoch [95/100], Step [ 54/ 78], Loss: 0.5132\n",
            "Epoch [95/100], Step [ 55/ 78], Loss: 0.5324\n",
            "Epoch [95/100], Step [ 56/ 78], Loss: 0.5347\n",
            "Epoch [95/100], Step [ 57/ 78], Loss: 0.5250\n",
            "Epoch [95/100], Step [ 58/ 78], Loss: 0.5303\n",
            "Epoch [95/100], Step [ 59/ 78], Loss: 0.4973\n",
            "Epoch [95/100], Step [ 60/ 78], Loss: 0.5305\n",
            "Epoch [95/100], Step [ 61/ 78], Loss: 0.5196\n",
            "Epoch [95/100], Step [ 62/ 78], Loss: 0.4980\n",
            "Epoch [95/100], Step [ 63/ 78], Loss: 0.4645\n",
            "Epoch [96/100], Step [  1/ 78], Loss: 0.4879\n",
            "Epoch [96/100], Step [  2/ 78], Loss: 0.5042\n",
            "Epoch [96/100], Step [  3/ 78], Loss: 0.5171\n",
            "Epoch [96/100], Step [  4/ 78], Loss: 0.5295\n",
            "Epoch [96/100], Step [  5/ 78], Loss: 0.5134\n",
            "Epoch [96/100], Step [  6/ 78], Loss: 0.4715\n",
            "Epoch [96/100], Step [  7/ 78], Loss: 0.5225\n",
            "Epoch [96/100], Step [  8/ 78], Loss: 0.5323\n",
            "Epoch [96/100], Step [  9/ 78], Loss: 0.5311\n",
            "Epoch [96/100], Step [ 10/ 78], Loss: 0.4755\n",
            "Epoch [96/100], Step [ 11/ 78], Loss: 0.5048\n",
            "Epoch [96/100], Step [ 12/ 78], Loss: 0.4935\n",
            "Epoch [96/100], Step [ 13/ 78], Loss: 0.4784\n",
            "Epoch [96/100], Step [ 14/ 78], Loss: 0.5185\n",
            "Epoch [96/100], Step [ 15/ 78], Loss: 0.4813\n",
            "Epoch [96/100], Step [ 16/ 78], Loss: 0.4665\n",
            "Epoch [96/100], Step [ 17/ 78], Loss: 0.5273\n",
            "Epoch [96/100], Step [ 18/ 78], Loss: 0.5431\n",
            "Epoch [96/100], Step [ 19/ 78], Loss: 0.5303\n",
            "Epoch [96/100], Step [ 20/ 78], Loss: 0.5190\n",
            "Epoch [96/100], Step [ 21/ 78], Loss: 0.5751\n",
            "Epoch [96/100], Step [ 22/ 78], Loss: 0.5345\n",
            "Epoch [96/100], Step [ 23/ 78], Loss: 0.5431\n",
            "Epoch [96/100], Step [ 24/ 78], Loss: 0.5309\n",
            "Epoch [96/100], Step [ 25/ 78], Loss: 0.5253\n",
            "Epoch [96/100], Step [ 26/ 78], Loss: 0.5358\n",
            "Epoch [96/100], Step [ 27/ 78], Loss: 0.5409\n",
            "Epoch [96/100], Step [ 28/ 78], Loss: 0.5223\n",
            "Epoch [96/100], Step [ 29/ 78], Loss: 0.5182\n",
            "Epoch [96/100], Step [ 30/ 78], Loss: 0.5640\n",
            "Epoch [96/100], Step [ 31/ 78], Loss: 0.5121\n",
            "Epoch [96/100], Step [ 32/ 78], Loss: 0.5111\n",
            "Epoch [96/100], Step [ 33/ 78], Loss: 0.5092\n",
            "Epoch [96/100], Step [ 34/ 78], Loss: 0.5511\n",
            "Epoch [96/100], Step [ 35/ 78], Loss: 0.5172\n",
            "Epoch [96/100], Step [ 36/ 78], Loss: 0.5131\n",
            "Epoch [96/100], Step [ 37/ 78], Loss: 0.5111\n",
            "Epoch [96/100], Step [ 38/ 78], Loss: 0.4998\n",
            "Epoch [96/100], Step [ 39/ 78], Loss: 0.5443\n",
            "Epoch [96/100], Step [ 40/ 78], Loss: 0.5284\n",
            "Epoch [96/100], Step [ 41/ 78], Loss: 0.5010\n",
            "Epoch [96/100], Step [ 42/ 78], Loss: 0.4991\n",
            "Epoch [96/100], Step [ 43/ 78], Loss: 0.5259\n",
            "Epoch [96/100], Step [ 44/ 78], Loss: 0.5056\n",
            "Epoch [96/100], Step [ 45/ 78], Loss: 0.5342\n",
            "Epoch [96/100], Step [ 46/ 78], Loss: 0.5295\n",
            "Epoch [96/100], Step [ 47/ 78], Loss: 0.5342\n",
            "Epoch [96/100], Step [ 48/ 78], Loss: 0.4896\n",
            "Epoch [96/100], Step [ 49/ 78], Loss: 0.5176\n",
            "Epoch [96/100], Step [ 50/ 78], Loss: 0.5104\n",
            "Epoch [96/100], Step [ 51/ 78], Loss: 0.5264\n",
            "Epoch [96/100], Step [ 52/ 78], Loss: 0.4963\n",
            "Epoch [96/100], Step [ 53/ 78], Loss: 0.4974\n",
            "Epoch [96/100], Step [ 54/ 78], Loss: 0.4845\n",
            "Epoch [96/100], Step [ 55/ 78], Loss: 0.5012\n",
            "Epoch [96/100], Step [ 56/ 78], Loss: 0.5033\n",
            "Epoch [96/100], Step [ 57/ 78], Loss: 0.5441\n",
            "Epoch [96/100], Step [ 58/ 78], Loss: 0.5519\n",
            "Epoch [96/100], Step [ 59/ 78], Loss: 0.5112\n",
            "Epoch [96/100], Step [ 60/ 78], Loss: 0.4985\n",
            "Epoch [96/100], Step [ 61/ 78], Loss: 0.5513\n",
            "Epoch [96/100], Step [ 62/ 78], Loss: 0.5205\n",
            "Epoch [96/100], Step [ 63/ 78], Loss: 0.5177\n",
            "Epoch [97/100], Step [  1/ 78], Loss: 0.5391\n",
            "Epoch [97/100], Step [  2/ 78], Loss: 0.5306\n",
            "Epoch [97/100], Step [  3/ 78], Loss: 0.5509\n",
            "Epoch [97/100], Step [  4/ 78], Loss: 0.5410\n",
            "Epoch [97/100], Step [  5/ 78], Loss: 0.5024\n",
            "Epoch [97/100], Step [  6/ 78], Loss: 0.5234\n",
            "Epoch [97/100], Step [  7/ 78], Loss: 0.5009\n",
            "Epoch [97/100], Step [  8/ 78], Loss: 0.4877\n",
            "Epoch [97/100], Step [  9/ 78], Loss: 0.5152\n",
            "Epoch [97/100], Step [ 10/ 78], Loss: 0.5045\n",
            "Epoch [97/100], Step [ 11/ 78], Loss: 0.5063\n",
            "Epoch [97/100], Step [ 12/ 78], Loss: 0.4934\n",
            "Epoch [97/100], Step [ 13/ 78], Loss: 0.5044\n",
            "Epoch [97/100], Step [ 14/ 78], Loss: 0.5216\n",
            "Epoch [97/100], Step [ 15/ 78], Loss: 0.4897\n",
            "Epoch [97/100], Step [ 16/ 78], Loss: 0.5015\n",
            "Epoch [97/100], Step [ 17/ 78], Loss: 0.4873\n",
            "Epoch [97/100], Step [ 18/ 78], Loss: 0.4899\n",
            "Epoch [97/100], Step [ 19/ 78], Loss: 0.5193\n",
            "Epoch [97/100], Step [ 20/ 78], Loss: 0.5157\n",
            "Epoch [97/100], Step [ 21/ 78], Loss: 0.5010\n",
            "Epoch [97/100], Step [ 22/ 78], Loss: 0.5362\n",
            "Epoch [97/100], Step [ 23/ 78], Loss: 0.4949\n",
            "Epoch [97/100], Step [ 24/ 78], Loss: 0.5322\n",
            "Epoch [97/100], Step [ 25/ 78], Loss: 0.5279\n",
            "Epoch [97/100], Step [ 26/ 78], Loss: 0.5295\n",
            "Epoch [97/100], Step [ 27/ 78], Loss: 0.5065\n",
            "Epoch [97/100], Step [ 28/ 78], Loss: 0.5164\n",
            "Epoch [97/100], Step [ 29/ 78], Loss: 0.5318\n",
            "Epoch [97/100], Step [ 30/ 78], Loss: 0.4660\n",
            "Epoch [97/100], Step [ 31/ 78], Loss: 0.4963\n",
            "Epoch [97/100], Step [ 32/ 78], Loss: 0.5138\n",
            "Epoch [97/100], Step [ 33/ 78], Loss: 0.5072\n",
            "Epoch [97/100], Step [ 34/ 78], Loss: 0.5244\n",
            "Epoch [97/100], Step [ 35/ 78], Loss: 0.5143\n",
            "Epoch [97/100], Step [ 36/ 78], Loss: 0.4907\n",
            "Epoch [97/100], Step [ 37/ 78], Loss: 0.5208\n",
            "Epoch [97/100], Step [ 38/ 78], Loss: 0.4878\n",
            "Epoch [97/100], Step [ 39/ 78], Loss: 0.5157\n",
            "Epoch [97/100], Step [ 40/ 78], Loss: 0.4834\n",
            "Epoch [97/100], Step [ 41/ 78], Loss: 0.4862\n",
            "Epoch [97/100], Step [ 42/ 78], Loss: 0.5080\n",
            "Epoch [97/100], Step [ 43/ 78], Loss: 0.5142\n",
            "Epoch [97/100], Step [ 44/ 78], Loss: 0.5357\n",
            "Epoch [97/100], Step [ 45/ 78], Loss: 0.5405\n",
            "Epoch [97/100], Step [ 46/ 78], Loss: 0.5553\n",
            "Epoch [97/100], Step [ 47/ 78], Loss: 0.4829\n",
            "Epoch [97/100], Step [ 48/ 78], Loss: 0.4972\n",
            "Epoch [97/100], Step [ 49/ 78], Loss: 0.5239\n",
            "Epoch [97/100], Step [ 50/ 78], Loss: 0.4956\n",
            "Epoch [97/100], Step [ 51/ 78], Loss: 0.5243\n",
            "Epoch [97/100], Step [ 52/ 78], Loss: 0.5455\n",
            "Epoch [97/100], Step [ 53/ 78], Loss: 0.5296\n",
            "Epoch [97/100], Step [ 54/ 78], Loss: 0.5176\n",
            "Epoch [97/100], Step [ 55/ 78], Loss: 0.4904\n",
            "Epoch [97/100], Step [ 56/ 78], Loss: 0.5163\n",
            "Epoch [97/100], Step [ 57/ 78], Loss: 0.5518\n",
            "Epoch [97/100], Step [ 58/ 78], Loss: 0.4966\n",
            "Epoch [97/100], Step [ 59/ 78], Loss: 0.4792\n",
            "Epoch [97/100], Step [ 60/ 78], Loss: 0.4882\n",
            "Epoch [97/100], Step [ 61/ 78], Loss: 0.5251\n",
            "Epoch [97/100], Step [ 62/ 78], Loss: 0.5116\n",
            "Epoch [97/100], Step [ 63/ 78], Loss: 0.4632\n",
            "Epoch [98/100], Step [  1/ 78], Loss: 0.4958\n",
            "Epoch [98/100], Step [  2/ 78], Loss: 0.4951\n",
            "Epoch [98/100], Step [  3/ 78], Loss: 0.4844\n",
            "Epoch [98/100], Step [  4/ 78], Loss: 0.5126\n",
            "Epoch [98/100], Step [  5/ 78], Loss: 0.5168\n",
            "Epoch [98/100], Step [  6/ 78], Loss: 0.5006\n",
            "Epoch [98/100], Step [  7/ 78], Loss: 0.5059\n",
            "Epoch [98/100], Step [  8/ 78], Loss: 0.5356\n",
            "Epoch [98/100], Step [  9/ 78], Loss: 0.4353\n",
            "Epoch [98/100], Step [ 10/ 78], Loss: 0.5296\n",
            "Epoch [98/100], Step [ 11/ 78], Loss: 0.5105\n",
            "Epoch [98/100], Step [ 12/ 78], Loss: 0.5004\n",
            "Epoch [98/100], Step [ 13/ 78], Loss: 0.4928\n",
            "Epoch [98/100], Step [ 14/ 78], Loss: 0.5093\n",
            "Epoch [98/100], Step [ 15/ 78], Loss: 0.5191\n",
            "Epoch [98/100], Step [ 16/ 78], Loss: 0.4705\n",
            "Epoch [98/100], Step [ 17/ 78], Loss: 0.5070\n",
            "Epoch [98/100], Step [ 18/ 78], Loss: 0.5105\n",
            "Epoch [98/100], Step [ 19/ 78], Loss: 0.5127\n",
            "Epoch [98/100], Step [ 20/ 78], Loss: 0.5229\n",
            "Epoch [98/100], Step [ 21/ 78], Loss: 0.5187\n",
            "Epoch [98/100], Step [ 22/ 78], Loss: 0.5207\n",
            "Epoch [98/100], Step [ 23/ 78], Loss: 0.4711\n",
            "Epoch [98/100], Step [ 24/ 78], Loss: 0.5127\n",
            "Epoch [98/100], Step [ 25/ 78], Loss: 0.4883\n",
            "Epoch [98/100], Step [ 26/ 78], Loss: 0.4814\n",
            "Epoch [98/100], Step [ 27/ 78], Loss: 0.5088\n",
            "Epoch [98/100], Step [ 28/ 78], Loss: 0.4873\n",
            "Epoch [98/100], Step [ 29/ 78], Loss: 0.5066\n",
            "Epoch [98/100], Step [ 30/ 78], Loss: 0.5082\n",
            "Epoch [98/100], Step [ 31/ 78], Loss: 0.4763\n",
            "Epoch [98/100], Step [ 32/ 78], Loss: 0.5675\n",
            "Epoch [98/100], Step [ 33/ 78], Loss: 0.4843\n",
            "Epoch [98/100], Step [ 34/ 78], Loss: 0.5399\n",
            "Epoch [98/100], Step [ 35/ 78], Loss: 0.5372\n",
            "Epoch [98/100], Step [ 36/ 78], Loss: 0.5005\n",
            "Epoch [98/100], Step [ 37/ 78], Loss: 0.4888\n",
            "Epoch [98/100], Step [ 38/ 78], Loss: 0.5048\n",
            "Epoch [98/100], Step [ 39/ 78], Loss: 0.5167\n",
            "Epoch [98/100], Step [ 40/ 78], Loss: 0.4966\n",
            "Epoch [98/100], Step [ 41/ 78], Loss: 0.5412\n",
            "Epoch [98/100], Step [ 42/ 78], Loss: 0.5105\n",
            "Epoch [98/100], Step [ 43/ 78], Loss: 0.5284\n",
            "Epoch [98/100], Step [ 44/ 78], Loss: 0.5228\n",
            "Epoch [98/100], Step [ 45/ 78], Loss: 0.5030\n",
            "Epoch [98/100], Step [ 46/ 78], Loss: 0.5210\n",
            "Epoch [98/100], Step [ 47/ 78], Loss: 0.5200\n",
            "Epoch [98/100], Step [ 48/ 78], Loss: 0.4993\n",
            "Epoch [98/100], Step [ 49/ 78], Loss: 0.5041\n",
            "Epoch [98/100], Step [ 50/ 78], Loss: 0.4888\n",
            "Epoch [98/100], Step [ 51/ 78], Loss: 0.4645\n",
            "Epoch [98/100], Step [ 52/ 78], Loss: 0.4673\n",
            "Epoch [98/100], Step [ 53/ 78], Loss: 0.4979\n",
            "Epoch [98/100], Step [ 54/ 78], Loss: 0.5215\n",
            "Epoch [98/100], Step [ 55/ 78], Loss: 0.5293\n",
            "Epoch [98/100], Step [ 56/ 78], Loss: 0.5432\n",
            "Epoch [98/100], Step [ 57/ 78], Loss: 0.5273\n",
            "Epoch [98/100], Step [ 58/ 78], Loss: 0.5120\n",
            "Epoch [98/100], Step [ 59/ 78], Loss: 0.5206\n",
            "Epoch [98/100], Step [ 60/ 78], Loss: 0.5216\n",
            "Epoch [98/100], Step [ 61/ 78], Loss: 0.5926\n",
            "Epoch [98/100], Step [ 62/ 78], Loss: 0.4909\n",
            "Epoch [98/100], Step [ 63/ 78], Loss: 0.5514\n",
            "Epoch [99/100], Step [  1/ 78], Loss: 0.4910\n",
            "Epoch [99/100], Step [  2/ 78], Loss: 0.5014\n",
            "Epoch [99/100], Step [  3/ 78], Loss: 0.5232\n",
            "Epoch [99/100], Step [  4/ 78], Loss: 0.5094\n",
            "Epoch [99/100], Step [  5/ 78], Loss: 0.5007\n",
            "Epoch [99/100], Step [  6/ 78], Loss: 0.4988\n",
            "Epoch [99/100], Step [  7/ 78], Loss: 0.5330\n",
            "Epoch [99/100], Step [  8/ 78], Loss: 0.5045\n",
            "Epoch [99/100], Step [  9/ 78], Loss: 0.5254\n",
            "Epoch [99/100], Step [ 10/ 78], Loss: 0.5126\n",
            "Epoch [99/100], Step [ 11/ 78], Loss: 0.4943\n",
            "Epoch [99/100], Step [ 12/ 78], Loss: 0.5178\n",
            "Epoch [99/100], Step [ 13/ 78], Loss: 0.4932\n",
            "Epoch [99/100], Step [ 14/ 78], Loss: 0.5086\n",
            "Epoch [99/100], Step [ 15/ 78], Loss: 0.5053\n",
            "Epoch [99/100], Step [ 16/ 78], Loss: 0.4861\n",
            "Epoch [99/100], Step [ 17/ 78], Loss: 0.5308\n",
            "Epoch [99/100], Step [ 18/ 78], Loss: 0.4619\n",
            "Epoch [99/100], Step [ 19/ 78], Loss: 0.4965\n",
            "Epoch [99/100], Step [ 20/ 78], Loss: 0.5017\n",
            "Epoch [99/100], Step [ 21/ 78], Loss: 0.5084\n",
            "Epoch [99/100], Step [ 22/ 78], Loss: 0.4684\n",
            "Epoch [99/100], Step [ 23/ 78], Loss: 0.4754\n",
            "Epoch [99/100], Step [ 24/ 78], Loss: 0.5467\n",
            "Epoch [99/100], Step [ 25/ 78], Loss: 0.5228\n",
            "Epoch [99/100], Step [ 26/ 78], Loss: 0.5515\n",
            "Epoch [99/100], Step [ 27/ 78], Loss: 0.5000\n",
            "Epoch [99/100], Step [ 28/ 78], Loss: 0.5509\n",
            "Epoch [99/100], Step [ 29/ 78], Loss: 0.4991\n",
            "Epoch [99/100], Step [ 30/ 78], Loss: 0.5411\n",
            "Epoch [99/100], Step [ 31/ 78], Loss: 0.5111\n",
            "Epoch [99/100], Step [ 32/ 78], Loss: 0.5477\n",
            "Epoch [99/100], Step [ 33/ 78], Loss: 0.4967\n",
            "Epoch [99/100], Step [ 34/ 78], Loss: 0.5039\n",
            "Epoch [99/100], Step [ 35/ 78], Loss: 0.4539\n",
            "Epoch [99/100], Step [ 36/ 78], Loss: 0.4789\n",
            "Epoch [99/100], Step [ 37/ 78], Loss: 0.5037\n",
            "Epoch [99/100], Step [ 38/ 78], Loss: 0.4673\n",
            "Epoch [99/100], Step [ 39/ 78], Loss: 0.4916\n",
            "Epoch [99/100], Step [ 40/ 78], Loss: 0.5059\n",
            "Epoch [99/100], Step [ 41/ 78], Loss: 0.5476\n",
            "Epoch [99/100], Step [ 42/ 78], Loss: 0.5128\n",
            "Epoch [99/100], Step [ 43/ 78], Loss: 0.5294\n",
            "Epoch [99/100], Step [ 44/ 78], Loss: 0.5415\n",
            "Epoch [99/100], Step [ 45/ 78], Loss: 0.5344\n",
            "Epoch [99/100], Step [ 46/ 78], Loss: 0.4972\n",
            "Epoch [99/100], Step [ 47/ 78], Loss: 0.5138\n",
            "Epoch [99/100], Step [ 48/ 78], Loss: 0.5203\n",
            "Epoch [99/100], Step [ 49/ 78], Loss: 0.5276\n",
            "Epoch [99/100], Step [ 50/ 78], Loss: 0.4874\n",
            "Epoch [99/100], Step [ 51/ 78], Loss: 0.4826\n",
            "Epoch [99/100], Step [ 52/ 78], Loss: 0.5085\n",
            "Epoch [99/100], Step [ 53/ 78], Loss: 0.4766\n",
            "Epoch [99/100], Step [ 54/ 78], Loss: 0.4821\n",
            "Epoch [99/100], Step [ 55/ 78], Loss: 0.5123\n",
            "Epoch [99/100], Step [ 56/ 78], Loss: 0.4964\n",
            "Epoch [99/100], Step [ 57/ 78], Loss: 0.4703\n",
            "Epoch [99/100], Step [ 58/ 78], Loss: 0.5248\n",
            "Epoch [99/100], Step [ 59/ 78], Loss: 0.5533\n",
            "Epoch [99/100], Step [ 60/ 78], Loss: 0.5111\n",
            "Epoch [99/100], Step [ 61/ 78], Loss: 0.4966\n",
            "Epoch [99/100], Step [ 62/ 78], Loss: 0.4787\n",
            "Epoch [99/100], Step [ 63/ 78], Loss: 0.4720\n",
            "Epoch [100/100], Step [  1/ 78], Loss: 0.5009\n",
            "Epoch [100/100], Step [  2/ 78], Loss: 0.4974\n",
            "Epoch [100/100], Step [  3/ 78], Loss: 0.4754\n",
            "Epoch [100/100], Step [  4/ 78], Loss: 0.5140\n",
            "Epoch [100/100], Step [  5/ 78], Loss: 0.5808\n",
            "Epoch [100/100], Step [  6/ 78], Loss: 0.5114\n",
            "Epoch [100/100], Step [  7/ 78], Loss: 0.5434\n",
            "Epoch [100/100], Step [  8/ 78], Loss: 0.5720\n",
            "Epoch [100/100], Step [  9/ 78], Loss: 0.4985\n",
            "Epoch [100/100], Step [ 10/ 78], Loss: 0.4502\n",
            "Epoch [100/100], Step [ 11/ 78], Loss: 0.5057\n",
            "Epoch [100/100], Step [ 12/ 78], Loss: 0.4994\n",
            "Epoch [100/100], Step [ 13/ 78], Loss: 0.4950\n",
            "Epoch [100/100], Step [ 14/ 78], Loss: 0.5182\n",
            "Epoch [100/100], Step [ 15/ 78], Loss: 0.4982\n",
            "Epoch [100/100], Step [ 16/ 78], Loss: 0.5234\n",
            "Epoch [100/100], Step [ 17/ 78], Loss: 0.4817\n",
            "Epoch [100/100], Step [ 18/ 78], Loss: 0.4405\n",
            "Epoch [100/100], Step [ 19/ 78], Loss: 0.4871\n",
            "Epoch [100/100], Step [ 20/ 78], Loss: 0.4913\n",
            "Epoch [100/100], Step [ 21/ 78], Loss: 0.5383\n",
            "Epoch [100/100], Step [ 22/ 78], Loss: 0.5346\n",
            "Epoch [100/100], Step [ 23/ 78], Loss: 0.5230\n",
            "Epoch [100/100], Step [ 24/ 78], Loss: 0.5110\n",
            "Epoch [100/100], Step [ 25/ 78], Loss: 0.5328\n",
            "Epoch [100/100], Step [ 26/ 78], Loss: 0.5213\n",
            "Epoch [100/100], Step [ 27/ 78], Loss: 0.5256\n",
            "Epoch [100/100], Step [ 28/ 78], Loss: 0.4779\n",
            "Epoch [100/100], Step [ 29/ 78], Loss: 0.4921\n",
            "Epoch [100/100], Step [ 30/ 78], Loss: 0.5112\n",
            "Epoch [100/100], Step [ 31/ 78], Loss: 0.4758\n",
            "Epoch [100/100], Step [ 32/ 78], Loss: 0.5170\n",
            "Epoch [100/100], Step [ 33/ 78], Loss: 0.5181\n",
            "Epoch [100/100], Step [ 34/ 78], Loss: 0.4710\n",
            "Epoch [100/100], Step [ 35/ 78], Loss: 0.4841\n",
            "Epoch [100/100], Step [ 36/ 78], Loss: 0.5013\n",
            "Epoch [100/100], Step [ 37/ 78], Loss: 0.5148\n",
            "Epoch [100/100], Step [ 38/ 78], Loss: 0.5079\n",
            "Epoch [100/100], Step [ 39/ 78], Loss: 0.5535\n",
            "Epoch [100/100], Step [ 40/ 78], Loss: 0.5503\n",
            "Epoch [100/100], Step [ 41/ 78], Loss: 0.5342\n",
            "Epoch [100/100], Step [ 42/ 78], Loss: 0.5041\n",
            "Epoch [100/100], Step [ 43/ 78], Loss: 0.5178\n",
            "Epoch [100/100], Step [ 44/ 78], Loss: 0.4982\n",
            "Epoch [100/100], Step [ 45/ 78], Loss: 0.5567\n",
            "Epoch [100/100], Step [ 46/ 78], Loss: 0.4987\n",
            "Epoch [100/100], Step [ 47/ 78], Loss: 0.5063\n",
            "Epoch [100/100], Step [ 48/ 78], Loss: 0.4895\n",
            "Epoch [100/100], Step [ 49/ 78], Loss: 0.4858\n",
            "Epoch [100/100], Step [ 50/ 78], Loss: 0.5034\n",
            "Epoch [100/100], Step [ 51/ 78], Loss: 0.4562\n",
            "Epoch [100/100], Step [ 52/ 78], Loss: 0.4763\n",
            "Epoch [100/100], Step [ 53/ 78], Loss: 0.4855\n",
            "Epoch [100/100], Step [ 54/ 78], Loss: 0.5054\n",
            "Epoch [100/100], Step [ 55/ 78], Loss: 0.5296\n",
            "Epoch [100/100], Step [ 56/ 78], Loss: 0.5175\n",
            "Epoch [100/100], Step [ 57/ 78], Loss: 0.5101\n",
            "Epoch [100/100], Step [ 58/ 78], Loss: 0.5045\n",
            "Epoch [100/100], Step [ 59/ 78], Loss: 0.5255\n",
            "Epoch [100/100], Step [ 60/ 78], Loss: 0.4815\n",
            "Epoch [100/100], Step [ 61/ 78], Loss: 0.4880\n",
            "Epoch [100/100], Step [ 62/ 78], Loss: 0.4885\n",
            "Epoch [100/100], Step [ 63/ 78], Loss: 0.4682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ty6CTkTwWZjj",
        "colab_type": "code",
        "outputId": "a8ef39a3-3fe1-4a86-b234-25085c3c1cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "cell_type": "code",
      "source": [
        "#Hadi curves\n",
        "\n",
        "# Visualization is borrowed from https://towardsdatas\n",
        "#cience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5\n",
        "\n",
        "epochs = 10\n",
        "losses = []\n",
        "epoch_losses = []\n",
        "train_losses, test_losses = [], []\n",
        "steps=0\n",
        "print_every=10\n",
        "running_loss = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputss, targetss) in enumerate(train_dl):\n",
        "        steps+=1\n",
        "        inputss = to_var(inputss)\n",
        "        targetss = to_var(targetss)\n",
        "                \n",
        "        # forward\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputss)\n",
        "        \n",
        "        # loss\n",
        "        loss = criterion(outputs, targetss)\n",
        "        losses += [loss.item()] # draw fig\n",
        "        \n",
        "        # loos backward\n",
        "        loss.backward()\n",
        "        \n",
        "        #update params\n",
        "        optimizer.step()\n",
        "        \n",
        "        #report\n",
        "        print ('Epoch [%2d/%3d], Step [%3d/%3d], Loss: %.4f' \n",
        "                   % (epoch + 1, num_epochs, i + 1, len(train_ds) // batch_size, loss.item()))\n",
        "        \n",
        "        if steps % print_every == 0:\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for images, labels in valid_dl:\n",
        "                    iinputs = to_var(images)\n",
        "                    labels = to_var(labels)\n",
        "                    logps = model.forward(iinputs)\n",
        "                    batch_loss = criterion(logps, labels)\n",
        "                    test_loss += batch_loss.item()\n",
        "                    ps = torch.exp(logps)\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    accuracy +=torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "            train_losses.append(running_loss/len(train_dl))\n",
        "            test_losses.append(test_loss/len(valid_dl))                    \n",
        "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
        "                  f\"Test loss: {test_loss/len(valid_dl):.3f}.. \"\n",
        "                  f\"Test accuracy: {accuracy/len(valid_dl):.3f}\")\n",
        "            running_loss = 0\n",
        "            model.train()\n",
        "        \n",
        "    epoch_losses.append(np.mean(losses))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 1/ 10], Step [  1/ 78], Loss: 0.6915\n",
            "Epoch [ 1/ 10], Step [  2/ 78], Loss: 0.6918\n",
            "Epoch [ 1/ 10], Step [  3/ 78], Loss: 0.6924\n",
            "Epoch [ 1/ 10], Step [  4/ 78], Loss: 0.6913\n",
            "Epoch [ 1/ 10], Step [  5/ 78], Loss: 0.6909\n",
            "Epoch [ 1/ 10], Step [  6/ 78], Loss: 0.6920\n",
            "Epoch [ 1/ 10], Step [  7/ 78], Loss: 0.6896\n",
            "Epoch [ 1/ 10], Step [  8/ 78], Loss: 0.6906\n",
            "Epoch [ 1/ 10], Step [  9/ 78], Loss: 0.6903\n",
            "Epoch [ 1/ 10], Step [ 10/ 78], Loss: 0.6918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-bf04264375de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtest_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             print(f\"Epoch {epoch+1}/{epochs}.. \"\n\u001b[0m\u001b[1;32m     51\u001b[0m                   \u001b[0;34mf\"Train loss: {running_loss/print_every:.3f}.. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                   \u001b[0;34mf\"Test loss: {test_loss/len(valid_dl):.3f}.. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "-BRBmTsVYiud",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Hadi curve printing\n",
        "\n",
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(test_losses, label='Validation loss')\n",
        "plt.legend(frameon=False)\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_sxmwrd3ak4_",
        "outputId": "2601abb4-e6c7-4f3e-f5c9-c85a40646abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(epoch_losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Model Loss')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Cross Entropy loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAEVCAYAAADAcXJ8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Bvlid1Xeak5B",
        "outputId": "988ee03e-785b-4ccf-d151-6cc2692e3780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "# def evaluate_model(model, DataLoader):\n",
        "#     model.eval()  # for batch norm layer\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in valid_dl:\n",
        "        images, labels = data\n",
        "        images = to_var(images)\n",
        "        labels = to_var(labels)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the valid images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the valid images: 75 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rAT0QS3hQ9tC",
        "outputId": "f3ab03a9-e8dc-49cb-eed9-e87d54144d77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in train_dl:\n",
        "        images, labels = data\n",
        "        images = to_var(images)\n",
        "        labels = to_var(labels)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the train images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the train images: 82 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0PBv8Wjiak5C",
        "scrolled": true,
        "outputId": "0a89bf37-adcb-4146-ed1f-54254ca3cbd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for data in test_dl:\n",
        "        images, labels = data\n",
        "        images = to_var(images)\n",
        "        labels = to_var(labels)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predictions.append(predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 53 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PrFdb00m2bd4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Submit the results"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ku6un7w7Gqgq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predictCSV(predicts, file_path):\n",
        "  labels = {0:'Cat', 1:'Dog'}\n",
        "  with open(file_path,'w',newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    writer.writerow((\"id\",\"label\"))\n",
        "    for i, label in enumerate(predicts):\n",
        "#       _, predict = torch.max(predicts[i].data, 0)\n",
        "#       print(int(predicts[i].data[0]))\n",
        "      writer.writerow((i+1,labels[int(predicts[i].data[0])]))\n",
        "#   debug(\"save predict csv files of model to :{}\".format(file_path))\n",
        "\n",
        "# print(predictions)\n",
        "predictCSV(predictions,\"/drive/My Drive/kaggle.csv\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}